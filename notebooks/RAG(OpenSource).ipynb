{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_DGvBhqm2gRj"
      },
      "outputs": [],
      "source": [
        "from llama_index import ServiceContext\n",
        "\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    llm=llm, embed_model=embed_model\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09QxvtNW2nFe"
      },
      "source": [
        "CHROMADB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh-mmnBtKiQ9"
      },
      "source": [
        "#### Initialize Postgres\n",
        "\n",
        "Using an existing postgres running at localhost, create the database we'll be using.\n",
        "\n",
        "**NOTE**: Of course there are plenty of other open-source/self-hosted databases you can use! e.g. Chroma, Qdrant, Weaviate, and many more. Take a look at our [vector store guide](https://gpt-index.readthedocs.io/en/stable/core_modules/data_modules/storage/vector_stores.html).\n",
        "\n",
        "**NOTE**: You will need to setup postgres on your local system. Here's an example of how to set it up on OSX: https://www.sqlshack.com/setting-up-a-postgresql-database-on-mac/.\n",
        "\n",
        "**NOTE**: You will also need to install pgvector (https://github.com/pgvector/pgvector).\n",
        "\n",
        "You can add a role like the following:\n",
        "```\n",
        "CREATE ROLE <user> WITH LOGIN PASSWORD '<password>';\n",
        "ALTER ROLE <user> SUPERUSER;\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "C2wJ-raFG9VG",
        "outputId": "2ec3e173-83e8-4017-ac70-95b3c6d852f1"
      },
      "outputs": [],
      "source": [
        "import psycopg2\n",
        "\n",
        "db_name = \"vector_db\"\n",
        "host = \"localhost\"\n",
        "password = \"password\"\n",
        "port = \"5432\"\n",
        "user = \"postgres\"\n",
        "# conn = psycopg2.connect(connection_string)\n",
        "conn = psycopg2.connect(\n",
        "    dbname=\"postgres\",\n",
        "    host=host,\n",
        "    password=password,\n",
        "    port=port,\n",
        "    user=user,\n",
        ")\n",
        "conn.autocommit = True\n",
        "\n",
        "with conn.cursor() as c:\n",
        "    c.execute(f\"DROP DATABASE IF EXISTS {db_name}\")\n",
        "    c.execute(f\"CREATE DATABASE {db_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/giac/RAG-with-LLamaIndex-Open\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.chdir(\"/home/giac/RAG-with-LLamaIndex-Open/\")\n",
        "cwd = os.getcwd()\n",
        "\n",
        "print(cwd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "get ChromaDB VECTORSTORES created\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from rag_open_source import embeddings\n",
        "\n",
        "embed_model = embeddings.getEmbeddingModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "HuggingFaceEmbedding(model_name='nickprock/sentence-bert-base-italian-uncased', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x7fcf1eacf5e0>, tokenizer_name='nickprock/sentence-bert-base-italian-uncased', max_length=512, pooling=<Pooling.CLS: 'cls'>, normalize=True, query_instruction=None, text_instruction=None, cache_folder=None)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embed_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "from rag_open_source import vector_stores\n",
        "\n",
        "vector_store = vector_stores.setupChromaDB(nodes=None, path=\"./VectorStore\", name=\"test1_collection\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChromaVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, collection_name=None, host=None, port=None, ssl=False, headers=None, persist_dir=None, collection_kwargs={})"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "from rag_open_source import retriever\n",
        "\n",
        "query = \"Qual è il costo totale per il fine-tuning del modello davinci-002 con 6 epoche, e come si confronta con il costo stimato per l'utilizzo del modello gpt-3.5-turbo?\"\n",
        "\n",
        "# Rinomina l'istanza per evitare conflitti con il nome della classe\n",
        "vector_db_retriever_instance = retriever.VectorDBRetriever(vector_store=vector_store, embed_model=embed_model, similarity_top_k=10)\n",
        "\n",
        "nodes_with_scores = vector_db_retriever_instance.retrieve(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[NodeWithScore(node=TextNode(id_='b88b9735-f4af-4d14-b571-9c8ec9ffffa2', embedding=None, metadata={'source': 'Tesi_Giacomo_Signorile (8).pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='1b30a1c8-b01f-4aae-b1fd-d81aa55d13e6', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='c10b0d202af2c8911e2bcbed44b25d565b261c358612019a5527579f261f0ebe'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='cf00fada-6beb-4d74-95bf-f67ea48f4d34', node_type=<ObjectType.TEXT: '1'>, metadata={'source': 'Tesi_Giacomo_Signorile (8).pdf'}, hash='68c608ec50dd51049ca86c4c41c26aa0cf52472daadf58d97b2e054c54c372a8'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='3e67e7c1-9b52-4036-9cb1-ef79be9c92fc', node_type=<ObjectType.TEXT: '1'>, metadata={'source': 'Tesi_Giacomo_Signorile (8).pdf'}, hash='08dea13d00652a3e30c4aaf984b20a7a2996151153c1d3873424f2dc88ab502b')}, text='2.3. Fine-Tuning di ChatGPT: Ottimizzazione dei Modelli OpenAI 15 Il Fine-Tuning migliora il few-shot learning addestrando il modello su un numero maggiore di esempi, consentendo risultati migliori su un’ampia gamma di compiti. Una volta completato il Fine-Tuning, il numero di esempi richiesti nei prompt diminuisce, riducendo i costi e consentendo richieste con latenza inferiore. 2.3.1 Fasi del Fine-Tuning A livello pratico, il processo di Fine-Tuning coinvolge le seguenti fasi: 1. Preparazione e upload dei dati di addestramento 2. Addestramento di un nuovo modello fine-tuned 3. Valutazione dei risultati e, se necessario, ritorno alla fase 1 4. Utilizzo del modello fine-tuned 2.3.2 Modelli Abilitati per il Fine-Tuning Attualmente, il Fine-Tuning è disponibile per modelli specifici, tra cui: • gpt-3.5-turbo-1106 (raccomandato) • gpt-3.5-turbo-0613 • babbage-002 • davinci-002 • gpt-4-0613 (sperimentale) GPT-3.5-turbo è generalmente consigliato per la maggior parte degli utenti in termini di risultati e', start_char_idx=0, end_char_idx=1015, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5634962670178039),\n",
              " NodeWithScore(node=TextNode(id_='3c2edd63-e806-4a0a-910b-83e399300704', embedding=None, metadata={'source': 'Tesi_Giacomo_Signorile (8).pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d391b18f-9bca-4f2b-999f-102385b7d4b0', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='09f73a1d31cab3da0d066277f90620ee770a0294090c6fdf572a0c6c94df5b76'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='8ffacf09-833b-4ce3-8258-cae92dfcabdb', node_type=<ObjectType.TEXT: '1'>, metadata={'source': 'Tesi_Giacomo_Signorile (8).pdf'}, hash='414eceb6d3db422e8720ed715098d6117c56a144ff72579c30f66d8147b1e2a3'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='4550d82b-a99c-4491-b6d1-2c8ce47697c1', node_type=<ObjectType.TEXT: '1'>, metadata={'source': 'Tesi_Giacomo_Signorile (8).pdf'}, hash='00b03b64081265d5451c6d71aa0e9fbf074b471774d14e77fde06a96b958dcdc')}, text='efficenza nell’addestramento con l’aumentare delle epoche, aumentando tuttavia il costo del Fine- Tuning dovuto al numero maggiore di tokens allenati. Anche GPT-3.5-Turbo a 4 epoche evidenzia una leggera previsione più accurata sul modello sempre con 4 epoche.', start_char_idx=0, end_char_idx=260, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5546747483578496),\n",
              " NodeWithScore(node=TextNode(id_='31ade1f3-4248-4246-922a-b073c8719739', embedding=None, metadata={'source': 'Tesi_Giacomo_Signorile (8).pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b17e46ce-3c0f-4936-be0b-fb8a7eb56049', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='2e425f214ca21de96a86464242c6d2f0e382075c08e91f39f3f7d3db9f1ba02d'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='31d47c14-bf64-4d8c-aa18-4001c701ad3d', node_type=<ObjectType.TEXT: '1'>, metadata={'source': 'Tesi_Giacomo_Signorile (8).pdf'}, hash='c8e17721df721af2ea3c2da7aa2909e5237c0e7ba4d2248a11767a76b9d78657'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='9bc9358c-2541-4389-8787-05f5271722f0', node_type=<ObjectType.TEXT: '1'>, metadata={'source': 'Tesi_Giacomo_Signorile (8).pdf'}, hash='c70d06dc016460de749d55356c769551534b43e2b3d08731395567aed0be97fc')}, text='number_of_tokens_used = response[’usage ’][’total_tokens ’] 28 print(f\"Numero di token utilizzati: { number_of_tokens_used }\") 29 data[’completion ’] = improved_completion 30 31 time.sleep (2) Per eseguire questo lavoro appunto è stata utilizzata la tecnica del Prompt-Engineering, discussa nei capitoli precedenti, applicata su text-davinci-003 per appunto ricercare il mi- glior prompt che eseguisse al meglio e più efficacemente il completamento del Dataset per fine-tunare il modello. 7.5 Fine-Tuning: Procedura e Risultati Metrici Nei notebook dedicati all’avvio del Fine-Tuning, sono stati inizialmente stimati i costi as- sociati ai modelli OpenAI, in particolare a davinci-002 e gpt-3.5-turbo, sui quali verrà suc- cessivamente eseguito il Fine-Tuning. I costi sono suddivisi tra il costo del Fine-Tuning in sé e il costo dell’utilizzo del modello Fine-Tuned.', start_char_idx=0, end_char_idx=867, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5471061266374437),\n",
              " NodeWithScore(node=TextNode(id_='bf7893ab-3b42-40d8-b0ea-d4a8094abaa2', embedding=None, metadata={'source': 'Tesi_Giacomo_Signorile (8).pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='defc7201-0d43-467b-8ae3-a97823a845c0', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='166a70899c5c9b59b7329d098e085d59459d54f7589659d014822190f134ebae'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='38d703a2-b8cd-4364-8b55-033a8765ece3', node_type=<ObjectType.TEXT: '1'>, metadata={'source': 'Tesi_Giacomo_Signorile (8).pdf'}, hash='0c4a4c690a349038fc8c7d76d3b24dc0123756a55bc495da273d7044a77ae041'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='2d64ca85-342b-47bb-b9df-349df6f9470e', node_type=<ObjectType.TEXT: '1'>, metadata={'source': 'Tesi_Giacomo_Signorile (8).pdf'}, hash='c52edfd815733d516adcd88506ff21b02c84bd125690eb531b39930c3534c581')}, text='componenti: modello di estrazioni dati, vector database, retriever e LLM pre-trained. 6.1.3 PEFT Il costo di PEFT tende ad essere superiore a quello della RAG. Ciò è dovuto al fatto che il fine-tuning, anche efficiente, richiede una considerevole potenza di calcolo, tempo ed esperienza in machine learning. Inoltre, per mantenere questo approccio, sarà necessario eseguire il fine-tuning periodicamente per incorporare nuovi dati rilevanti nel modello.', start_char_idx=0, end_char_idx=453, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5199790975322542),\n",
              " NodeWithScore(node=TextNode(id_='9bc9358c-2541-4389-8787-05f5271722f0', embedding=None, metadata={'source': 'Tesi_Giacomo_Signorile (8).pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='913a5f41-f745-4e03-af18-cf606a780e10', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='a29fa7b7a7424c5544b6fc14253b6fe4cf87e411a85fc7abe1d8419c12e9ec27'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='31ade1f3-4248-4246-922a-b073c8719739', node_type=<ObjectType.TEXT: '1'>, metadata={'source': 'Tesi_Giacomo_Signorile (8).pdf'}, hash='4cfb965d3f6e110417c9c0666929072a784d03fa2dd4012d96239cc9c1aac220'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='bfee7470-f57e-419c-9bdc-af49820df9ae', node_type=<ObjectType.TEXT: '1'>, metadata={'source': 'Tesi_Giacomo_Signorile (8).pdf'}, hash='ea5fc7da17235291761790b3851a77ec71c5b11a694d99721cc931b78f179f6d')}, text='7.5. Fine-Tuning: Procedura e Risultati Metrici 41 Complessivamente, nel corso del progetto, il Fine-Tuning è stato eseguito per tre itera- zioni. La prima iterazione ha coinvolto il Fine-Tuning del dataset comprendente i prompt generati e le completion migliorate, con un numero di epoche pari a 4 (solitamente utiliz- zato per impostazione predefinita). Nel secondo processo di Fine-Tuning, il dataset con i prompt generati è stato utilizzato, ma in questo caso la completion non è stata migliorata da text-davinci-003. Il numero di epoche è stato aumentato a 6 al fine di ottenere una risposta più simile al modello addestrato su un dataset migliorato. Nel terzo processo, il dataset è stato trasfor- mato nel formato JSONL e sottoposto a Fine-Tuning utilizzando la struttura conversazio- nale di \"gpt-3.5-turbo\". Questo modello è stato necessario per la fase di RetrievalQA di LangChain per la RAG. Il totale dei token nell’intero dataset si aggira intorno ai 120,000. Questa è la tabella che include il costo di', start_char_idx=0, end_char_idx=1016, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5195074519653602),\n",
              " NodeWithScore(node=TextNode(id_='7d06e221-7214-4ad2-b138-0a761b5cb0dd', embedding=None, metadata={'source': 'Tesi_Giacomo_Signorile (8).pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='c417fe49-73f6-40dd-8b18-1f496dd8b244', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='1133201d9874fb5f0183293a61a9b71d38fe2fa9eb338e4cb2c355c650f185f1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='9d85ad8f-79c5-41be-99ff-11c9889a9435', node_type=<ObjectType.TEXT: '1'>, metadata={'source': 'Tesi_Giacomo_Signorile (8).pdf'}, hash='3e6456616e9956f95eb9113816ca723fb93f5e5073d29a10e3e9df2ebff70ebf'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='3262b6d8-621d-446b-9b9d-e640b228b526', node_type=<ObjectType.TEXT: '1'>, metadata={'source': 'Tesi_Giacomo_Signorile (8).pdf'}, hash='2bb660943a9f41145d196606ec92261e20e7abe5c118ab0da6269601a943cd1f')}, text='prestazioni: • Una volta completato il fine-tuning, valutare le prestazioni del modello sul test set, che non ha mai visto prima. • Le metriche potrebbero includere il punteggio BLEU, il punteggio ROUGE o valutazioni umane per misurare la qualità e la pertinenza degli abstract generati rispetto a quelli originali. 6. Iterare fino a ottenere prestazioni soddisfacenti: • Sulla base dei risultati della valutazione, iterare sui passaggi precedenti, even- tualmente raccogliendo ulteriori dati, regolando gli iperparametri o provando diverse configurazioni del modello per migliorare le prestazioni. 2.1.1 Cosa rende il full fine-tuning vantaggioso? • Richiede meno dati rispetto all’addestramento da zero: Il full fine-tuning può es- sere efficace anche con un dataset specifico per il compito relativamente piccolo. Il LLM preaddestrato già comprende costrutti linguistici generali, e il processo di fine-tuning si concentra principalmente sull’adattamento della conoscenza del mo- dello alle specificità dei nuovi dati.', start_char_idx=0, end_char_idx=1021, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5146919175435911),\n",
              " NodeWithScore(node=TextNode(id_='38d703a2-b8cd-4364-8b55-033a8765ece3', embedding=None, metadata={'source': 'Tesi_Giacomo_Signorile (8).pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='27ec0d01-e370-4068-9904-b36fcabff0be', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='1fa1a71118b8032aeae303fd6054819c36f735457c6a8c694909e57e22f3d81c'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c2e90762-bc10-4a14-85d9-a6c090c723b8', node_type=<ObjectType.TEXT: '1'>, metadata={'source': 'Tesi_Giacomo_Signorile (8).pdf'}, hash='b4274479ec0004d4b819c1f1a1b86aa1477b736df73ce166d231e6ac0d3e8e8b'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='bf7893ab-3b42-40d8-b0ea-d4a8094abaa2', node_type=<ObjectType.TEXT: '1'>, metadata={'source': 'Tesi_Giacomo_Signorile (8).pdf'}, hash='4c23b9f4bc679ec23e78401fc6f577bd6fc86b998331ca728f56eeb84959bb37')}, text='29 Capitolo 6 Scelta del giusto approccio per l’applicazione Dopo aver esaminato i quattro approcci all’adattamento dei Large Language Model(LLM), confrontiamoli su tre metriche importanti: complessità, costo e accuratezza. 6.1 Costo Nel valutare il costo di un approccio, ha senso prendere in considerazione il costo della sua implementazione iniziale, insieme al costo di mantenimento della soluzione. Dato ciò, confrontiamo i costi dei quattro approcci. 6.1.1 Prompt-Engineering Il prompt-engineering ha il costo più basso tra i quattro approcci. Si riduce alla scrittu- ra e al test di prompt per trovare quelli che danno buoni risultati quando alimentati al LLM preaddestrato. Potrebbe anche implicare l’aggiornamento dei prompt se il modello preaddestrato viene periodicamente aggiornato o sostituito, ad esempio con un modello commerciale come il GPT-4 di OpenAI. 6.1.2 RAG Il costo di implementazione di RAG potrebbe essere superiore a quello del prompt- engineering. Ciò è dovuto alla necessità di diversi', start_char_idx=0, end_char_idx=1014, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5131588619455814),\n",
              " NodeWithScore(node=TextNode(id_='8ffacf09-833b-4ce3-8258-cae92dfcabdb', embedding=None, metadata={'source': 'Tesi_Giacomo_Signorile (8).pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='773cbb47-e06f-405a-9e18-4d0f47777716', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='519f36716209f4eb7466ed057c90ddf388ac2de4a8c39fd8b9613c8e8da97f3f'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='2262ad46-3dcf-4b6c-a68e-95aca4fd5fc9', node_type=<ObjectType.TEXT: '1'>, metadata={'source': 'Tesi_Giacomo_Signorile (8).pdf'}, hash='b31285a21b5b8df5e5b855eef9a88a3c7c1ef59062565f341e7874e0570ddc48'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='3c2edd63-e806-4a0a-910b-83e399300704', node_type=<ObjectType.TEXT: '1'>, metadata={'source': 'Tesi_Giacomo_Signorile (8).pdf'}, hash='3800c884918e0e1f397c09fdbbfb1d316ba173506e84bb35d77f875b6ed960c1')}, text='openai.FineTuningJob.retrieve(jobname), è possibile ottenere lo stato del Fine-Tuning, visualizzando tutti gli iperparametri utiliz- zati nell’addestramento. Inoltre, con openai.FineTuningJob.list_events(id=jobname, li- mit=10), è possibile visualizzare gli eventi, inclusi gli step e le relative training loss del processo di addestramento Queste sono le metriche di tutti le esecuzioni di Fine-Tuning per i modelli:  <start_table1>| Modello | Epochs | Trained tokens | Training Loss | |:--------------|---------:|:-----------------|----------------:| | Davinci-002 | 4 | 251 120 | 1.1438 | | Davinci-002 | 6 | 613 428 | 0.7629 | | Gpt-3.5-turbo | 4 | 251 120 | 1.0772 | <end_table1>TABELLA 7.2: Tabella relativa alle metriche di addestramento del fine- tuning di OpenAI Come possiamo notare dalla tabella eseguendo il Fine-Tuning di Davinci-002 con 4 epoche produce una Training loss molto maggiore rispetto ad addestrarlo con 6 epoche, il che si traduce in un potenziale miglioramento del modello o una maggiore', start_char_idx=0, end_char_idx=1014, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5111185368466359),\n",
              " NodeWithScore(node=TextNode(id_='2d64ca85-342b-47bb-b9df-349df6f9470e', embedding=None, metadata={'source': 'Tesi_Giacomo_Signorile (8).pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='00f4f6e9-923a-4fad-9ba3-a83680bbb0a6', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='5f40266060cba3eb0afe301de8be71ed15c1bcb0b97dbc929d11f334afced71b'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='bf7893ab-3b42-40d8-b0ea-d4a8094abaa2', node_type=<ObjectType.TEXT: '1'>, metadata={'source': 'Tesi_Giacomo_Signorile (8).pdf'}, hash='4c23b9f4bc679ec23e78401fc6f577bd6fc86b998331ca728f56eeb84959bb37'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='bca2aec1-926d-459e-978c-22e9aa996eeb', node_type=<ObjectType.TEXT: '1'>, metadata={'source': 'Tesi_Giacomo_Signorile (8).pdf'}, hash='42fa580d691a978a897bc69a4b76158fe2b27cfca334fda9c9ec13290e9dfa09')}, text='30 Capitolo 6. Scelta del giusto approccio per l’applicazione 6.1.4 Full fine-tuning Questo metodo è significativamente più costoso rispetto a PEFT, dato che richiede ancora più potenza di calcolo e tempo. 6.2 Complessità dell’implementazione Dall’approccio relativamente semplice del prompt-engineering alle configurazioni più intricate della RAG e metodi di fine-tuning, la complessità può variare notevolmente. Ecco una rapida panoramica di cosa comporta ciascun metodo. 6.2.1 Prompt-Engineering Questo metodo ha una complessità di implementazione relativamente bassa. Richiede poca o nessuna programmazione. Per redigere un buon prompt e condurre esperimenti, un ingegnere delle istruzioni ha bisogno di buone competenze linguistiche, competenze di dominio e familiarità con gli approcci di apprendimento pochi tiri. 6.2.2 RAG Questo approccio ha una complessità di implementazione più elevata rispetto all’inge- gneria delle istruzioni. Per implementare questa soluzione, sono necessarie competen- ze di codifica e di', start_char_idx=0, end_char_idx=1022, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5108494294370091),\n",
              " NodeWithScore(node=TextNode(id_='cfdfc08f-f264-41bf-850b-e7b803072a48', embedding=None, metadata={'source': 'Tesi_Giacomo_Signorile (8).pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='71653edd-296b-4169-8d32-6518600f3df7', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='08d221dea4e59426abe84d799f12e43f84b845c0d3a3277fa71d14dddc070d9e'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='3e67e7c1-9b52-4036-9cb1-ef79be9c92fc', node_type=<ObjectType.TEXT: '1'>, metadata={'source': 'Tesi_Giacomo_Signorile (8).pdf'}, hash='08dea13d00652a3e30c4aaf984b20a7a2996151153c1d3873424f2dc88ab502b'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='55740c57-55c8-468c-a472-a5152d2f3946', node_type=<ObjectType.TEXT: '1'>, metadata={'source': 'Tesi_Giacomo_Signorile (8).pdf'}, hash='82c372a486b35f7c634050286dde75e3a370caea5dc573fe2530a93b04695c14')}, text='16 Capitolo 2. Full Fine-Tuning 2.3.4 Casi Comuni di Utilizzo del Fine-Tuning Alcuni scenari comuni in cui il Fine-Tuning può migliorare i risultati includono: • Definire stile, tono, formato e altri aspetti qualitativi • Migliorare l’affidabilità nella produzione di output desiderati • Correggere errori nel seguire prompt complessi • Gestire casi limite in modi specifici • Apprendere nuove abilità o compiti difficili da esprimere in un prompt 2.3.5 Preparazione del Dataset per il Fine-Tuning Una volta deciso che il Fine-Tuning è la soluzione appropriata, è necessario preparare il dataset di addestramento. Questo deve comprendere conversazioni di dimostrazione simili a quelle con cui si chiederà al modello di rispondere nella produzione. Il formato del dataset varia in base al modello selezionato, ad esempio: • Per gpt-3.5-turbo, è richiesto un formato di chat conversazionale. • Per babbage-002 e davinci-002, è richiesta una coppia di prompt e completamento. 2.3.6 Gestione dei Limiti di Token I limiti di', start_char_idx=0, end_char_idx=1019, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.4891929702121979)]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nodes_with_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ./model/mistral-Ita-7b-q4_k_m.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = mistral\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name     = mistral\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
            "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
            "................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 3900\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   487.50 MiB\n",
            "llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB\n",
            "llama_new_context_with_model:        CPU input buffer size   =    16.65 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   275.75 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 1\n",
            "AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'mistral', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n"
          ]
        }
      ],
      "source": [
        "query_engine = retriever.get_query_engine(vector_db_retriever_instance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception in adding task cannot pickle 'generator' object\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/giac/RAG-with-LLamaIndex-Open/py310/lib/python3.10/site-packages/langfuse/task_manager.py\", line 235, in add_task\n",
            "    json.dumps(event, cls=EventSerializer)\n",
            "  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n",
            "    **kw).encode(obj)\n",
            "  File \"/usr/lib/python3.10/json/encoder.py\", line 199, in encode\n",
            "    chunks = self.iterencode(o, _one_shot=True)\n",
            "  File \"/usr/lib/python3.10/json/encoder.py\", line 257, in iterencode\n",
            "    return _iterencode(o, 0)\n",
            "  File \"/home/giac/RAG-with-LLamaIndex-Open/py310/lib/python3.10/site-packages/langfuse/serializer.py\", line 31, in default\n",
            "    return asdict(obj)\n",
            "  File \"/usr/lib/python3.10/dataclasses.py\", line 1238, in asdict\n",
            "    return _asdict_inner(obj, dict_factory)\n",
            "  File \"/usr/lib/python3.10/dataclasses.py\", line 1245, in _asdict_inner\n",
            "    value = _asdict_inner(getattr(obj, f.name), dict_factory)\n",
            "  File \"/usr/lib/python3.10/dataclasses.py\", line 1279, in _asdict_inner\n",
            "    return copy.deepcopy(obj)\n",
            "  File \"/usr/lib/python3.10/copy.py\", line 161, in deepcopy\n",
            "    rv = reductor(4)\n",
            "TypeError: cannot pickle 'generator' object\n",
            "Exception in adding task cannot pickle 'generator' object\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/giac/RAG-with-LLamaIndex-Open/py310/lib/python3.10/site-packages/langfuse/task_manager.py\", line 235, in add_task\n",
            "    json.dumps(event, cls=EventSerializer)\n",
            "  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n",
            "    **kw).encode(obj)\n",
            "  File \"/usr/lib/python3.10/json/encoder.py\", line 199, in encode\n",
            "    chunks = self.iterencode(o, _one_shot=True)\n",
            "  File \"/usr/lib/python3.10/json/encoder.py\", line 257, in iterencode\n",
            "    return _iterencode(o, 0)\n",
            "  File \"/home/giac/RAG-with-LLamaIndex-Open/py310/lib/python3.10/site-packages/langfuse/serializer.py\", line 31, in default\n",
            "    return asdict(obj)\n",
            "  File \"/usr/lib/python3.10/dataclasses.py\", line 1238, in asdict\n",
            "    return _asdict_inner(obj, dict_factory)\n",
            "  File \"/usr/lib/python3.10/dataclasses.py\", line 1245, in _asdict_inner\n",
            "    value = _asdict_inner(getattr(obj, f.name), dict_factory)\n",
            "  File \"/usr/lib/python3.10/dataclasses.py\", line 1279, in _asdict_inner\n",
            "    return copy.deepcopy(obj)\n",
            "  File \"/usr/lib/python3.10/copy.py\", line 161, in deepcopy\n",
            "    rv = reductor(4)\n",
            "TypeError: cannot pickle 'generator' object\n",
            "Exception in adding task cannot pickle 'generator' object\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/giac/RAG-with-LLamaIndex-Open/py310/lib/python3.10/site-packages/langfuse/task_manager.py\", line 235, in add_task\n",
            "    json.dumps(event, cls=EventSerializer)\n",
            "  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n",
            "    **kw).encode(obj)\n",
            "  File \"/usr/lib/python3.10/json/encoder.py\", line 199, in encode\n",
            "    chunks = self.iterencode(o, _one_shot=True)\n",
            "  File \"/usr/lib/python3.10/json/encoder.py\", line 257, in iterencode\n",
            "    return _iterencode(o, 0)\n",
            "  File \"/home/giac/RAG-with-LLamaIndex-Open/py310/lib/python3.10/site-packages/langfuse/serializer.py\", line 31, in default\n",
            "    return asdict(obj)\n",
            "  File \"/usr/lib/python3.10/dataclasses.py\", line 1238, in asdict\n",
            "    return _asdict_inner(obj, dict_factory)\n",
            "  File \"/usr/lib/python3.10/dataclasses.py\", line 1245, in _asdict_inner\n",
            "    value = _asdict_inner(getattr(obj, f.name), dict_factory)\n",
            "  File \"/usr/lib/python3.10/dataclasses.py\", line 1279, in _asdict_inner\n",
            "    return copy.deepcopy(obj)\n",
            "  File \"/usr/lib/python3.10/copy.py\", line 161, in deepcopy\n",
            "    rv = reductor(4)\n",
            "TypeError: cannot pickle 'generator' object\n",
            "\n",
            "llama_print_timings:        load time =   48451.05 ms\n",
            "llama_print_timings:      sample time =      60.14 ms /   256 runs   (    0.23 ms per token,  4257.02 tokens per second)\n",
            "llama_print_timings: prompt eval time =   87092.83 ms /   886 tokens (   98.30 ms per token,    10.17 tokens per second)\n",
            "llama_print_timings:        eval time =   70936.39 ms /   255 runs   (  278.18 ms per token,     3.59 tokens per second)\n",
            "llama_print_timings:       total time =  158622.93 ms /  1141 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100000 token costano circa 10 dollari per eseguire il fine-tuning di Davinci-002 fine-tuned con 4 epoche.\n",
            "\n",
            "source: Tesi_Giacomo_Signorile (8).pdf\n",
            "\n",
            "token sono limitati da un modello come gpt-3.5-turbo, dove i token vengono limitati a 1000 per ogni completamento. Per i modelli come babbage-002 e davinci-002, i token non sono limitati e possono essere utilizzati a piacere. 2.3.7 Risultati Metriche Il risultato del Fine-Tuning è stato valutato in base a due metriche principali: • La precisione della risposta • La qualità della risposta 2.3.7.1 Precisione della Risposta La precisione della risposta è stata misurata in base alla percentuale di risposte corrette rispetto al numero totale di risposte. 2.3.7.2 Qualità della Ris\n"
          ]
        }
      ],
      "source": [
        "query_str = \"Quanto costa eseguire il fine-tuning di Davinci-002 fine-tuned con 4 epoche con circa 200000 token?\"\n",
        "\n",
        "response = retriever.get_response_italian(query_str,query_engine)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:langfuse:Exception in adding task cannot pickle 'generator' object\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/giac/RAG-with-LLamaIndex-Open/py310/lib/python3.10/site-packages/langfuse/task_manager.py\", line 235, in add_task\n",
            "    json.dumps(event, cls=EventSerializer)\n",
            "  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n",
            "    **kw).encode(obj)\n",
            "  File \"/usr/lib/python3.10/json/encoder.py\", line 199, in encode\n",
            "    chunks = self.iterencode(o, _one_shot=True)\n",
            "  File \"/usr/lib/python3.10/json/encoder.py\", line 257, in iterencode\n",
            "    return _iterencode(o, 0)\n",
            "  File \"/home/giac/RAG-with-LLamaIndex-Open/py310/lib/python3.10/site-packages/langfuse/serializer.py\", line 31, in default\n",
            "    return asdict(obj)\n",
            "  File \"/usr/lib/python3.10/dataclasses.py\", line 1238, in asdict\n",
            "    return _asdict_inner(obj, dict_factory)\n",
            "  File \"/usr/lib/python3.10/dataclasses.py\", line 1245, in _asdict_inner\n",
            "    value = _asdict_inner(getattr(obj, f.name), dict_factory)\n",
            "  File \"/usr/lib/python3.10/dataclasses.py\", line 1279, in _asdict_inner\n",
            "    return copy.deepcopy(obj)\n",
            "  File \"/usr/lib/python3.10/copy.py\", line 161, in deepcopy\n",
            "    rv = reductor(4)\n",
            "TypeError: cannot pickle 'generator' object\n",
            "ERROR:langfuse:Exception in adding task cannot pickle 'generator' object\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/giac/RAG-with-LLamaIndex-Open/py310/lib/python3.10/site-packages/langfuse/task_manager.py\", line 235, in add_task\n",
            "    json.dumps(event, cls=EventSerializer)\n",
            "  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n",
            "    **kw).encode(obj)\n",
            "  File \"/usr/lib/python3.10/json/encoder.py\", line 199, in encode\n",
            "    chunks = self.iterencode(o, _one_shot=True)\n",
            "  File \"/usr/lib/python3.10/json/encoder.py\", line 257, in iterencode\n",
            "    return _iterencode(o, 0)\n",
            "  File \"/home/giac/RAG-with-LLamaIndex-Open/py310/lib/python3.10/site-packages/langfuse/serializer.py\", line 31, in default\n",
            "    return asdict(obj)\n",
            "  File \"/usr/lib/python3.10/dataclasses.py\", line 1238, in asdict\n",
            "    return _asdict_inner(obj, dict_factory)\n",
            "  File \"/usr/lib/python3.10/dataclasses.py\", line 1245, in _asdict_inner\n",
            "    value = _asdict_inner(getattr(obj, f.name), dict_factory)\n",
            "  File \"/usr/lib/python3.10/dataclasses.py\", line 1279, in _asdict_inner\n",
            "    return copy.deepcopy(obj)\n",
            "  File \"/usr/lib/python3.10/copy.py\", line 161, in deepcopy\n",
            "    rv = reductor(4)\n",
            "TypeError: cannot pickle 'generator' object\n",
            "ERROR:langfuse:Exception in adding task cannot pickle 'generator' object\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/giac/RAG-with-LLamaIndex-Open/py310/lib/python3.10/site-packages/langfuse/task_manager.py\", line 235, in add_task\n",
            "    json.dumps(event, cls=EventSerializer)\n",
            "  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n",
            "    **kw).encode(obj)\n",
            "  File \"/usr/lib/python3.10/json/encoder.py\", line 199, in encode\n",
            "    chunks = self.iterencode(o, _one_shot=True)\n",
            "  File \"/usr/lib/python3.10/json/encoder.py\", line 257, in iterencode\n",
            "    return _iterencode(o, 0)\n",
            "  File \"/home/giac/RAG-with-LLamaIndex-Open/py310/lib/python3.10/site-packages/langfuse/serializer.py\", line 31, in default\n",
            "    return asdict(obj)\n",
            "  File \"/usr/lib/python3.10/dataclasses.py\", line 1238, in asdict\n",
            "    return _asdict_inner(obj, dict_factory)\n",
            "  File \"/usr/lib/python3.10/dataclasses.py\", line 1245, in _asdict_inner\n",
            "    value = _asdict_inner(getattr(obj, f.name), dict_factory)\n",
            "  File \"/usr/lib/python3.10/dataclasses.py\", line 1279, in _asdict_inner\n",
            "    return copy.deepcopy(obj)\n",
            "  File \"/usr/lib/python3.10/copy.py\", line 161, in deepcopy\n",
            "    rv = reductor(4)\n",
            "TypeError: cannot pickle 'generator' object\n",
            "\n",
            "llama_print_timings:        load time =   49923.12 ms\n",
            "llama_print_timings:      sample time =      62.14 ms /   256 runs   (    0.24 ms per token,  4119.46 tokens per second)\n",
            "llama_print_timings: prompt eval time =  108859.30 ms /  1061 tokens (  102.60 ms per token,     9.75 tokens per second)\n",
            "llama_print_timings:        eval time =   87541.51 ms /   255 runs   (  343.30 ms per token,     2.91 tokens per second)\n",
            "llama_print_timings:       total time =  197143.56 ms /  1316 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1) Lavoratori italiani: Importo contributivo convenzionale = € 9,60 + 13^ = € 10,39; Importo contributivo comprensivo quota CUAf = € 9,60 + 13^ + 14^ = € 11,13; 2) Lavoratori stranieri: Importo contributivo convenzionale = € 9,60 + 13^ = € 10,39; Importo contributivo comprensivo quota CUAf = € 9,60 + 13^ + 14^ = € 11,13.\n",
            "\n",
            "Query: Qual è l'importo del contributo orario convenzionale e il relativo contributo comprensivo quota CUAf per un lavoratore in Italia che guadagna un'effettiva retribuzione oraria di € 9,60 o più?\n",
            "Answer: 1) Lavoratori italiani: Importo contributivo convenzionale = € 9,60 + 13^ = € 10,39;\n"
          ]
        }
      ],
      "source": [
        "query_str = \"Qual è l'importo del contributo orario convenzionale e il relativo contributo comprensivo quota CUAf per un lavoratore in Italia che guadagna un'effettiva retribuzione oraria di € 9,60 o più?\"\n",
        "\n",
        "response = retriever.get_response_italian(query_str,query_engine)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:langfuse:Exception in adding task cannot pickle 'generator' object\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/giac/RAG-with-LLamaIndex-Open/py310/lib/python3.10/site-packages/langfuse/task_manager.py\", line 235, in add_task\n",
            "    json.dumps(event, cls=EventSerializer)\n",
            "  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n",
            "    **kw).encode(obj)\n",
            "  File \"/usr/lib/python3.10/json/encoder.py\", line 199, in encode\n",
            "    chunks = self.iterencode(o, _one_shot=True)\n",
            "  File \"/usr/lib/python3.10/json/encoder.py\", line 257, in iterencode\n",
            "    return _iterencode(o, 0)\n",
            "  File \"/home/giac/RAG-with-LLamaIndex-Open/py310/lib/python3.10/site-packages/langfuse/serializer.py\", line 31, in default\n",
            "    return asdict(obj)\n",
            "  File \"/usr/lib/python3.10/dataclasses.py\", line 1238, in asdict\n",
            "    return _asdict_inner(obj, dict_factory)\n",
            "  File \"/usr/lib/python3.10/dataclasses.py\", line 1245, in _asdict_inner\n",
            "    value = _asdict_inner(getattr(obj, f.name), dict_factory)\n",
            "  File \"/usr/lib/python3.10/dataclasses.py\", line 1279, in _asdict_inner\n",
            "    return copy.deepcopy(obj)\n",
            "  File \"/usr/lib/python3.10/copy.py\", line 161, in deepcopy\n",
            "    rv = reductor(4)\n",
            "TypeError: cannot pickle 'generator' object\n",
            "ERROR:langfuse:Exception in adding task cannot pickle 'generator' object\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/giac/RAG-with-LLamaIndex-Open/py310/lib/python3.10/site-packages/langfuse/task_manager.py\", line 235, in add_task\n",
            "    json.dumps(event, cls=EventSerializer)\n",
            "  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n",
            "    **kw).encode(obj)\n",
            "  File \"/usr/lib/python3.10/json/encoder.py\", line 199, in encode\n",
            "    chunks = self.iterencode(o, _one_shot=True)\n",
            "  File \"/usr/lib/python3.10/json/encoder.py\", line 257, in iterencode\n",
            "    return _iterencode(o, 0)\n",
            "  File \"/home/giac/RAG-with-LLamaIndex-Open/py310/lib/python3.10/site-packages/langfuse/serializer.py\", line 31, in default\n",
            "    return asdict(obj)\n",
            "  File \"/usr/lib/python3.10/dataclasses.py\", line 1238, in asdict\n",
            "    return _asdict_inner(obj, dict_factory)\n",
            "  File \"/usr/lib/python3.10/dataclasses.py\", line 1245, in _asdict_inner\n",
            "    value = _asdict_inner(getattr(obj, f.name), dict_factory)\n",
            "  File \"/usr/lib/python3.10/dataclasses.py\", line 1279, in _asdict_inner\n",
            "    return copy.deepcopy(obj)\n",
            "  File \"/usr/lib/python3.10/copy.py\", line 161, in deepcopy\n",
            "    rv = reductor(4)\n",
            "TypeError: cannot pickle 'generator' object\n",
            "ERROR:langfuse:Exception in adding task cannot pickle 'generator' object\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/giac/RAG-with-LLamaIndex-Open/py310/lib/python3.10/site-packages/langfuse/task_manager.py\", line 235, in add_task\n",
            "    json.dumps(event, cls=EventSerializer)\n",
            "  File \"/usr/lib/python3.10/json/__init__.py\", line 238, in dumps\n",
            "    **kw).encode(obj)\n",
            "  File \"/usr/lib/python3.10/json/encoder.py\", line 199, in encode\n",
            "    chunks = self.iterencode(o, _one_shot=True)\n",
            "  File \"/usr/lib/python3.10/json/encoder.py\", line 257, in iterencode\n",
            "    return _iterencode(o, 0)\n",
            "  File \"/home/giac/RAG-with-LLamaIndex-Open/py310/lib/python3.10/site-packages/langfuse/serializer.py\", line 31, in default\n",
            "    return asdict(obj)\n",
            "  File \"/usr/lib/python3.10/dataclasses.py\", line 1238, in asdict\n",
            "    return _asdict_inner(obj, dict_factory)\n",
            "  File \"/usr/lib/python3.10/dataclasses.py\", line 1245, in _asdict_inner\n",
            "    value = _asdict_inner(getattr(obj, f.name), dict_factory)\n",
            "  File \"/usr/lib/python3.10/dataclasses.py\", line 1279, in _asdict_inner\n",
            "    return copy.deepcopy(obj)\n",
            "  File \"/usr/lib/python3.10/copy.py\", line 161, in deepcopy\n",
            "    rv = reductor(4)\n",
            "TypeError: cannot pickle 'generator' object\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   49923.12 ms\n",
            "llama_print_timings:      sample time =      56.49 ms /   256 runs   (    0.22 ms per token,  4531.62 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_print_timings:        eval time =   82743.80 ms /   256 runs   (  323.22 ms per token,     3.09 tokens per second)\n",
            "llama_print_timings:       total time =   83362.45 ms /   257 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Il costo totale per il fine-tuning del modello davinci-002 con 6 epoche è di $0,00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n"
          ]
        }
      ],
      "source": [
        "query_str = \"Qual è il costo totale per il fine-tuning del modello davinci-002 con 6 epoche, e come si confronta con il costo stimato per l'utilizzo del modello gpt-3.5-turbo?\"\n",
        "\n",
        "response = retriever.get_response_italian(query_str,query_engine)\n",
        "\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "os.chdir(\"/home/giacomo/Documenti/GitHub/RAG-with-LLamaIndex-Open/\")\n",
        "\n",
        "cwd = os.getcwd()\n",
        "\n",
        "import yaml\n",
        "from src.pdf_parser import chunk_splitter, extract_layout, pdf_ingestion\n",
        "from src.retrieval import embeddings, retriever,vector_stores\n",
        "\n",
        "config_path = './src/config_project.yaml' \n",
        "\n",
        "with open(config_path, 'r') as file:\n",
        "        config = yaml.safe_load(file)\n",
        "\n",
        "documents,tables_md = chunk_splitter.get_documents(path = config['custom_splitter']['path'][1])\n",
        "\n",
        "embed_model = embeddings.getEmbeddingModel(name = config['embeddings']['name'][1])\n",
        "\n",
        "nodes = embeddings.createEmbeddings(documents, embed_model)\n",
        "\n",
        "#nodes = getattr(embeddings, 'createEmbeddings')(documents, embed_model)\n",
        "embeddings_dimensiom = len(nodes[0].embedding)\n",
        "\n",
        "#vector_store = vector_stores.setupChromaDB(nodes, config['vector_store']['path'][0], \n",
        "#                                           config['vector_store']['name'][1])\n",
        "\n",
        "vector_store = vector_stores.choose_vector_store(store_name = 'AstraDB', nodes = nodes, embeddings_dim = embeddings_dimensiom, collection_name = 'prova')\n",
        "\n",
        "vector_db_retriever_instance = retriever.VectorDBRetriever(vector_store=vector_store, \n",
        "                                                            embed_model=embed_model, \n",
        "                                                            similarity_top_k=3)    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 3.95 GiB of which 10.12 MiB is free. Including non-PyTorch memory, this process has 3.93 GiB memory in use. Of the allocated memory 3.87 GiB is allocated by PyTorch, and 20.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpdf_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m chunk_splitter, extract_layout, pdf_ingestion\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretrieval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m embeddings, retriever,vector_stores\n\u001b[0;32m---> 13\u001b[0m embed_model \u001b[38;5;241m=\u001b[39m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetEmbeddingModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mintfloat/multilingual-e5-large-instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m vector_stores\u001b[38;5;241m.\u001b[39mchoose_vector_store(store_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAstraDB\u001b[39m\u001b[38;5;124m'\u001b[39m, nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, collection_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprova\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m vector_db_retriever_instance \u001b[38;5;241m=\u001b[39m retriever\u001b[38;5;241m.\u001b[39mVectorDBRetriever(vector_store\u001b[38;5;241m=\u001b[39mvector_store, \n\u001b[1;32m     20\u001b[0m                                                             embed_model\u001b[38;5;241m=\u001b[39membed_model, \n\u001b[1;32m     21\u001b[0m                                                             similarity_top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
            "File \u001b[0;32m~/Documenti/GitHub/RAG-with-LLamaIndex-Open/src/retrieval/embeddings.py:9\u001b[0m, in \u001b[0;36mgetEmbeddingModel\u001b[0;34m(name, max_len)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetEmbeddingModel\u001b[39m(name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnickprock/sentence-bert-base-italian-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m):\n\u001b[0;32m----> 9\u001b[0m     embed_model \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embed_model\n",
            "File \u001b[0;32m~/Documenti/GitHub/RAG-with-LLamaIndex-Open/py310/lib/python3.10/site-packages/llama_index/embeddings/huggingface/base.py:87\u001b[0m, in \u001b[0;36mHuggingFaceEmbedding.__init__\u001b[0;34m(self, model_name, tokenizer_name, pooling, max_length, query_instruction, text_instruction, normalize, model, tokenizer, embed_batch_size, cache_folder, trust_remote_code, device, callback_manager)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# Extract model_name from model\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mname_or_path\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# Use tokenizer_name with AutoTokenizer\u001b[39;00m\n\u001b[1;32m     90\u001b[0m     tokenizer_name \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     91\u001b[0m         model_name \u001b[38;5;129;01mor\u001b[39;00m tokenizer_name \u001b[38;5;129;01mor\u001b[39;00m DEFAULT_HUGGINGFACE_EMBEDDING_MODEL\n\u001b[1;32m     92\u001b[0m     )\n",
            "File \u001b[0;32m~/Documenti/GitHub/RAG-with-LLamaIndex-Open/py310/lib/python3.10/site-packages/transformers/modeling_utils.py:2556\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2552\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2553\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2554\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2555\u001b[0m         )\n\u001b[0;32m-> 2556\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documenti/GitHub/RAG-with-LLamaIndex-Open/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documenti/GitHub/RAG-with-LLamaIndex-Open/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m~/Documenti/GitHub/RAG-with-LLamaIndex-Open/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "    \u001b[0;31m[... skipping similar frames: Module._apply at line 802 (2 times)]\u001b[0m\n",
            "File \u001b[0;32m~/Documenti/GitHub/RAG-with-LLamaIndex-Open/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m~/Documenti/GitHub/RAG-with-LLamaIndex-Open/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
            "File \u001b[0;32m~/Documenti/GitHub/RAG-with-LLamaIndex-Open/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 3.95 GiB of which 10.12 MiB is free. Including non-PyTorch memory, this process has 3.93 GiB memory in use. Of the allocated memory 3.87 GiB is allocated by PyTorch, and 20.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "os.chdir(\"/home/giacomo/Documenti/GitHub/RAG-with-LLamaIndex-Open/\")\n",
        "\n",
        "cwd = os.getcwd()\n",
        "\n",
        "import yaml\n",
        "from src.pdf_parser import chunk_splitter, extract_layout, pdf_ingestion\n",
        "from src.retrieval import embeddings, retriever,vector_stores\n",
        "\n",
        "embed_model = embeddings.getEmbeddingModel(name = \"intfloat/multilingual-e5-large-instruct\")\n",
        "\n",
        "vector_store = vector_stores.choose_vector_store(store_name = 'AstraDB', nodes = None, collection_name = 'prova')\n",
        "\n",
        "\n",
        "\n",
        "vector_db_retriever_instance = retriever.VectorDBRetriever(vector_store=vector_store, \n",
        "                                                            embed_model=embed_model, \n",
        "                                                            similarity_top_k=3)\n",
        "\n",
        "query_engine = retriever.get_query_engine(vector_db_retriever_instance, model_path='./src/model/mistral-Ita-7b-q5_k_m.gguf')\n",
        "\n",
        "query_str= \"Qual è l'agevolazione prevista per gli investimenti compresi tra 50.000 e 100.000 euro per le iniziative in continuità con imprese pre-esistenti? in riferimento al documento: 'Scheda NIDI - Nuove iniziative d'impresa_ Strumento di ingegneria finanziaria.pdf'\"\n",
        "\n",
        "response = retriever.get_response_italian(query_str,query_engine)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1            |        cudaMalloc retries: 2         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   3923 MiB |   3923 MiB | 727247 MiB | 723323 MiB |\n",
            "|       from large pool |   3921 MiB |   3921 MiB | 646254 MiB | 642332 MiB |\n",
            "|       from small pool |      2 MiB |    196 MiB |  80993 MiB |  80991 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   3923 MiB |   3923 MiB | 727247 MiB | 723323 MiB |\n",
            "|       from large pool |   3921 MiB |   3921 MiB | 646254 MiB | 642332 MiB |\n",
            "|       from small pool |      2 MiB |    196 MiB |  80993 MiB |  80991 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |   3923 MiB |   3923 MiB | 718918 MiB | 714994 MiB |\n",
            "|       from large pool |   3921 MiB |   3921 MiB | 637949 MiB | 634027 MiB |\n",
            "|       from small pool |      2 MiB |    196 MiB |  80969 MiB |  80967 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   3952 MiB |   3952 MiB |   5336 MiB |   1384 MiB |\n",
            "|       from large pool |   3948 MiB |   3948 MiB |   5136 MiB |   1188 MiB |\n",
            "|       from small pool |      4 MiB |    198 MiB |    200 MiB |    196 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  29274 KiB | 129909 KiB | 724133 MiB | 724105 MiB |\n",
            "|       from large pool |  27344 KiB | 128394 KiB | 639498 MiB | 639471 MiB |\n",
            "|       from small pool |   1930 KiB |  66875 KiB |  84635 MiB |  84633 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     669    |     765    |  313510    |  312841    |\n",
            "|       from large pool |     251    |     415    |  121090    |  120839    |\n",
            "|       from small pool |     418    |     617    |  192420    |  192002    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     669    |     765    |  313510    |  312841    |\n",
            "|       from large pool |     251    |     415    |  121090    |  120839    |\n",
            "|       from small pool |     418    |     617    |  192420    |  192002    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     100    |     212    |     260    |     160    |\n",
            "|       from large pool |      98    |     113    |     160    |      62    |\n",
            "|       from small pool |       2    |      99    |     100    |      98    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      13    |     127    |  144868    |  144855    |\n",
            "|       from large pool |      11    |      40    |   60320    |   60309    |\n",
            "|       from small pool |       2    |     102    |   84548    |   84546    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.memory_summary())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "08bfd751f1ee4566877fc8767fcfe738": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b15ce17e91f64c52b620bc983dc9b956",
              "IPY_MODEL_34ab576d14ed4c2eae565340a041b550",
              "IPY_MODEL_54b6d1efc3604a0cbb63eb3fcc0b26f9"
            ],
            "layout": "IPY_MODEL_4e0d249ab72249b08abfac22140d3edd"
          }
        },
        "0927c229a31f46cc9bf0ea2a96b60022": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09a5244e85574d95b3af0d3ab92b90ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0eb41a5e76274f4b897d9b66bff3bc77": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "106f44be7a1847bea9bb9097aee728aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11070937913b4467b193168d7fa73995": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "113762d6dc754861ba155bebb7da58e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17704a95d8b347e6a18991356cf75a16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f9db1425c7974aa1903e054de2f48761",
              "IPY_MODEL_75fd09f9ce29419a90111ed0a0aaa203",
              "IPY_MODEL_4d1acca4a7254af4ac7fd0b6b55de417"
            ],
            "layout": "IPY_MODEL_1e4fb20005e3402c80f67edcd0755300"
          }
        },
        "1a1d8381f15b4c96a15ca4a740f83929": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d157b8f1e834846869736042caea633": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_97c26a1bdf07484a82c8dad800738bf5",
              "IPY_MODEL_d3e3c326efbf4eb987d41c08de71e06b",
              "IPY_MODEL_a11cee124a974fe290bdbf3e6a499e89"
            ],
            "layout": "IPY_MODEL_5b53e866f31f4f9986605547a24a32af"
          }
        },
        "1ddc15082c4c489880093305fc6d3312": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d04611b2fc3a4b9b8ddd0ba40582c4c1",
            "max": 133466304,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_11070937913b4467b193168d7fa73995",
            "value": 133466304
          }
        },
        "1e1c7a27b7f94c5e8d8c2f3488a7074e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e4fb20005e3402c80f67edcd0755300": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24a18c01fc024e13a1a056c7de66f40a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30414fe9f37b4c41924470f045bbb8a6",
            "max": 366,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e9124152dc184c64ade965dea48a19a2",
            "value": 366
          }
        },
        "264b8b07432a42f58d4ec22018f661a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d5b86f05f67448c91014a199e6d8175",
            "placeholder": "​",
            "style": "IPY_MODEL_113762d6dc754861ba155bebb7da58e0",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "2e110952062240bf919c7f33806e7679": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30414fe9f37b4c41924470f045bbb8a6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34ab576d14ed4c2eae565340a041b550": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4ca47ce82db4a55a421544f9c276580",
            "max": 684,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7b9f9816b27548199678242f45237835",
            "value": 684
          }
        },
        "377f4718a1b5417d81e1fe17908f98f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ad8369e08774b588b8a96926ce72876": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_264b8b07432a42f58d4ec22018f661a1",
              "IPY_MODEL_24a18c01fc024e13a1a056c7de66f40a",
              "IPY_MODEL_685b7f51dd09414e9297bc41e6b6a1e1"
            ],
            "layout": "IPY_MODEL_106f44be7a1847bea9bb9097aee728aa"
          }
        },
        "3d5b86f05f67448c91014a199e6d8175": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d1acca4a7254af4ac7fd0b6b55de417": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bbaa4bd459b4f3d9fedf322adad5634",
            "placeholder": "​",
            "style": "IPY_MODEL_54fbfddb381d46e0b64e4044091467c3",
            "value": " 711k/711k [00:00&lt;00:00, 22.4MB/s]"
          }
        },
        "4e0d249ab72249b08abfac22140d3edd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "545a80cd3a9a4c6e8e6cc97384075e01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54b6d1efc3604a0cbb63eb3fcc0b26f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e21581513fa34d45a50874ac37fe547f",
            "placeholder": "​",
            "style": "IPY_MODEL_d53e327f131c445688b1cd256d6ea4e9",
            "value": " 684/684 [00:00&lt;00:00, 35.2kB/s]"
          }
        },
        "54fbfddb381d46e0b64e4044091467c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "585d9964fb40472cbbd5804bf8bca571": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b53e866f31f4f9986605547a24a32af": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "685b7f51dd09414e9297bc41e6b6a1e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb0ca7b9913a4b7483c8151f877bebd8",
            "placeholder": "​",
            "style": "IPY_MODEL_8b0d7b3ea90b477b8873045a1cea023c",
            "value": " 366/366 [00:00&lt;00:00, 17.2kB/s]"
          }
        },
        "6e6627b024b441f5bc19b9d51245771d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74fe9c02301044e1baef1f14c21d4174": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_93f6e9dfde914993887f87ea299d453f",
              "IPY_MODEL_1ddc15082c4c489880093305fc6d3312",
              "IPY_MODEL_852c6ecda208428ebb5121baa98c52c9"
            ],
            "layout": "IPY_MODEL_ec8cd6d1513e4c5ebb2f5b10a7e738b3"
          }
        },
        "754ef2409153456eb881d64670409271": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75fd09f9ce29419a90111ed0a0aaa203": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f44392c9eddf4bb4a3b9215e55d9aa9b",
            "max": 711396,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e12b6915de5642ffae4d110c514ae51b",
            "value": 711396
          }
        },
        "77565f61e7c44fbd9695a74e9880c006": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b565e0991114b90a80e3aebf74ce773": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b9f9816b27548199678242f45237835": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "84631dff0faa47d392eed77061e185fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77565f61e7c44fbd9695a74e9880c006",
            "placeholder": "​",
            "style": "IPY_MODEL_7b565e0991114b90a80e3aebf74ce773",
            "value": " 232k/232k [00:00&lt;00:00, 4.17MB/s]"
          }
        },
        "852c6ecda208428ebb5121baa98c52c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df4a0f5c93364d51a82a63292cd457de",
            "placeholder": "​",
            "style": "IPY_MODEL_1a1d8381f15b4c96a15ca4a740f83929",
            "value": " 133M/133M [00:00&lt;00:00, 164MB/s]"
          }
        },
        "865f101c44bb4829ac35cc5622eb1c74": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b0d7b3ea90b477b8873045a1cea023c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b0f974b3ce34eb387de20d03b7402c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_865f101c44bb4829ac35cc5622eb1c74",
            "placeholder": "​",
            "style": "IPY_MODEL_6e6627b024b441f5bc19b9d51245771d",
            "value": "vocab.txt: 100%"
          }
        },
        "93f6e9dfde914993887f87ea299d453f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_754ef2409153456eb881d64670409271",
            "placeholder": "​",
            "style": "IPY_MODEL_1e1c7a27b7f94c5e8d8c2f3488a7074e",
            "value": "model.safetensors: 100%"
          }
        },
        "97c26a1bdf07484a82c8dad800738bf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e96e1df975624a16bc5595f7414012da",
            "placeholder": "​",
            "style": "IPY_MODEL_545a80cd3a9a4c6e8e6cc97384075e01",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "9bbaa4bd459b4f3d9fedf322adad5634": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a057d684e9854fc097844e3f6b38220d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a11cee124a974fe290bdbf3e6a499e89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5d40d8ced9d474cbe3803fae44cd398",
            "placeholder": "​",
            "style": "IPY_MODEL_377f4718a1b5417d81e1fe17908f98f4",
            "value": " 125/125 [00:00&lt;00:00, 4.58kB/s]"
          }
        },
        "a3c9d2b644ca451a8be4c32d1e74dc98": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6322be6a6b443298bc027df18ae13e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0927c229a31f46cc9bf0ea2a96b60022",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e953b734276d4c53aec49cd572802c5c",
            "value": 231508
          }
        },
        "b15ce17e91f64c52b620bc983dc9b956": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f27357a06fce4b8fbac5ec5b16df4712",
            "placeholder": "​",
            "style": "IPY_MODEL_a057d684e9854fc097844e3f6b38220d",
            "value": "config.json: 100%"
          }
        },
        "cb0ca7b9913a4b7483c8151f877bebd8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d04611b2fc3a4b9b8ddd0ba40582c4c1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3e3c326efbf4eb987d41c08de71e06b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e110952062240bf919c7f33806e7679",
            "max": 125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_09a5244e85574d95b3af0d3ab92b90ca",
            "value": 125
          }
        },
        "d4ca47ce82db4a55a421544f9c276580": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d53e327f131c445688b1cd256d6ea4e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df4a0f5c93364d51a82a63292cd457de": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e082d0d2a09c4be189cdac846e3c5776": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8b0f974b3ce34eb387de20d03b7402c0",
              "IPY_MODEL_a6322be6a6b443298bc027df18ae13e4",
              "IPY_MODEL_84631dff0faa47d392eed77061e185fa"
            ],
            "layout": "IPY_MODEL_a3c9d2b644ca451a8be4c32d1e74dc98"
          }
        },
        "e12b6915de5642ffae4d110c514ae51b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e21581513fa34d45a50874ac37fe547f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9124152dc184c64ade965dea48a19a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e953b734276d4c53aec49cd572802c5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e96e1df975624a16bc5595f7414012da": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec8cd6d1513e4c5ebb2f5b10a7e738b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f27357a06fce4b8fbac5ec5b16df4712": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f44392c9eddf4bb4a3b9215e55d9aa9b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5d40d8ced9d474cbe3803fae44cd398": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9db1425c7974aa1903e054de2f48761": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0eb41a5e76274f4b897d9b66bff3bc77",
            "placeholder": "​",
            "style": "IPY_MODEL_585d9964fb40472cbbd5804bf8bca571",
            "value": "tokenizer.json: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
