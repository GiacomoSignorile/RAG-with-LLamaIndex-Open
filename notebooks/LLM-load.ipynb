{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitsandbytes \n",
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SIGNORILEG\\OneDrive - Links S.p.A\\Documenti\\GitHub\\RAG-with-LLamaIndex-Open\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"c:/Users/SIGNORILEG/OneDrive - Links S.p.A/Documenti/GitHub/RAG-with-LLamaIndex-Open\")\n",
    "cwd = os.getcwd()\n",
    "\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "hf_hub_download(repo_id=\"TheBloke/Llama-2-7B-Chat-GGUF\", filename=\"llama-2-13b.Q4_0.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama-2-7b-chat.Q4_K_M.gguf: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.08G/4.08G [12:18<00:00, 5.53MB/s]\n",
      "c:\\Users\\SIGNORILEG\\OneDrive - Links S.p.A\\Documenti\\GitHub\\RAG-with-LLamaIndex-Open\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\SIGNORILEG\\.cache\\huggingface\\hub\\models--TheBloke--Llama-2-7B-Chat-GGUF. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\SIGNORILEG\\\\.cache\\\\huggingface\\\\hub\\\\models--TheBloke--Llama-2-7B-Chat-GGUF\\\\snapshots\\\\191239b3e26b2882fb562ffccdd1cf0f65402adb\\\\llama-2-7b-chat.Q4_K_M.gguf'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "hf_hub_download(repo_id=\"TheBloke/Llama-2-7B-Chat-GGUF\", filename=\"llama-2-7b-chat.Q4_K_M.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 363 tensors from C:/Users/SIGNORILEG/Downloads/llamantino-2-chat-13b-hf-ita.Q3_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 11\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q3_K:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q3_K - Small\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 5.27 GiB (3.48 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  5396.11 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1600.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1600.00 MiB, K (f16):  800.00 MiB, V (f16):  800.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    14.01 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   209.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '5120', 'llama.block_count': '40', 'llama.feed_forward_length': '13824', 'llama.attention.head_count': '40', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '11', 'llama.attention.head_count_kv': '40', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false'}\n",
      "\n",
      "llama_print_timings:        load time =    7096.77 ms\n",
      "llama_print_timings:      sample time =       2.82 ms /    16 runs   (    0.18 ms per token,  5673.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7096.73 ms /    10 tokens (  709.67 ms per token,     1.41 tokens per second)\n",
      "llama_print_timings:        eval time =   11535.49 ms /    15 runs   (  769.03 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time =   18701.47 ms /    25 tokens\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "model_path = \"C:/Users/SIGNORILEG/Downloads/llamantino-2-chat-13b-hf-ita.Q3_K_S.gguf\"\n",
    "# model_path = \"C:/Users/SIGNORILEG/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-GGUF/snapshots/b106d1c018ac999af9130b83134fb6b7c5331dea/llama-2-13b.Q4_0.gguf\"\n",
    "# model_path = \"C:/Users/SIGNORILEG/.cache/huggingface/hub/models--TheBloke--Llama-2-7B-Chat-GGUF/snapshots/191239b3e26b2882fb562ffccdd1cf0f65402adb/llama-2-7b-chat.Q4_K_M.gguf\"\n",
    "LLM = Llama(model_path=model_path,n_ctx=2048)\n",
    "\n",
    "prompt = \"Sai parlare in Italiano? \"\n",
    "output = LLM(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-80b0f601-3455-4348-9d77-07050a2cb2eb',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1707746683,\n",
       " 'model': 'C:/Users/SIGNORILEG/Downloads/llamantino-2-chat-13b-hf-ita.Q3_K_S.gguf',\n",
       " 'choices': [{'text': 'ðŸ˜‰ Vuoi conoscere il nostro mondo, le',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 10, 'completion_tokens': 16, 'total_tokens': 26}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "ðŸ˜‰ Vuoi conoscere il nostro mondo, le"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, Code\n",
    "\n",
    "Markdown(output['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7096.77 ms\n",
      "llama_print_timings:      sample time =       0.91 ms /     4 runs   (    0.23 ms per token,  4415.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =   72135.58 ms /   299 tokens (  241.26 ms per token,     4.14 tokens per second)\n",
      "llama_print_timings:        eval time =    1617.87 ms /     3 runs   (  539.29 ms per token,     1.85 tokens per second)\n",
      "llama_print_timings:       total time =   73773.20 ms /   302 tokens\n"
     ]
    }
   ],
   "source": [
    "output = LLM(\n",
    "      \"Q: Qual'Ã¨ il nome della terza colonna? '| Unnamed: 0 | Unnamed: 1 | TruthfulQA â†‘ | ToxiGen â†“ | |:-------------|:-------------|:---------------|:------------| | MPT | 7B 30B | 29.13 35.25 | 22.3222.61 | | Falcon | 7B 40B | 25.95 40.39 | 14.5323.44 | | | 7B | 27.42 | 23.00 | | Llama 1 | 13B 33B | 41.74 44.19 | 23.0822.57 | | | 65B | 48.71 | 21.77 | | | 7B | 33.29 | 21.25 | | Llama 2 | 13B 34B | 41.86 43.45 | 26.1021.19 | | | 70B | 50.18 | 24.60 |' A: \", # Prompt\n",
    "      max_tokens=None, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      echo=True # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Q: Qual'Ã¨ il nome della terza colonna? '| Unnamed: 0 | Unnamed: 1 | TruthfulQA â†‘ | ToxiGen â†“ | |:-------------|:-------------|:---------------|:------------| | MPT | 7B 30B | 29.13 35.25 | 22.3222.61 | | Falcon | 7B 40B | 25.95 40.39 | 14.5323.44 | | | 7B | 27.42 | 23.00 | | Llama 1 | 13B 33B | 41.74 44.19 | 23.0822.57 | | | 65B | 48.71 | 21.77 | | | 7B | 33.29 | 21.25 | | Llama 2 | 13B 34B | 41.86 43.45 | 26.1021.19 | | | 70B | 50.18 | 24.60 |' A: 7B."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, Code\n",
    "\n",
    "Markdown(output['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2070.97 ms\n",
      "llama_print_timings:      sample time =      33.71 ms /   198 runs   (    0.17 ms per token,  5872.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =   60518.20 ms /   292 tokens (  207.25 ms per token,     4.82 tokens per second)\n",
      "llama_print_timings:        eval time =  105937.58 ms /   197 runs   (  537.75 ms per token,     1.86 tokens per second)\n",
      "llama_print_timings:       total time =  166978.45 ms /   489 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-5aababb8-0ee3-4257-a34e-7c1bc4d4822e',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1707325070,\n",
       " 'model': 'C:/Users/SIGNORILEG/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-GGUF/snapshots/b106d1c018ac999af9130b83134fb6b7c5331dea/llama-2-13b.Q4_0.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': \"\\n[INST] <<SYS>>\\nYou are an useful assistant!\\n<</SYS>> What is this table? '| Unnamed: 0 | Unnamed: 1 | TruthfulQA â†‘ | ToxiGen â†“ | |:-------------|:-------------|:---------------|:------------| | MPT | 7B 30B | 29.13 35.25 | 22.3222.61 | | Falcon | 7B 40B | 25.95 40.39 | 14.5323.44 | | | 7B | 27.42 | 23.00 | | Llama 1 | 13B 33B | 41.74 44.19 | 23.0\"},\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 314,\n",
       "  'completion_tokens': 198,\n",
       "  'total_tokens': 512}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLM.create_chat_completion(\n",
    "      messages = [\n",
    "          {\"role\": \"system\", \"content\": \"You are an useful assistant!\"},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": \"What is this table? '| Unnamed: 0 | Unnamed: 1 | TruthfulQA â†‘ | ToxiGen â†“ | |:-------------|:-------------|:---------------|:------------| | MPT | 7B 30B | 29.13 35.25 | 22.3222.61 | | Falcon | 7B 40B | 25.95 40.39 | 14.5323.44 | | | 7B | 27.42 | 23.00 | | Llama 1 | 13B 33B | 41.74 44.19 | 23.0822.57 | | | 65B | 48.71 | 21.77 | | | 7B | 33.29 | 21.25 | | Llama 2 | 13B 34B | 41.86 43.45 | 26.1021.19 | | | 70B | 50.18 | 24.60 |'\"\n",
    "          }\n",
    "      ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-2eaaa3ea-fcda-406b-a915-69ea3e554e00', 'object': 'chat.completion', 'created': 1707324632, 'model': 'C:/Users/SIGNORILEG/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-GGUF/snapshots/b106d1c018ac999af9130b83134fb6b7c5331dea/llama-2-13b.Q4_0.gguf', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"\\n[INST] <<SYS>>\\nYou are an useful assistant!\\n<</SYS>> Can you read this table? '| Unnamed: 0 | Unnamed: 1 | TruthfulQA â†‘ | ToxiGen â†“ | |:-------------|:-------------|:---------------|:------------| | MPT | 7B 30B | 29.13 35.25 | 22.3222.61 | | Falcon | 7B 40B | 25.95 40.39 | 14.5323.44 | | | 7B | 27.42 | 23.00 | | Llama 1 | 13B 33B | 41.74 44.19 | 23\"}, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 315, 'completion_tokens': 197, 'total_tokens': 512}}\n"
     ]
    }
   ],
   "source": [
    "print(chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Markdown expects text, not {'id': 'chatcmpl-2eaaa3ea-fcda-406b-a915-69ea3e554e00', 'object': 'chat.completion', 'created': 1707324632, 'model': 'C:/Users/SIGNORILEG/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-GGUF/snapshots/b106d1c018ac999af9130b83134fb6b7c5331dea/llama-2-13b.Q4_0.gguf', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"\\n[INST] <<SYS>>\\nYou are an useful assistant!\\n<</SYS>> Can you read this table? '| Unnamed: 0 | Unnamed: 1 | TruthfulQA â†‘ | ToxiGen â†“ | |:-------------|:-------------|:---------------|:------------| | MPT | 7B 30B | 29.13 35.25 | 22.3222.61 | | Falcon | 7B 40B | 25.95 40.39 | 14.5323.44 | | | 7B | 27.42 | 23.00 | | Llama 1 | 13B 33B | 41.74 44.19 | 23\"}, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 315, 'completion_tokens': 197, 'total_tokens': 512}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display, Markdown, Code\n\u001b[1;32m----> 3\u001b[0m \u001b[43mMarkdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SIGNORILEG\\OneDrive - Links S.p.A\\Documenti\\GitHub\\RAG-with-LLamaIndex-Open\\.venv\\lib\\site-packages\\IPython\\core\\display.py:328\u001b[0m, in \u001b[0;36mDisplayObject.__init__\u001b[1;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreload()\n\u001b[1;32m--> 328\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SIGNORILEG\\OneDrive - Links S.p.A\\Documenti\\GitHub\\RAG-with-LLamaIndex-Open\\.venv\\lib\\site-packages\\IPython\\core\\display.py:407\u001b[0m, in \u001b[0;36mTextDisplayObject._check_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 407\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expects text, not \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata))\n",
      "\u001b[1;31mTypeError\u001b[0m: Markdown expects text, not {'id': 'chatcmpl-2eaaa3ea-fcda-406b-a915-69ea3e554e00', 'object': 'chat.completion', 'created': 1707324632, 'model': 'C:/Users/SIGNORILEG/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-GGUF/snapshots/b106d1c018ac999af9130b83134fb6b7c5331dea/llama-2-13b.Q4_0.gguf', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"\\n[INST] <<SYS>>\\nYou are an useful assistant!\\n<</SYS>> Can you read this table? '| Unnamed: 0 | Unnamed: 1 | TruthfulQA â†‘ | ToxiGen â†“ | |:-------------|:-------------|:---------------|:------------| | MPT | 7B 30B | 29.13 35.25 | 22.3222.61 | | Falcon | 7B 40B | 25.95 40.39 | 14.5323.44 | | | 7B | 27.42 | 23.00 | | Llama 1 | 13B 33B | 41.74 44.19 | 23\"}, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 315, 'completion_tokens': 197, 'total_tokens': 512}}"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, Code\n",
    "\n",
    "Markdown(chat.completion['choices'][0]['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
