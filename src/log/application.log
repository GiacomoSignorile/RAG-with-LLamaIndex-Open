2024-04-02 11:02:00,799 - INFO - Tesi_Giacomo_Signorile (8).pdf
2024-04-02 11:02:02,341 - INFO - [DetectionCheckpointer] Loading from model/model_final.pth ...
2024-04-02 11:02:02,341 - INFO - [Checkpointer] Loading from model/model_final.pth ...
2024-04-02 11:02:02,987 - WARNING - The checkpoint state_dict contains keys that are not used by the model:
  [35mpixel_mean[0m
  [35mpixel_std[0m
2024-04-02 11:02:10,871 - INFO - 1
2024-04-02 11:02:10,872 - INFO - No tables
2024-04-02 11:02:11,331 - INFO - 2
2024-04-02 11:02:11,331 - INFO - No tables
2024-04-02 11:02:11,783 - INFO - 3
2024-04-02 11:02:11,783 - INFO - No tables
2024-04-02 11:02:12,233 - INFO - 4
2024-04-02 11:02:12,233 - INFO - No tables
2024-04-02 11:02:12,686 - INFO - 5
2024-04-02 11:02:12,686 - INFO - No tables
2024-04-02 11:02:13,147 - INFO - 6
2024-04-02 11:02:13,148 - INFO - No tables
2024-04-02 11:02:13,599 - INFO - 7
2024-04-02 11:02:13,599 - INFO - No tables
2024-04-02 11:02:14,062 - INFO - 8
2024-04-02 11:02:14,062 - INFO - No tables
2024-04-02 11:02:14,509 - INFO - 9
2024-04-02 11:02:14,509 - INFO - No tables
2024-04-02 11:02:14,967 - INFO - 10
2024-04-02 11:02:14,968 - INFO - No tables
2024-04-02 11:02:15,411 - INFO - 11
2024-04-02 11:02:15,411 - INFO - No tables
2024-04-02 11:02:15,870 - INFO - 12
2024-04-02 11:02:15,870 - INFO - No tables
2024-04-02 11:02:16,318 - INFO - 13
2024-04-02 11:02:16,318 - INFO - No tables
2024-04-02 11:02:16,761 - INFO - 14
2024-04-02 11:02:16,761 - INFO - No tables
2024-04-02 11:02:17,210 - INFO - 15
2024-04-02 11:02:17,210 - INFO - No tables
2024-04-02 11:02:17,657 - INFO - 16
2024-04-02 11:02:17,657 - INFO - No tables
2024-04-02 11:02:18,105 - INFO - 17
2024-04-02 11:02:18,105 - INFO - No tables
2024-04-02 11:02:18,557 - INFO - 18
2024-04-02 11:02:18,557 - INFO - No tables
2024-04-02 11:02:19,017 - INFO - 19
2024-04-02 11:02:19,017 - INFO - No tables
2024-04-02 11:02:19,479 - INFO - 20
2024-04-02 11:02:19,480 - INFO - No tables
2024-04-02 11:02:19,918 - INFO - 21
2024-04-02 11:02:19,918 - INFO - No tables
2024-04-02 11:02:20,359 - INFO - 22
2024-04-02 11:02:20,360 - INFO - No tables
2024-04-02 11:02:20,809 - INFO - 23
2024-04-02 11:02:20,809 - INFO - No tables
2024-04-02 11:02:21,258 - INFO - 24
2024-04-02 11:02:21,259 - INFO - No tables
2024-04-02 11:02:21,719 - INFO - 25
2024-04-02 11:02:21,719 - INFO - No tables
2024-04-02 11:02:22,193 - INFO - 26
2024-04-02 11:02:22,193 - INFO - No tables
2024-04-02 11:02:22,674 - INFO - 27
2024-04-02 11:02:22,674 - INFO - No tables
2024-04-02 11:02:23,130 - INFO - 28
2024-04-02 11:02:23,130 - INFO - No tables
2024-04-02 11:02:23,593 - INFO - 29
2024-04-02 11:02:23,593 - INFO - No tables
2024-04-02 11:02:24,065 - INFO - 30
2024-04-02 11:02:24,065 - INFO - No tables
2024-04-02 11:02:24,527 - INFO - 31
2024-04-02 11:02:24,527 - INFO - No tables
2024-04-02 11:02:24,988 - INFO - 32
2024-04-02 11:02:24,988 - INFO - No tables
2024-04-02 11:02:25,448 - INFO - 33
2024-04-02 11:02:25,448 - INFO - No tables
2024-04-02 11:02:25,915 - INFO - 34
2024-04-02 11:02:25,915 - INFO - No tables
2024-04-02 11:02:26,367 - INFO - 35
2024-04-02 11:02:26,367 - INFO - No tables
2024-04-02 11:02:26,824 - INFO - 36
2024-04-02 11:02:26,824 - INFO - No tables
2024-04-02 11:02:27,287 - INFO - 37
2024-04-02 11:02:27,288 - INFO - No tables
2024-04-02 11:02:27,753 - INFO - 38
2024-04-02 11:02:27,753 - INFO - No tables
2024-04-02 11:02:28,227 - INFO - 39
2024-04-02 11:02:28,227 - INFO - No tables
2024-04-02 11:02:28,694 - INFO - 40
2024-04-02 11:02:28,694 - INFO - No tables
2024-04-02 11:02:29,163 - INFO - 41
2024-04-02 11:02:29,163 - INFO - No tables
2024-04-02 11:02:29,630 - INFO - 42
2024-04-02 11:02:29,630 - INFO - No tables
2024-04-02 11:02:30,087 - INFO - 43
2024-04-02 11:02:30,087 - INFO - No tables
2024-04-02 11:02:30,550 - INFO - 44
2024-04-02 11:02:30,550 - INFO - No tables
2024-04-02 11:02:31,005 - INFO - 45
2024-04-02 11:02:31,005 - INFO - No tables
2024-04-02 11:02:31,459 - INFO - 46
2024-04-02 11:02:31,460 - INFO - No tables
2024-04-02 11:02:31,917 - INFO - 47
2024-04-02 11:02:31,917 - INFO - No tables
2024-04-02 11:02:32,374 - INFO - 48
2024-04-02 11:02:32,374 - INFO - No tables
2024-04-02 11:02:32,829 - INFO - 49
2024-04-02 11:02:32,829 - INFO - No tables
2024-04-02 11:02:33,279 - INFO - 50
2024-04-02 11:02:33,279 - INFO - No tables
2024-04-02 11:02:33,734 - INFO - 51
2024-04-02 11:02:33,737 - INFO - 
2024-04-02 11:02:37,260 - INFO - 52
2024-04-02 11:02:37,262 - INFO - 
2024-04-02 11:02:38,429 - INFO - 53
2024-04-02 11:02:38,429 - INFO - No tables
2024-04-02 11:02:38,902 - INFO - 54
2024-04-02 11:02:38,902 - INFO - No tables
2024-04-02 11:02:39,365 - INFO - 55
2024-04-02 11:02:39,368 - INFO - Le agevolazioni sono erogate in conto interessi con un
tasso fisso pari al 0.25% annuo. Il rimborso delle stesse
avviene in unâ€™unica soluzione alla scadenza del
finanziamento (anno 5)...
SÃ¬, la scadenza Ã¨ il 30 giugno 2016. Per prese...
I criteri di valutazione utilizzati per selezi...
Aiuti agli Investimenti delle Piccole e Medie ...
Si possono ottenere agevolazioni per investime...
lla relativa a Domande e Risposte generate dal modello
esenza ricorrente di frasi ripetute in forma di loop nelle ri-
n parametro denominato frequency_penalty nellâ€™ambito del-
metro agisce come un metodo di controllo sulla ripetitivitÃ ,
cun token generato. Un intervallo ragionevole per il valo-
da 0.1 a 1. Un incremento oltre 2 potrebbe compromettere
Ã  della risposta.
videnzia la presenza di completamenti molto estesi, superan-
oken. Questa caratteristica Ã¨ il risultato diretto del processo
il quale il modello ha appreso a rispondere alle domande se-
sempi forniti. Va sottolineato che tali esempi presentavano
ca 1400 token.
uned con 6 epoche
eseguito un confronto del comportamento del secondo mo-
o la completion che include il testo non migliorato. Questa
suddiviso per ciascun prompt correlato.

2024-04-02 11:02:40,423 - INFO - 56
2024-04-02 11:02:40,423 - INFO - No tables
2024-04-02 11:02:40,874 - INFO - 57
2024-04-02 11:02:40,874 - INFO - No tables
2024-04-02 11:02:41,340 - INFO - 58
2024-04-02 11:02:41,340 - INFO - No tables
2024-04-02 11:02:41,791 - INFO - 59
2024-04-02 11:02:41,791 - INFO - No tables
2024-04-02 11:02:42,255 - INFO - 60
2024-04-02 11:02:42,255 - INFO - No tables
2024-04-02 11:02:42,722 - INFO - 61
2024-04-02 11:02:42,722 - INFO - No tables
2024-04-02 11:02:43,176 - INFO - 62
2024-04-02 11:02:43,176 - INFO - No tables
2024-04-02 11:02:43,638 - INFO - 63
2024-04-02 11:02:43,638 - INFO - No tables
2024-04-02 11:02:44,091 - INFO - 64
2024-04-02 11:02:44,092 - INFO - No tables
2024-04-02 11:02:44,554 - INFO - 65
2024-04-02 11:02:44,554 - INFO - No tables
2024-04-02 11:02:45,009 - INFO - 66
2024-04-02 11:02:45,009 - INFO - No tables
2024-04-02 11:02:45,475 - INFO - 67
2024-04-02 11:02:45,476 - INFO - No tables
2024-04-02 11:02:45,942 - INFO - 68
2024-04-02 11:02:45,942 - INFO - No tables
2024-04-02 11:02:46,396 - INFO - 69
2024-04-02 11:02:46,396 - INFO - No tables
2024-04-02 11:02:46,863 - INFO - 70
2024-04-02 11:02:46,863 - INFO - No tables
2024-04-02 11:02:47,325 - INFO - 71
2024-04-02 11:02:47,326 - INFO - No tables
2024-04-02 11:02:47,791 - INFO - 72
2024-04-02 11:02:47,791 - INFO - No tables
2024-04-02 11:02:47,838 - INFO - 124
2024-04-02 11:02:47,839 - INFO - length : 1019 text splitted : ['UNIVERSITÃ€ DEGLI STUDI DI BARI "ALDO MORO" DIPARTIMENTO DI INFORMATICA CORSO DI LAUREA IN INFORMATICA TESI DI LAUREA IN SISTEMI AD AGENTI TECNICHE DI FINE-TUNING DI LLM PER IL QUESTION-ANSWERING SUI BANDI DELLA REGIONE PUGLIA Relatore: Prof. Berardina Nadja DE CAROLIS Laureando: Giacomo SIGNORILE matr. 704897 ANNO ACCADEMICO 2022/2023\nII Â«Artificial intelligence is not just another big invention, but the mother of all inventions, capable of spawning a cascade of new technologies that redefine what is possible.Â» Kevin Kelly\nIII Indice Elenco delle figure VII Elenco delle tabelle IX Introduzione 1 1 Introduzione ai Large Language Models (LLMs) 3 1.1 Breve Storia e Evoluzione degli LLM . . . . . . . . . . . . . . . . . . . . . . 3 1.2 Caratteristiche principali degli LLMs . . . . . . . . . . . . . . . . . . . . . . 4 1.2.1 Tokenizzazione . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.3 Architettura . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.3.1 Meccanismi di', 'Attention . . . . . . . . . . . . . . . . . . . . . . . . . 6 1.4 Implicazioni e Applicazioni . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 1.5 LLMs vs Chat Model vs Embedding Models . . . . . . . . . . . . . . . . . 8 1.5.1 LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 1.5.2 Chat Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 1.5.3 Embeddings Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 1.6 Sfide e Considerazioni Future . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2 Full Fine-Tuning 11 2.1 Full Fine-tuning in 6 step . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.1.1 Cosa rende il full fine-tuning vantaggioso? . . . . . . . . . . . . . . 12 2.1.2 Gli svantaggi del full fine-tuning . . . . . . . . . . . . . . . . . . . . 13 2.2 Nuove tecniche e metodologie . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.2.1 Tecniche di Data Augmentation per il miglioramento del fine-tuning 13 2.2.2', 'Paraphrasing Automatico . . . . . . . . . . . . . . . . . . . . . . . . 14 2.2.3 Generazione Sintetica di Testi . . . . . . . . . . . . . . . . . . . . . . 14 2.3 Fine-Tuning di ChatGPT: Ottimizzazione dei Modelli OpenAI . . . . . . . 14 2.3.1 Fasi del Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 2.3.2 Modelli Abilitati per il Fine-Tuning . . . . . . . . . . . . . . . . . . 15 2.3.3 Quando Utilizzare il Fine-Tuning . . . . . . . . . . . . . . . . . . . . 15 2.3.4 Casi Comuni di Utilizzo del Fine-Tuning . . . . . . . . . . . . . . . 16 2.3.5 Preparazione del Dataset per il Fine-Tuning . . . . . . . . . . . . . . 16 2.3.6 Gestione dei Limiti di Token . . . . . . . . . . . . . . . . . . . . . . 16 2.3.7 Verifica della Formattazione dei Dati . . . . . . . . . . . . . . . . . . 16\nIV 3 Parameter-efficient fine-tuning (PEFT) 17 3.1 LoRA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 3.2 Vantaggi di LoRA . . . . . . . . . . . . . . . . . . . . . . . . . .', '. . . . . . . 18 3.3 Metodo VeRA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 3.3.1 Avanzamenti rispetto a LoRA . . . . . . . . . . . . . . . . . . . . . . 18 3.4 Dove PEFT batte il Full Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . 19 4 Prompt-Engineering 21 4.1 Prompting Zero-shot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 4.2 Prompting Few-shot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 4.3 Prompting Chain-of-thought . . . . . . . . . . . . . . . . . . . . . . . . . . 22 4.4 Limitazioni e Sfide . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 4.5 Applicazioni nel progetto . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 5 Retrieval Augmented Generation(RAG) 25 5.1 Importanza nel Contesto Attuale dei Modelli di Linguaggio . . . . . . . . 25 5.2 Come funziona la RAG? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 5.3 Casi dâ€™uso RAG . . . . . . . . . . . . . . . . . . . . . . . . . . .', '. . . . . . . 26 5.4 Vantaggi della RAG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 5.5 Limitazioni della RAG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 6 Scelta del giusto approccio per lâ€™applicazione 29 6.1 Costo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 6.1.1 Prompt-Engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 6.1.2 RAG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 6.1.3 PEFT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 6.1.4 Full fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 6.2 ComplessitÃ  dellâ€™implementazione . . . . . . . . . . . . . . . . . . . . . . . 30 6.2.1 Prompt-Engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 6.2.2 RAG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 6.2.3 PEFT e Full fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . 30 6.3', 'Accuratezza . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 6.3.1 Terminologia specifica del dominio . . . . . . . . . . . . . . . . . . 31 6.4 Approccio Implementato . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 7 Progetto: sviluppo su notebook 33 7.1 Raccolta dei Documenti . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 7.2 Estrazione del Testo dai PDF . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 7.2.1 Libreria PyPDF2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 7.3 Organizzazione del Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 7.4 Gestione dei Documenti di Grandi Dimensioni . . . . . . . . . . . . . . . . 35 7.4.1 Libreria LangChain . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 7.4.2 Text Splitter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\nV 7.4.3 Generazione Automatica del Dataset con lâ€™API di OpenAI . . . . . 38 7.5 Fine-Tuning: Procedura e Risultati Metrici . . .', '. . . . . . . . . . . . . . . . 40 7.6 Evaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 7.6.1 Davinci-002 fine-tuned con 4 epoche . . . . . . . . . . . . . . . . . . 43 7.6.2 Davinci-002 fine-tuned con 6 epoche . . . . . . . . . . . . . . . . . 45 7.6.3 Fine-Tuning di GPT-3.5 Turbo . . . . . . . . . . . . . . . . . . . . . . 47 7.6.4 Confronto RAG e Fine-Tuning nel chatbot sui Bandi . . . . . . . . . 49 Pinecone . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 7.6.5 Query Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 8 Applicazione chatbot: BandiPugliaBot 51 8.1 Streamlit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 8.2 Classe chatbt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 8.3 Questionario Valutazione UsabilitÃ  del Chatbot . . . . . . . . . . . . . . . . 56 Conclusioni 57 Implicazioni e Riflessioni . . . . . . . . . . . . . . . . . . . . . . . . 57', 'Sviluppi Futuri . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 Bibliografia 61\n\nVII Elenco delle figure 2.1 Fine-Tuning flow : Figura presa da altro autore [8] . . . . . . . . . . . . . . 11 7.1 Istogramma che raffigura la frequenza del numero della lunghezza di to- ken nei chunk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 8.1 Streamlit Interface per BandiPugliaBot . . . . . . . . . . . . . . . . . . . . . 53\n\nIX Elenco delle tabelle 7.1 Tabella relativa alla stima dei costi di Fine-Tuning . . . . . . . . . . . . . . 41 7.2 Tabella relativa alle metriche di addestramento del fine-tuning di OpenAI 42 7.3 Tabella che rappresenta il confronto tra le risposte date dal modello davinci- 002 con 4 epoche . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 7.4 Tabella relativa a Domande e Risposte generate dal modello . . . . . . . . 45 7.5 Tabella contenente prompt e risposta del modello fine-tuned con 6 epoche con completion non migliorata . . . . . . .', '. . . . . . . . . . . . . . . . . . 46 7.6 Tabella con risposte generate dalla Rag e dal modello fine-tuned con gpt- 3.5-turbo e davinci-002 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n\n1 Introduzione Lâ€™introduzione di questa tesi si colloca allâ€™incrocio tra la crescente attrazione per i Large Language Models (LLMs) come ChatGPT e la necessitÃ  imperativa di adattarli per affron- tare sfide specifiche e complesse nel contesto aziendale, nella vita quotidiana o in aiuto a utenti difronte a una marea di informazione indecifrabile. Gli obiettivi di questa ricerca sono quelli di migliorare alcuni problemi degli LLMs come: 1. Output Personalizzati: Alcune applicazioni richiedono strutture o stili unici, come un tool di valutazione della scrittura che fornisce un feedback conciso a punti per gli elaborati; 2. Contesto Mancante: Gli LLMs preaddestrati potrebbero non essere a conoscenza di documenti specifici cruciali per unâ€™applicazione, compromettendo lâ€™accuratezza. Ad esempio, nel caso di un', 'chatbot che risponda a domande sui bandi potrebbe avere difficoltÃ  senza lâ€™accesso ai nuovi documenti caricati ultimamente; 3. Vocabolario Specializzato: Alcuni settori, industrie e aziende utilizzano termino- logie e concetti unici non ben rappresentati nei dati di preaddestramento generale. Questo puÃ² limitare la capacitÃ  di un LLM preaddestrato nel riassumere dati fi- nanziari, interpretare documenti di ricerca medica o comprendere la terminologia presente nei bandi pubblici. Nel primo capitolo della tesi verrÃ  introdotto cosâ€™Ã¨ un Large Language e Model e nei quattro capitoli successivi verranno esplorati quattro metodi prominenti di fine-tuning o miglioramento di LLM: 1. Full Fine-tuning: Regola tutti i parametri degli LLMs utilizzando dati specifici del compito; 2. Parameter-efficient Fine-tuning (PEFT): Modifica la selezione di parametri per unâ€™a- dattamento piÃ¹ efficiente; 3. Prompt Engineering: Perfeziona lâ€™input del modello per guidare lâ€™output; 4. RAG (Retrieval Augmented Generation): Combina', 'prompt engineering con la consultazione(retrieval) del database per risposte generate dal contesto. Questi metodi differiscono in termini di competenze richieste, costi e idoneitÃ  per scenari diversi. Il progetto di tesi si Ã¨ incentrato sul modellare un sistema di risposta automatizzato, mettendo a frutto le potenzialitÃ  del modello linguistico GPT per fornire\n2 Introduzione risposte precise e contestualizzate alle domande poste dagli utenti sui bandi regionali. Gli obiettivi prefissati sono stati molteplici: personalizzare il modello GPT per adattarlo specificamente al corpus dei bandi regionali, garantire risposte immediate e pertinenti tramite un Chatbot dedicato, il tutto mantenendo una rigorosa ottimizzazione dei costi e lâ€™efficienza del processo di fine-tuning.\n3 Capitolo 1 Introduzione ai Large Language Models (LLMs) I Large Language Models (LLMs) rappresentano una classe avanzata di modelli di intelli- genza artificiale progettati per comprendere, generare e interagire con il linguaggio uma- no a', 'livelli di complessitÃ  senza precedenti. Questi modelli, come illustrato in una pa- noramica comprensiva da Naveed et al. [18], sono stati allâ€™avanguardia delle ricerche nel campo del Natural Language Processing (NLP) e sono stati progettati per comprendere, interpretare, generare e tradurre testi in linguaggio naturale. La loro "grandezza"(large) non deriva solo dalla dimensione del modello in termini di numero di parametri, ma an- che dallâ€™enrome quantitÃ  di dati su cui vengono addestrati. Questi modelli apprendono autonomamente le strutture linguistiche e le relazioni semantiche dai dati, permettendo loro di generare testi coerenti e pertinenti, rispondere a domande, riassumere documenti e molto altro. 1.1 Breve Storia e Evoluzione degli LLM Lâ€™evoluzione degli LLM ha avuto inizio con modelli relativamente semplici come Word2Vec e GloVe, che rappresentavano le parole in spazi vettoriali. Questo permetteva operazioni matematiche semplici in grado di catturare le relazioni semantiche e sintattiche tra le pa-', 'role. Tuttavia, questi primi modelli presentavano una limitazione notevole: faticavano a comprendere il contesto in cui venivano utilizzate le parole. Lâ€™introduzione dellâ€™architettura Transformer nel 2017, attraverso il paper "Attention is All You Need" di Vaswani et al. [25], ha rivoluzionato il campo dei Language Mo- dels, cambiando il modo in cui potevano elaborare il testo, consentendo cosÃ¬ agli LLM di considerare lâ€™intero contesto per generare risposte o completamenti. Questo ha portato allo sviluppo di modelli sempre piÃ¹ sofisticati come GPT (Generative Pre-trained Tran- sformer) di OpenAI, BERT (Bidirectional Encoder Representations from Transformers) di Google, e molti altri, ognuno con i propri miglioramenti e specializzazioni. Questo ha portato cosÃ¬ allo sviluppo di modelli piÃ¹ sofisticati come:\n4 Capitolo 1. Introduzione ai Large Language Models (LLMs) â€¢ GPT(Generative Pre-trained Transformer) introdotto usando un articolo intitola- to "Improving Language Understanding by Generative Pre-Training"', 'da Radford e Narasimhan [23], esso ha segnato un importante progersso per la sua capacitÃ  di generare testo coerente al contesto dopo esser stato pre-addestrato su un vasto dataset di testo. OpenAI ha sviluppato e rilasciato vesioni piÃ¹ avanzate, inclusi GPT-2 nel 2019 E GPT-3 nel 2020, ciascuna che andasse a migliorare la dimensione, la capacitÃ  e complessitÃ  di GPT. â€¢ BERT (Bidirectional Encoder Representations from Transformers) Ã¨ stato introdotto in un articolo intitolato "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", pubblicato da Devlin et al. [4], ha introdotto numerosi novita nel NLP per la sua capacitÃ  di comprendere il contesto delle parole in una frase in modo bidirezionale, ovvero osservando sia le parole che lo precedono sia le parole che seguono nella frase, ciÃ² Ã¨ ottenuto assegnando dei pesi a ciascuna parola nella frase. 1.2 Caratteristiche principali degli LLMs Gli LLMs si distinguono per la loro enorme capacitÃ  di memorizzazione e la complessitÃ  dei loro', 'meccanismi di apprendimento. Sono allenati su vasti dataset di testo che spa- ziano attraverso una vasta gamma di domini e stili, permettendo loro di catturare sottili sfumature linguistiche e varietÃ  di conoscenza umana. Come riportato da Naveed et al. [18]: â€¢ Architettura: Data lâ€™enorme dimensione degli LLMs, piccole modifiche nellâ€™archi- tettura e nelle strategie di addestramento hanno un grande impatto sulle presta- zioni e sulla stabilitÃ . Si sottolinea lâ€™importanza dei moduli architettonici chiave utilizzati in vari LLMs, che portano a migliori prestazioni, riduzione del tempo e della memoria di addestramento e maggiore stabilitÃ . â€¢ Normalizzazione dei Livelli: Ha un effetto significativo sulle prestazioni e sulla stabilitÃ  degli LLMs. La pre-norm (normalizzazione degli input piuttosto che degli output) Ã¨ piÃ¹ comune tra gli LLMs per stabilizzare lâ€™addestramento. â€¢ Attention: Ã¨ un meccanismo che permette al modello di focalizzarsi su parti spe- cifiche dellâ€™input mentre elabora o genera testo,', 'migliorando la sua capacitÃ  di comprendere e produrre linguaggio naturale in modo contestualmente rilevante. â€¢ Miscela di Esperti (MoE): Permette di scalare facilmente il modello a trilioni di parametri, attivando solo pochi esperti durante il calcolo, rendendoli efficienti dal punto di vista computazionale. â€¢ Strategie di Addestramento: Addestrare modelli su vasta scala richiede accorgi- menti per ridurre i costi di addestramento, evitare divergenze di perdita e ottenere migliori prestazioni.\n1.3. Architettura 5 â€¢ Precisione Mista: Ãˆ un metodo famoso per ridurre lâ€™uso della memoria e migliorare lâ€™efficienza di addestramento degli LLMs. â€¢ InstabilitÃ  dellâ€™Addestramento: Ãˆ un problema comune negli LLMs, dove si osser- vano divergenze o picchi di perdita durante lâ€™addestramento. â€¢ Inizializzazione dei Pesi: Gioca un ruolo cruciale nella convergenza del modello e nella stabilitÃ  dellâ€™addestramento. â€¢ Modelli Supervisionati vs Modelli Generalizzati: Sebbene i modelli generalizzati siano capaci di eseguire', 'compiti diversi con buone prestazioni, non hanno ancora superato i modelli addestrati in contesti supervisionati. â€¢ Zero-Shot vs Few-Shot: Gli LLMs si comportano bene sia in contesti zero-shot(un singolo esempio) che few-shot(un limitato numero di esempi labellati), ma la diffe- renza di prestazione tra i due Ã¨ significativa. â€¢ Encoder vs Decoder vs Encoder-Decoder: Tradizionalmente, queste architetture si comportano bene per compiti diversi. Modelli solo-encoder sono noti per modelli piÃ¹ piccoli, mentre gli LLMs sono o solo-decoder o encoder-decoder. Varie varia- zioni nellâ€™architettura e negli obiettivi di addestramento consentono a un modello di esibirsi bene in diversi contesti. Questa sezione evidenzia lâ€™importanza dellâ€™architettura, delle strategie di addestra- mento e della configurazione dei modelli per ottimizzare le prestazioni e la stabilitÃ  degli LLMs. Mostra anche come piccole modifiche possono avere impatti significativi e co- me la scelta tra diversi approcci architettonici e di addestramento', 'possa influenzare le capacitÃ  del modello in vari compiti di NLP. 1.2.1 Tokenizzazione La tokenizzazione Ã¨ essenziale per preparare i dati di input per gli LLMs, convertendo il testo in unitÃ  piÃ¹ piccole come parole o sub-parole. Tecniche come WordPiece, Byte Pair Encoding (BPE) e Unigram Language Model sono state ampiamente adottate [18]. 1.3 Architettura Nel documento di Zhao et al. [26] sono presentati diverse architecture design degli LLMs, grazie allo sviluppo dei Transformers che riescono a far sviluppare con migliardi di parametri, vengono descritte principalemnte tre architetture: â€¢ Encoder-decoder Architecture: usa due blocchi di Trnasformers, come encoder e decoder, rispettivamente, lâ€™encoder adotta dei layers di multi-head self-attention per tradurre la sequenza di input e generare la sua rappresentazione, mentre il decoder processa una cross-attention su questa rappresentazione per generare la sequenza di target.\n6 Capitolo 1. Introduzione ai Large Language Models (LLMs) â€¢ Casual Decoder', 'Architecture:Utilizza un layer di attention unidirezionale per as- sicurare che ogni token di input possa interfacciarsi con i token passati o lâ€™input stesso.I modelli di OpenAI sono sviluppati basandosi su questa architettura. â€¢ Prefix Decode Architecture: Modifica il meccanismo di masking dei Casual Deco- der per permettere unâ€™attention bidirezionale sui token del prefisso e unâ€™attention unidirezionale solo sui token generati. â€¢ Mixture-of-Expert(MoE): Questo approccio prende tutte e tre le architetture prece- denti e le estende con un sottoinsieme di pesi della rete neurale che per ogni input e attivato in maniera sparsa, permettendo di aumentare la dimensione del modello mantenendo gli stessi costi. Nuove architetture emergenti puntano a migliorare lâ€™inferenza con input lunghi, incor- porando meccanismi di aggiornamento ricorsivo, offrendo il vantaggio di generare token facendo fede allo stato precedente e alla codifica di frasi in parallelo come i Transformer, beneficiando del parallelismo delle GPU.', '1.3.1 Meccanismi di Attention Gli LLMs utilizzano lâ€™Attention per ponderare lâ€™importanza delle diverse parti dellâ€™input. Questo approccio ha notevolmente migliorato la capacitÃ  dei modelli di comprendere e generare testi complessi come discusso da Zhao et al. [26]. â€¢ Full Attention: Nel architettura Transformer originale, lâ€™attention viene applicata considerando tutte le coppie di token nella sequenza mediante lâ€™attention a prodot- to scalare normalizzato. Utilizza inoltre lâ€™attention multi-head, che proietta query, chiavi e valori in diverse "head" per catturare informazioni. Lâ€™output di ciascuna â€™headâ€™ viene poi concatenato per formare lâ€™output finale. â€¢ Sparse Attention: Affronta la sfida della complessitÃ  quadratica o(n2) dellâ€™attention completa, che diventa problematica con lunghe sequenze. Vari modelli piÃ¹ efficien- ti di Transformer sono stati proposti per ridurre questa complessitÃ . Ad esempio, la sparse attention local banded Ã¨ stata adottata in GPT-3, consentendo a ogni query di riferire solo a un', 'sottoinsieme di token in base alle loro posizioni. â€¢ Multi-query/Grouped-query Attention: In questa variante, diverse heads condivi- dono le stesse matrici di trasformazione lineare per chiavi e valori, aumentando la velocitÃ  di inferenza con un minimo sacrificio nella qualitÃ  del modello. Lâ€™attention grouped/query assegna le heads a diversi gruppi che condividono le stesse matrici di trasformazione, esplorata nel modello LLaMA 2. â€¢ Flash Attention: Ottimizza la velocitÃ  e il consumo di memoria dei moduli di at- tention su GPU, organizzando lâ€™input in blocchi e introducendo la ricomputazione necessaria per utilizzare la memoria SRAM che Ã¨ piÃ¹ veloce. La Flash Attention Ã¨ stata implementata come un kernel fuso in CUDA e integrato in PyTorch.\n1.4. Implicazioni e Applicazioni 7 â€¢ Paged Attention: Migliora lâ€™efficienza della memoria e il throughput degli LLMs distribuiti, partizionando ogni sequenza in sottosequenze. Questa tecnica aumen- ta lâ€™utilizzo della GPU e consente una condivisione efficiente della', 'memoria nel campionamento parallelo. 1.4 Implicazioni e Applicazioni Lâ€™impiego di LLMs ha trovato terreno fertile in numerosi ambiti applicativi, rivoluzio- nando il modo in cui interagiamo con la tecnologia. Dalla creazione di assistenti vir- tuali capaci di comprendere e rispondere a richieste complesse, alla generazione auto- matica di testi per la creazione di contenuti, gli LLMs stanno ridefinendo le potenzialitÃ  delle macchine di "comprendere" e utilizzare il linguaggio umano in modi sempre piÃ¹ sofisticati. â€¢ Salute e Medicina: Gli LLMs stanno trasformando lâ€™assistenza sanitaria, offren- do supporto decisionale in oncologia, migliorando la comprensione della letteratu- ra medica e assistendo nelle decisioni cliniche. Sono stati esplorati per migliorare lâ€™educazione medica e la collaborazione medico-paziente. â€¢ Educazione: Gli LLMs offrono opportunitÃ  per migliorare lâ€™istruzione, fornendo feedback personalizzato agli studenti e supportando lâ€™apprendimento delle lingue. Tuttavia, esistono sfide legate', 'alla qualitÃ  dellâ€™insegnamento e considerazioni etiche riguardo il loro impiego nellâ€™educazione. â€¢ Legge: Gli LLMs sono utilizzati per analizzare testi legali, fornire spiegazioni e sup- portare la ricerca legale. BenchÃ© promettenti, ci sono preoccupazioni riguardanti lâ€™accuratezza e la responsabilitÃ  nellâ€™uso degli LLMs in contesti legali. â€¢ Finanza: Gli LLMs trovano applicazione nellâ€™analisi finanziaria e nella consulenza, offrendo potenziali benefici in termini di efficienza e precisione. Tuttavia, la loro implementazione richiede cautela per garantire decisioni finanziarie affidabili. â€¢ Interazione Uomo-Macchina: Gli LLMs migliorano lâ€™interazione uomo-macchina, promuovendo la fiducia e lâ€™efficacia nella collaborazione. Sono esplorati per piani- ficazione di compiti e movimenti, dimostrando il potenziale nel migliorare lâ€™assi- stenza personalizzata tramite robot. â€¢ Considerazioni Energetiche e Politiche: Lâ€™impiego di LLMs solleva questioni re- lative al consumo energetico e alle politiche di', 'regolamentazione, specialmente nel contesto del deep learning in NLP. Ãˆ fondamentale affrontare queste preoccupazio- ni per garantire uno sviluppo sostenibile e responsabile degli LLMs. â€¢ Privacy e Etica: Gli LLMs pongono sfide etiche significative, incluse preoccupazio- ni sulla privacy e il rischio di generare contenuti polarizzanti o inesatti. La traspa- renza, la regolamentazione e le strategie di mitigazione sono cruciali per navigare queste questioni.\n8 Capitolo 1. Introduzione ai Large Language Models (LLMs) â€¢ Ricerca e Sviluppo Futuro: La ricerca continua a esplorare le capacitÃ  e le applica- zioni degli LLMs, con unâ€™enfasi crescente sulla sicurezza, lâ€™affidabilitÃ  e lâ€™etica. La collaborazione tra ricercatori, sviluppatori e regolatori sarÃ  vitale per realizzare il pieno potenziale degli LLMs in modo responsabile. 1.5 LLMs vs Chat Model vs Embedding Models Questi modelli, pur essendo interconnessi, servono a scopi diversi e presentano architet- ture e applicazioni uniche. La comprensione delle loro', 'differenze Ã¨ cruciale per applicare lâ€™AI in maniera efficace e innovativa. 1.5.1 LLMs Gli LLMs come GPT di OpenAI, e come discusso nel capitolo, sono capaci di generare testo e rispondere a domande contestualmente rilevanti. 1.5.2 Chat Models I Chat Models, o modelli conversazionali, sono una sottocategoria degli LLMs specifi- camente addestrati o ottimizzati per la generazione di dialoghi. Questi modelli, come GPT-3.5-Turbo quado applicato a chatbot, mirano a produrre conversazioni fluide e na- turali. Mentre gli LLMs generali possono eseguire una vasta gamma di compiti di NLP, i Chat Models sono particolamente tunati per capire le sfumature del dialogo, per gestire gli scambi conversazionali e manterene la coerenza nel corso del dialogo. 1.5.3 Embeddings Models Un embedding Ã¨ un vettore (lista) di numeri in virgola mobile. La distanza tra due vettori misura il loro grado di correlazione. Distanze piccole suggeriscono unâ€™alta correlazione mentre distanze grandi indicano una bassa correlazione. Gli Embedding', 'Models si concentrano sulla rappresentazione vettoriale delle parole o delle frasi in uno spazio continuo. Quesit modelli catturano la semantica e le relazio- ni tra parole basandosi sullâ€™occorrenza nel testo.A differenza A differenza degli LLMs e dei Chat Models, che possono generare testo o rispondere a domande, gli Embedding Models sono utilizzati principalmente per trasformare il testo in vettori che possono poi essere utilizzati per compiti come la classificazione del testo, il clustering o il recupero di informazioni. La loro forza risiede nella capacitÃ  di catturare il significato contestua- le delle parole e di utilizzare queste rappresentazioni per migliorare le prestazioni dei modelli downstream in vari compiti di NLP. Come spiegato da OpenAI [21] i text-embeddings di OpenAI misurano il grado di correlazione tra stringhe di testo. Essi sono comunemente utilizzati per:\n1.6. Sfide e Considerazioni Future 9 â€¢ Ricerca (dove i risultati sono ordinati per rivelanza rispetto a una stringa di query) â€¢', 'Clustering (dove le stringhe di testo sono raggruppate in base alla similarity) â€¢ Referenze (dove vengono consgliati gruppi di testo con stringhe correlate) â€¢ Rilevamento di anomalie(dove vengono analizzate le distribuzioni di somiglianza) â€¢ Classificazione (dove le stringhe di testo sono classificate in base alla loro etichetta piÃ¹ simile) 1.6 Sfide e Considerazioni Future Nonostante i notevoli successi, lo sviluppo e lâ€™impiego di LLMs sollevano anche impor- tanti questioni etiche e tecniche, inclusa la gestione della privacy dei dati,lâ€™allineamento dei modelli con i valori umani e la necessitÃ  di risorse considerevoli per lâ€™addestramen- to, il rischio di generazione di informazioni false o fuorvianti e la necessitÃ  di mitigare i bias nei modelli di apprendimento. La ricerca futura dovrÃ  quindi concentrarsi non solo sullâ€™espansione delle capacitÃ  degli LLMs ma anche sul garantire che il loro impiego sia responsabile e benefico per la societÃ . [26, 10].\n\n11 Capitolo 2 Full Fine-Tuning 2.1 Full Fine-tuning in', '6 step Il fine-tuning Ã¨ un processo di apprendimento automatico che consente di migliorare le prestazioni di un modello di apprendimento automatico pre-addestrato su nuovi esempi. In particolare, il fine-tuning consente di adattare il modello alle caratteristiche specifiche del nuovo compito, migliorando la sua accuratezza e robustezza. FIGURA 2.1: Fine-Tuning flow : Figura presa da altro autore [8] Nel caso del Chatbot per i Bandi in corso della Regione Puglia, per eseguire il full fine-tuning, Ã¨ necessario seguire i seguenti passi: 1. Creare il dataset: â€¢ Raccogliere tutti i documenti pdf riguardanti i bandi attivi della Regione Pu- glia, assicurandosi che ciascun articolo sia corredato del suo abstract originale. â€¢ Suddividere questa raccolta in chunk, ovvero pezzi di documenti piÃ¹ piccoli in modo da venir processati meglio. 2. Preprocessare i dati:\n12 Capitolo 2. Full Fine-Tuning â€¢ Convertire ciascun documento di ricerca in un formato adatto al modello, ov- vero seguire la struttura di addestramento, che', 'puÃ² essere un file JSON con â€™promptâ€™ e â€™completionâ€™. â€¢ Associare il contenuto di ciascun documento processato con il suo abstract corrispondente per formare coppie input-output per lâ€™addestramento super- visionato. 3. Configurare il modello: â€¢ Caricare il Large Language Model (LLM) preaddestrato (ad esempio, una ver- sione preaddestrata di GPT-4). â€¢ Decidere gli iperparametri per il fine-tuning, come il learning rate, la dimen- sione del batch e il numero di epoche, basandosi su test preliminari o cono- scenze di dominio. 4. Addestrare il modello: â€¢ Alimentare il contenuto processato al LLM come input e addestrarlo a genera- re lâ€™abstract corrispondente come output. â€¢ Monitorare le prestazioni del modello sul validation set per prevenire lâ€™over- fitting e decidere quando interrompere lâ€™addestramento o apportare eventuali aggiustamenti. 5. Valutare le prestazioni: â€¢ Una volta completato il fine-tuning, valutare le prestazioni del modello sul test set, che non ha mai visto prima. â€¢ Le metriche potrebbero', 'includere il punteggio BLEU, il punteggio ROUGE o valutazioni umane per misurare la qualitÃ  e la pertinenza degli abstract generati rispetto a quelli originali. 6. Iterare fino a ottenere prestazioni soddisfacenti: â€¢ Sulla base dei risultati della valutazione, iterare sui passaggi precedenti, even- tualmente raccogliendo ulteriori dati, regolando gli iperparametri o provando diverse configurazioni del modello per migliorare le prestazioni. 2.1.1 Cosa rende il full fine-tuning vantaggioso? â€¢ Richiede meno dati rispetto allâ€™addestramento da zero: Il full fine-tuning puÃ² es- sere efficace anche con un dataset specifico per il compito relativamente piccolo. Il LLM preaddestrato giÃ  comprende costrutti linguistici generali, e il processo di fine-tuning si concentra principalmente sullâ€™adattamento della conoscenza del mo- dello alle specificitÃ  dei nuovi dati. Un LLM preaddestrato, inizialmente addestrato\n2.2. Nuove tecniche e metodologie 13 su circa 1 trilione di token e con una solida performance generale, puÃ²', 'essere effi- cientemente fine-tunato con solo alcuni centinaia di esempi, equivalenti a diverse centinaia di migliaia di token. â€¢ Miglioramento dellâ€™accuratezza: Attraverso il fine-tuning su un dataset specifico per il compito, il LLM puÃ² cogliere le sfumature di quel particolare dominio. Que- sto Ã¨ particolarmente cruciale in settori con gergo, concetti o strutture specializzate, come documenti legali, testi medici o relazioni finanziarie. Di conseguenza, di fron- te a esempi non visti dal dominio o dal compito specifico, il modello Ã¨ probabile che effettui predizioni o generi output con maggiore accuratezza e rilevanza. â€¢ Aumento della robustezza: Il fine-tuning consente di esporre il modello a piÃ¹ esempi, in particolare casi limite o scenari meno comuni nel dataset specifico del dominio. CiÃ² rende il modello piÃ¹ preparato per gestire una vasta gamma di input senza produrre output erronei. 2.1.2 Gli svantaggi del full fine-tuning â€¢ Alti costi computazionali: Il full fine-tuning implica lâ€™aggiornamento di', 'tutti i pa- rametri di un grande modello. Con LLM su larga scala che vantano decine o cen- tinaia di miliardi di parametri, lâ€™addestramento richiede enormi quantitÃ  di poten- za di calcolo. Anche quando il dataset di fine-tuning Ã¨ relativamente piccolo, il numero di token puÃ² essere elevato e costoso da calcolare. â€¢ Requisiti di memoria sostanziali: Lavorare con modelli di grandi dimensioni puÃ² richiedere hardware specializzato, come GPU o TPU di fascia alta, con capacitÃ  di memoria significative. Questo Ã¨ spesso impraticabile per molte aziende. â€¢ Tempo e competenze intensive: Quando il modello Ã¨ molto grande, spesso Ã¨ neces- sario distribuire il calcolo su piÃ¹ GPU e nodi. CiÃ² richiede competenze adeguate. A seconda delle dimensioni del modello e del dataset, il fine-tuning puÃ² richiedere ore, giorni o addirittura settimane. 2.2 Nuove tecniche e metodologie 2.2.1 Tecniche di Data Augmentation per il miglioramento del fine-tuning Ultimamente Ã¨ cresciuta esponenzialmente la ricerca di tecniche innovative per', 'la Data Augmentation, ovvero per arricchire i dati da dare in pasto per lâ€™addestramento. Esse si sono dimostrate cruciali per superare un delle sfide piÃ¹ significative nel campo: la di- sponibilitÃ  limitata di dataset di alta qualitÃ  per lâ€™addestramento dei modelli. Nel lavoro svolto da Feng et al. [5] viene sottolineato come lâ€™adozione di strategie di Data Augmen- tation, abbia portato a miglioramenti notevoli nelle prestazioni dei modelli su compiti\n14 Capitolo 2. Full Fine-Tuning specifici. Questo approccio non solo arricchisce i set di dati esistenti ma offre anche una soluzione pratica al problema della raccolta manuale di nuovi dati. 2.2.2 Paraphrasing Automatico Il paraphrasing automatico si rivela una tecnica efficace nella Data Augmentation per la sua capacitÃ  di generare diverse varianti linguistiche di una stessa frase o paragrafo, mantenendo inalterato il significato originale. Questa tecnica migliora la robustezza dei modelli di apprendimento automatico permettendo loro di interpretare e', 'comprendere meglio le variazioni linguistiche naturali. Implementando algoritmi di paraphrasing au- tomatico, i ricercatori possono espandere in modo significativo il volume e la diversitÃ  dei dati di addestramento disponibili, migliorando cosÃ¬ lâ€™accuratezza e lâ€™affidabilitÃ  dei modelli. 2.2.3 Generazione Sintetica di Testi La generazione sintetica di testi, unâ€™altra pietra miliare nella Data Augmentation, utilizza tecniche avanzate di intelligenza artificiale per produrre testi nuovi che rispecchiano la varietÃ  e la complessitÃ  dei dati di addestramento originali. Questo metodo non solo amplia il dataset di addestramento ma introduce anche una ricchezza di scenari,esempi e contesti nuovi per i modelli, potenziando la loro capacitÃ  di generalizzazione e abilitÃ  di variare il testo generato. La generazione sintetica di testi si affida a modelli di linguaggio avanzati, che, attraverso lâ€™apprendimento dalle immense collezioni di testi disponibili, sono capaci di produrre contenuti verosimili e coerenti con gli', 'esempi di addestramento. 2.3 Fine-Tuning di ChatGPT: Ottimizzazione dei Modelli Ope- nAI [19] Il processo di Fine-Tuning rappresenta una fase essenziale nellâ€™ottimizzazione delle prestazioni dei modelli API di OpenAI. Tale procedura offre diversi vantaggi rispetto ai tradizionali prompt, consentendo: 1. Risultati di qualitÃ  superiore 2. CapacitÃ  di addestrare su un maggior numero di esempi rispetto a quanto possa contenere un prompt 3. Risparmi di token grazie a prompt piÃ¹ brevi 4. Richieste con latenza inferiore I modelli di generazione di testo di OpenAI sono preaddestrati su un vasto corpus di testo, richiedendo istruzioni e talvolta diversi esempi in un prompt per unâ€™applica- zione efficace. Questo approccio, noto come few-shot learning, dimostra come eseguire unâ€™attivitÃ .\n2.3. Fine-Tuning di ChatGPT: Ottimizzazione dei Modelli OpenAI 15 Il Fine-Tuning migliora il few-shot learning addestrando il modello su un numero maggiore di esempi, consentendo risultati migliori su unâ€™ampia gamma di compiti. Una', 'volta completato il Fine-Tuning, il numero di esempi richiesti nei prompt diminuisce, riducendo i costi e consentendo richieste con latenza inferiore. 2.3.1 Fasi del Fine-Tuning A livello pratico, il processo di Fine-Tuning coinvolge le seguenti fasi: 1. Preparazione e upload dei dati di addestramento 2. Addestramento di un nuovo modello fine-tuned 3. Valutazione dei risultati e, se necessario, ritorno alla fase 1 4. Utilizzo del modello fine-tuned 2.3.2 Modelli Abilitati per il Fine-Tuning Attualmente, il Fine-Tuning Ã¨ disponibile per modelli specifici, tra cui: â€¢ gpt-3.5-turbo-1106 (raccomandato) â€¢ gpt-3.5-turbo-0613 â€¢ babbage-002 â€¢ davinci-002 â€¢ gpt-4-0613 (sperimentale) GPT-3.5-turbo Ã¨ generalmente consigliato per la maggior parte degli utenti in termini di risultati e facilitÃ  dâ€™uso. 2.3.3 Quando Utilizzare il Fine-Tuning Il Fine-Tuning puÃ² migliorare le prestazioni dei modelli di generazione di testo OpenAI per applicazioni specifiche, ma richiede un investimento attento di tempo ed energie. Prima di', 'ricorrere al Fine-Tuning, si consiglia di esplorare strategie come ingegneria del prompt, concatenazione di prompt e chiamate di funzioni, poichÃ©: â€¢ Alcuni compiti possono sembrare inizialmente poco performanti, ma risultati mi- gliori possono essere ottenuti con prompt adeguati, evitando la necessitÃ  di Fine- Tuning. â€¢ La rapiditÃ  di feedback Ã¨ maggiore iterando su prompt rispetto al Fine-Tuning, che richiede la creazione di dataset e lâ€™esecuzione diversi batch di addestramento. â€¢ Nel caso in cui il Fine-Tuning sia ancora necessario, il prompt-engineering puÃ² contribuire a migliorare i risultati finali.\n16 Capitolo 2. Full Fine-Tuning 2.3.4 Casi Comuni di Utilizzo del Fine-Tuning Alcuni scenari comuni in cui il Fine-Tuning puÃ² migliorare i risultati includono: â€¢ Definire stile, tono, formato e altri aspetti qualitativi â€¢ Migliorare lâ€™affidabilitÃ  nella produzione di output desiderati â€¢ Correggere errori nel seguire prompt complessi â€¢ Gestire casi limite in modi specifici â€¢ Apprendere nuove abilitÃ  o compiti', 'difficili da esprimere in un prompt 2.3.5 Preparazione del Dataset per il Fine-Tuning Una volta deciso che il Fine-Tuning Ã¨ la soluzione appropriata, Ã¨ necessario preparare il dataset di addestramento. Questo deve comprendere conversazioni di dimostrazione simili a quelle con cui si chiederÃ  al modello di rispondere nella produzione. Il formato del dataset varia in base al modello selezionato, ad esempio: â€¢ Per gpt-3.5-turbo, Ã¨ richiesto un formato di chat conversazionale. â€¢ Per babbage-002 e davinci-002, Ã¨ richiesta una coppia di prompt e completamento. 2.3.6 Gestione dei Limiti di Token I limiti di token dipendono dal modello selezionato. Ad esempio, gpt-3.5-turbo-1106 ha una lunghezza massima del contesto di 4096 token per esempio di addestramento. Per stimare i costi, Ã¨ possibile utilizzare la formula fornita, considerando il costo base per 1.000 token, il numero di token nel file di input e il numero di epoche addestrate. 2.3.7 Verifica della Formattazione dei Dati Prima di eseguire il Fine-Tuning, Ã¨', 'fondamentale verificare la formattazione dei dati. A tal fine, Ã¨ stato creato uno script Python che facilita questa verifica, individuando potenziali errori, contando i token e stimando i costi del lavoro di Fine-Tuning.\n17 Capitolo 3 Parameter-efficient fine-tuning (PEFT) Negli ultimi anni, il campo del Natural Language Processing (NLP) ha assistito a una crescita nello sviluppo e nellâ€™implementazione di language model pre-trained su larga scala. Questi modelli, come GPT-3 con i suoi impressionanti 175 miliardi di parametri addestrati, si sono dimostrati altamente efficaci in varie applicazioni NLP. Dunque lâ€™uti- lizzo e lâ€™adattamento di questi enormi modelli per i diversi compiti specifici rappresenta una enorme sfida, specialmente quando si effettua un fine-tuning. La Parameter-efficient fine-tuning (PEFT) utilizza tecniche per affinare ulteriormente un modello pre-addestrato aggiornando solo un piccolo numero dei suoi parametri totali [16]. Un grande modello di linguaggio pre-addestrato su enormi', 'quantitÃ  di dati ha giÃ  appreso un ampio spettro di costrutti e conoscenze linguistiche. In breve, possiede giÃ  molte delle informazioni necessarie per molti compiti. Data la portata piÃ¹ piccola, spesso Ã¨ inutile e inefficiente regolare lâ€™intero modello. Il processo di sintonizzazione fine viene condotto su un piccolo insieme di parametri. I metodi PEFT variano nel loro approccio per determinare quali componenti del mo- dello sono addestrabili. Alcune tecniche danno prioritÃ  allâ€™addestramento di porzioni se- lezionate dei parametri del modello originale. Altre integrano e addestrano componenti aggiuntive piÃ¹ piccole, come gli adaptation layer, senza modificare la struttura originale. 3.1 LoRA LoRA, acronimo di Low-Rank Adaptation of Large Language Models, Ã¨ stato introdotto allâ€™inizio del 2023 [9]. Da allora Ã¨ diventato il metodo PEFT piÃ¹ comunemente utilizzato, aiutando aziende e ricercatori a ridurre i loro costi di fine-tuning. Utilizzando la rire- parametrizzazione, questa tecnica riduce lâ€™insieme di', 'parametri addestrabili eseguendo unâ€™approssimazione di basso rango [9]. Ad esempio, se abbiamo una matrice di pesi 100.000 x 100.000, allora per il full fine- tuning bisognerebbe aggiornare 10.000.000.000 parametri. Utilizzando LoRA, possiamo\n18 Capitolo 3. Parameter-efficient fine-tuning (PEFT) catturare tutte o la maggior parte delle informazioni cruciali utilizzando una matrice di basso rango che contiene i parametri selezionati aggiornati[9]. Per ottenere questa matrice di basso rango, possiamo rireparametrizzare la matrice di pesi originale in due matrici, A e B, ciascuna di basso rango r. La nostra nuova ma- trice di basso rango viene quindi presa come il prodotto di A e B. Se r = 2, finiamo per aggiornare (100.000 x 2) + (100.000 x 2) = 400.000 parametri invece di 10.000.000. Aggior- nando un numero molto piÃ¹ piccolo di parametri, riduciamo i requisiti computazionali e di memoria necessari per il fine-tuning [9]. 3.2 Vantaggi di LoRA â€¢ Efficienza nel cambio di compito - Creare diverse versioni del', 'modello per compiti specifici diventa piÃ¹ facile. Ãˆ possibile semplicemente memorizzare una singola co- pia dei pesi pre-addestrati e costruire molti piccoli moduli LoRA. Quando si passa da un compito allâ€™altro si sostituiscono solo le matrici, A e B, e si mantiene il LLM. Questo riduce significativamente i requisiti di archiviazione. â€¢ Meno GPU richieste - LoRA riduce i requisiti di memoria della GPU fino a 3 volte poichÃ© non calcoliamo/riaddestriamo i gradienti per la maggior parte dei parametri. â€¢ Alta precisione - Su una varietÃ  di benchmark di valutazione, le prestazioni di LoRA si dimostrano quasi equivalenti al full fine-tuning, a una frazione del costo. 3.3 Metodo VeRA VeRA, che sta per Vector-based Random Matrix Adaptation, si distingue per il suo ap- proccio unico nel ridurre il numero di parametri addestrabili, utilizzando una coppia di matrici di basso rango condivise attraverso tutti i layer del modello e apprendendo pic- coli vettori di scaling. Questa strategia consente a VeRA di ridurre il', 'numero di parametri addestrabili di un ordine di grandezza rispetto a LoRA, mantenendo allo stesso tempo prestazioni comparabili nei compiti di benchmark.[12] 3.3.1 Avanzamenti rispetto a LoRA â€¢ Riduzione del numero dei parametri significativa: VeRA utilizza matrici low-rank condivise e vettori di scaling, per ridurre in modo significativo il numero di para- metri di training, facilitando cosÃ¬ il fine-tuning efficente su hardware con risorse limitate. â€¢ Stesse prestazioni - Nonostante la significativa riduzione dei parametri, Vera man- tiene prestazioni e risultati paragonabili a metodi tradizionali di full-fine-tuning e rispetto anche alla tecnina LoRA.[12]\n3.4. Dove PEFT batte il Full Fine-Tuning 19 â€¢ VersatilitÃ  - Il metodo testato su una varietÃ  di benchmark di valutazione, ha avu- to successo nellâ€™istruction-following utilizzando il modello Llama2 7B, utilizzando invece di 7 miliardi, solamente 1.4 milioni di parametri. [12] 3.4 Dove PEFT batte il Full Fine-Tuning â€¢ Addestramento piÃ¹ efficiente e veloce', '- Meno utilizzo di parametri significa meno calcoli, che si traducono direttamente in sessioni di training piÃ¹ veloci che richiedo- no meno potenza di calcolo e meno risorse di memoria. Questo rende il fine-tuning piÃ¹ pratico in scenari con risorse limitate.[6]. â€¢ Preservazione della conoscenza del pre-training - Un pre-training su ampi dataset inocula nei modelli preziose conoscenze e capacitÃ . Con il PEFT, viene assicurato che questo bagaglio di conoscenza non vada perso quando si adatta o specializza il modello a nuovi compiti perchÃ© viene mantenuta la maggior parte o tutti i pesi del modello originale [6].\n\n21 Capitolo 4 Prompt-Engineering Il Prompt-Engineering Ã¨ emersa come una tecnica cruciale per massimizzare lâ€™efficacia dei modelli di linguaggio di grandi dimensioni (LLM), come GPT-3.5-Turbo. Attraverso lâ€™arte di formulare prompt, gli sviluppatori possono guidare i modelli a produrre risposte desiderate, sfruttando la loro capacitÃ  di generalizzare da pochi esempi e di eseguire ragionamenti di base.', 'Questo capitolo esplora le metodologie di prompting zero-shot, few-shot, e chain-of-thought, evidenziando come possono essere applicate per migliorare le prestazioni dei modelli in vari contesti. 4.1 Prompting Zero-shot Nel prompting zero-shot, il modello riceve una nuova task senza esempi precedenti. Questa tecnica sfrutta la capacitÃ  del modello di generalizzare basandosi sul suo adde- stramento pregresso. Unâ€™applicazione tipica Ã¨ lo sviluppo di chatbot di assistenza tec- nica, dove un prompt chiaro e ben formulato puÃ² guidare il modello a fornire soluzioni tecniche pertinenti, anche senza esperienza diretta nella risoluzione di quel particolare problema, come ad esempio: â€¢ Prompt: Fornisci una soluzione di assistenza tecnica basata sulla seguente preoccu- pazione dellâ€™utente. â€¢ Preoccupazione dellâ€™utente: Il mio computer non si accende. Proponendo unâ€™istruzione alla query dellâ€™utente (â€œIl mio computer non si accendeâ€), viene dato al modello il contesto per il tipo di risposta desiderata. Questo Ã¨ un modo', 'per adattare lâ€™ output per lâ€™assistenza tecnica anche senza esempi espliciti di soluzioni tecniche. Nello studio di al. [3] viene dimostrata lâ€™efficacia del prompting zero-shot nel guidare i modelli di linguaggio di grandi dimensioni come GPT-3, offrendo una panoramica delle potenzialitÃ  di questa tecnica nellâ€™elaborazione del linguaggio naturale.\n22 Capitolo 4. Prompt-Engineering 4.2 Prompting Few-shot Nel prompting few-shot, il modello viene informato con alcuni esempi prima di affron- tare un nuovo task. Questi esempi sono essenzialmente coppie di input di esempio e output del modello atteso. Nel caso si crei unâ€™app di salute che classifica i piatti in â€˜Basso contenuto di grassiâ€™ o â€˜Alto contenuto di grassiâ€™ utilizzando un modello di linguaggio. Per orientare il modello, un paio di esempi sono preposti alla query dellâ€™utente: â€¢ Classifica il seguente piatto in base al suo contenuto di grassi: Pollo alla griglia, limone, erbe. â€¢ Risposta: Basso contenuto di grassi â€¢ Classifica il seguente piatto in base al', 'suo contenuto di grassi: Mac e cheese con panna pesante e burro. â€¢ Risposta: Alto contenuto di grassi â€¢ Classifica il seguente piatto in base al suo contenuto di grassi: Toast allâ€™avocado con olio dâ€™oliva â€¢ Risposta: Informato dagli esempi nel prompt, un LLM abbastanza grande e ben addestrato risponderÃ  in modo affidabile: â€œAlto contenuto di grassi.â€ Nellâ€™articolo proposto da [1] viene discussa lâ€™importanza del contesto e degli esempi nellâ€™ottimizzare le prestazioni dei language models, evidenziando come queste tecniche possano essere utilizzate per adattare i modelli a compiti specifici. Il prompting few-shot Ã¨ un buon modo per indurre il modello a un certo formato di risposta. Ad esempio la risposta del modello si potrebbe conformare a una certa struttura, indicando anche restrizioni di lunghezza . 4.3 Prompting Chain-of-thought Il prompting chain-of-thought incoraggia il modello a scomporre un problema complesso in passaggi intermedi piÃ¹ gestibili. Insieme al prompting few-shot puÃ² migliorare le', 'prestazioni su compiti che necessitano di unâ€™analisi riflessiva. Ad esempio: â€¢ Sottraendo il numero piÃ¹ piccolo dal piÃ¹ grande in questo gruppo si ottiene un numero pari: 5, 8, 9. A: Sottraendo 5 da 9 si ottiene 4. La risposta Ã¨ Vero.\n4.4. Limitazioni e Sfide 23 â€¢ Sottraendo il numero piÃ¹ piccolo dal piÃ¹ grande in questo gruppo si ottiene un numero pari: 10, 15, 20. A: Sottraendo 10 da 20 si ottiene 10. La risposta Ã¨ Vero. Infatti, il prompting chain of thought puÃ² anche essere accoppiato con il prompting zero shot per migliorare le prestazioni su compiti che richiedono unâ€™analisi passo-passo. Se si vuole migliorare le prestazioni del modello, si potrebbe chiedergli di scomporre la soluzione passo dopo passo. Seguendo questi prompt: â€¢ Scomponi la soluzione di assistenza tecnica passo dopo passo in base alla seguente preoccupazione dellâ€™utente. Preoccupazione dellâ€™utente: Il mio computer non si accende. â€¢ Soluzione: Per una varietÃ  di applicazioni, lâ€™ingegneria del prompt di base di un LLM molto grande puÃ²', 'fornire unâ€™accuratezza â€˜sufficientemente buonaâ€™. Fornisce un metodo di adattamento economico perchÃ© Ã¨ veloce e non coinvolge grandi quantitÃ  di potenza di calcolo. Lo svantaggio Ã¨ che semplicemente non Ã¨ abbastanza accurato o robusto per i casi dâ€™uso in cui Ã¨ richiesta una conoscenza di background aggiuntiva. Nellâ€™articolo al. [2] Ã¨ illustrato come il prompting chain-of-thought, dopo aver spe- rimentato tale tecnica su tre grandi language models, ha migliorato le prestazione in unâ€™ampia gamma di operazioni di aritmetica, di prompting e di ragionamenti simboli. I guadagni ottenuti, sono stati notevoli, ad esempio il prompting di un PaLM 540B con solo otto esempi di chain-of-thought ragiunge un accuracy allâ€™avanguardia su problemi di matematica testuale, superando perfino un modello GPT-3 fine-tuned. 4.4 Limitazioni e Sfide Nonostante il potenziale del Prompt-Engineering, esistono limitazioni significative. Que- ste includono la difficoltÃ  di formulare prompt efficaci per compiti specifici e la sfida di', 'garantire che il modello generalizzi correttamente da un limitato numero di esempi. La ricerca futura Ã¨ fondamentale per affrontare queste sfide e per esplorare nuovi metodi che possano rendere i modelli ancora piÃ¹ adattabili e precisi. 4.5 Applicazioni nel progetto Questa metodologia sarÃ  utilizzata per realizzare lâ€™augmentation dei dati, ovvero per arricchire e generare dati sintetici. Impiegando un modello GPT-3 per lâ€™analisi testuale, sono stati ricercati e implementati i prompt piÃ¹ efficaci, al fine di fornire risposte precise e complete. Questo approccio mira a migliorare il testo esistente o a creare nuovi esempi pertinenti al contenuto trattato.\n\n25 Capitolo 5 Retrieval Augmented Generation(RAG) Questo capitolo introduce la Retrieval Augmented Generation(RAG), esplorando le sue origini, la sua importanza nel panorama attuale dellâ€™AI e la sua evoluzione nel tempo. La RAG Ã¨ una tecnica che combina il Prompt-Engineering con la Retrieval delle in- formazioni da fonti esterne per guidare la generazione di', 'testo di un LLM. Introdotta da ricercatori di Meta (precedentemente Facebook), la RAG offre una struttura in grado di migliorare significativamente la precisione, la rilevanza e la ricchezza delle risposte generate dai modelli.Un lavoro cha introdotto il concetto di RAG puÃ² essere trovato da quanto scritto da Lewis et al. [15] 5.1 Importanza nel Contesto Attuale dei Modelli di Linguaggio In unâ€™era dominata da unâ€™enorme quantitÃ  di informazioni digitali, la capacitÃ  di gene- rare risposte precise e contestualmente rilevanti Ã¨ piÃ¹ critica che mai. I Large Language Models, pur essendo sempre piÃ¹ capaci, spesso lottano con la generazione di contenu- ti che richiedono conoscenze aggiornate o altamente specifiche. La RAG affronta que- sta sfida integrando direttamente le informazioni esterne nel processo di generazione, permettendo ai modelli di attingere da un vasto insieme di dati forniti in tempo reale. 5.2 Come funziona la RAG? Essenzialmente,la RAG accoppia meccanismi di recupero delle informazioni con', 'modelli di generazione di testo. Il componente di recupero delle informazioni aiuta ad estrarre informazioni contestuali rilevanti da un database, e il modello di generazione di testo utilizza questo contesto aggiunto per produrre una risposta piÃ¹ precisa e "informata". Ecco come funziona: 1. Vector Database: Implementare la RAG comporta il caricamento di unâ€™insieme di dati , la creazione di vettori da esso e la memorizzazione in un Vector Database. CiÃ² consente di cercare e recuperare informazioni in modo efficiente, basandosi sulla similaritÃ  semantica tra la query dellâ€™utente e i contenuti del database.\n26 Capitolo 5. Retrieval Augmented Generation(RAG) 2. Query dellâ€™utente: La RAG inizia con la ricezione di una query dellâ€™utente nel prompt di un chatbot o un box, questa query rappresenta la domanda o lâ€™afferma- zione che lâ€™utente desidera esplorare o a cui desidera ricevere una risposta. 3. Retrieval: Una volta ricevuta la query dellâ€™utente, la chain di recupero o retrieval esamina il database vettoriale', 'per identificare frammenti di informazioni che pre- sentano somiglianze semantiche(usando ad esempio la similaritÃ  del coseno) con la query. Questi pezzi rilevanti vengono quindi utilizzati per fornire ulteriore conte- sto al LLM, consentendogli di generare una risposta piÃ¹ precisa e consapevole del contesto. 4. Concatenazione: I documenti o dati recuperati vengono concatenati con la que- ry originale in un prompt che fornisce ulteriore contesto per la generazione delle risposte. 5. Generazione di testo: Infine, il prompt arricchito viene mandato al LLM, che ge- nera una risposta finale. Questo testo non solo risponde alla query iniziale ma lo fa con un livello di precisione, dettaglio e rilevanza notevolmente migliorato grazie al contesto fornito dai dati recuperati. Lâ€™articolo "Retrieval-Augmented Generation for Large Language Models: A Survey" [7] presenta una visione completa dei paradigmi di Retrieval-Augmented Generation (RAG), che utilizza conoscenze da database esterni per migliorare lâ€™accuratezza e', 'la credi- bilitÃ  dei modelli di linguaggio, specialmente per compiti specifici. Esamina lâ€™evoluzione della RAG, dalle versioni Naive alle piÃ¹ avanzate e modulari, analizzando le tecnolo- gie allâ€™avanguardia per il recupero, la generazione e lâ€™arricchimento delle informazioni. Lâ€™articolo discute anche metriche e benchmark per valutare i modelli RAG e suggeri- sce future direzioni di ricerca, come lâ€™espansione delle multimodalitÃ  e il miglioramento dellâ€™infrastruttura RAG, inoltre cerca di ridurre il fenomeno dellâ€™hallucination. 5.3 Casi dâ€™uso RAG La RAG trova applicazione in una vasta gamma di scenari, ove la generazione di risposte accurate e contestualizzate Ã¨ cruciale. Ecco alcuni esempi significativi: â€¢ Chatbots e Assistenti Virtuali: Integrando la RAG, questi sistemi possono forni- re risposte piÃ¹ precise e informative, attingendo da unâ€™ampia base di conoscenze esterne. â€¢ Ricerca e Sintesi di Informazioni: La RAG puÃ² essere impiegata per riassumere arti- coli, rapporti di ricerca o documentazione', 'tecnica, fornendo sintesi contestualizzate e dettagliate basate su fonti affidabili. â€¢ Applicazioni in Medicina e Scienze: Nel campo medico, la RAG puÃ² aiutare a for- nire diagnosi assistite o riassunti di trattamenti basati sullâ€™ultimo stato della ricerca, migliorando lâ€™accuratezza delle informazioni fornite ai professionisti e ai pazienti.\n5.4. Vantaggi della RAG 27 La RAG Ã¨ inoltre utile quando lâ€™applicazione necessita di informazioni e documen- ti aggiornati che non facevano parte del set di addestramento del LLM. Alcuni esem- pi potrebbero includere database di notizie o applicazioni che cercano ricerche mediche associate a nuovi trattamenti. Il prompt-engineering o un LLM semplice non possono gestire questi casi a causa della finestra di contesto limitata del LLM. Attualmente, per la maggior parte dei casi dâ€™uso, non Ã¨ possibile inserire lâ€™intero set di documenti nel prompt del LLM. Lâ€™articolo â€™Development and Testing of Retrieval Augmented Generation in Large Language Models - A Case Study Reportâ€™', '[11] esplora lâ€™uso dei Large Language Models (LLMs) insieme alla Retrieval Augmented Generation (RAG) per migliorare le applica- zioni mediche, concentrandosi sulla medicina preoperatoria. Viene valutata la precisio- ne e la sicurezza delle risposte generate da LLM-RAG rispetto a quelle umane e LLM standard, usando 35 linee guida preoperatorie per la valutazione. Il metodo RAG migliora significativamente lâ€™accuratezza dei modelli LLM, rendendo- li piÃ¹ veloci e affidabili rispetto alle risposte umane per fornire istruzioni preoperatorie, dimostrando non inferioritÃ . Questo approccio offre vantaggi come lâ€™aggiornabilitÃ , la scalabilitÃ  e una base di conoscenza solida, cruciali per lâ€™implementazione sanitaria degli LLM. 5.4 Vantaggi della RAG La Retrieval Augmented Generation (RAG) offre numerosi vantaggi che la rendono una tecnologia preziosa per lâ€™evoluzione dei modelli di linguaggio e lâ€™ampliamento delle lo- ro applicazioni. Questi benefici sono particolarmente evidenti in confronto alle tecni- che di', 'generazione basate esclusivamente sullâ€™ingegneria delle istruzioni o sui modelli preaddestrati senza accesso a dati esterni. â€¢ Minimizza le Allucinazioni I modelli di linguaggio, specialmente quelli di grandi dimensioni, sono talvolta propensi a generare risposte non basate sul contesto della risposta e su informazioni reali, fenomeno noto come "allucinazioni". La RAG ridu- ce significativamente questo rischio integrando informazioni provenienti da fonti esterne verificate o inserite dallâ€™utente, assicurando che le risposte siano ancorate a dati reali e pertinenti. â€¢ Si Adatta Facilmente a Nuovi Dati In un mondo in rapida evoluzione, la capacitÃ  di aggiornarsi con nuove informazioni Ã¨ cruciale. La RAG permette ai modelli di linguaggio di rimanere aggiornati integrando le ultime pubblicazioni e dati, senza la necessitÃ  di un costoso e continuo processo di re-addestramento. â€¢ Interpretabile La tracciabilitÃ  delle informazioni Ã¨ un altro vantaggio significativo della RAG. Essendo in grado di identificare le', 'fonti delle informazioni utilizzate per\n28 Capitolo 5. Retrieval Augmented Generation(RAG) generare una risposta, gli utenti e i sviluppatori possono verificare lâ€™accuratezza e lâ€™affidabilitÃ  delle risposte fornite dal sistema. â€¢ Economico Dal punto di vista dellâ€™efficienza delle risorse, la RAG offre un modo per migliorare le prestazioni dei modelli di linguaggio senza la necessitÃ  di vasti set di dati etichettati o di complesse operazioni di addestramento. Questo puÃ² tradursi in significativi risparmi di tempo e costi. [17] Allâ€™interno di questo articolo viene comparata la RAG con la DPR(Dense Pas- sage Retrieval ) evidenziando lâ€™ottenimento di risultati ancora migliori, su question- answering. 5.5 Limitazioni della RAG La RAG Ã¨ progettata per potenziare le capacitÃ  di recupero delle informazioni di un LLM estraendo il contesto da documenti esterni. Tuttavia, in determinati casi dâ€™uso, il contesto aggiuntivo potrebbe non essere sufficiente. Se un LLM preaddestrato fatica a riassumere dati finanziari o a', 'interpretare tabelle, grafi o immagini e a trarre spunti dalla documenta- zione medica di un paziente, potrebbe un singolo documento non bastare, ma al contrario produrre ulteriore rumore. In tali casi, Ã¨ piÃ¹ probabile che il Fine-Tuning possa produrre la risposta desiderata.\n29 Capitolo 6 Scelta del giusto approccio per lâ€™applicazione Dopo aver esaminato i quattro approcci allâ€™adattamento dei Large Language Model(LLM), confrontiamoli su tre metriche importanti: complessitÃ , costo e accuratezza. 6.1 Costo Nel valutare il costo di un approccio, ha senso prendere in considerazione il costo della sua implementazione iniziale, insieme al costo di mantenimento della soluzione. Dato ciÃ², confrontiamo i costi dei quattro approcci. 6.1.1 Prompt-Engineering Il prompt-engineering ha il costo piÃ¹ basso tra i quattro approcci. Si riduce alla scrittu- ra e al test di prompt per trovare quelli che danno buoni risultati quando alimentati al LLM preaddestrato. Potrebbe anche implicare lâ€™aggiornamento dei prompt se il', 'modello preaddestrato viene periodicamente aggiornato o sostituito, ad esempio con un modello commerciale come il GPT-4 di OpenAI. 6.1.2 RAG Il costo di implementazione di RAG potrebbe essere superiore a quello del prompt- engineering. CiÃ² Ã¨ dovuto alla necessitÃ  di diversi componenti: modello di estrazioni dati, vector database, retriever e LLM pre-trained. 6.1.3 PEFT Il costo di PEFT tende ad essere superiore a quello della RAG. CiÃ² Ã¨ dovuto al fatto che il fine-tuning, anche efficiente, richiede una considerevole potenza di calcolo, tempo ed esperienza in machine learning. Inoltre, per mantenere questo approccio, sarÃ  necessario eseguire il fine-tuning periodicamente per incorporare nuovi dati rilevanti nel modello.\n30 Capitolo 6. Scelta del giusto approccio per lâ€™applicazione 6.1.4 Full fine-tuning Questo metodo Ã¨ significativamente piÃ¹ costoso rispetto a PEFT, dato che richiede ancora piÃ¹ potenza di calcolo e tempo. 6.2 ComplessitÃ  dellâ€™implementazione Dallâ€™approccio relativamente semplice del', 'prompt-engineering alle configurazioni piÃ¹ intricate della RAG e metodi di fine-tuning, la complessitÃ  puÃ² variare notevolmente. Ecco una rapida panoramica di cosa comporta ciascun metodo. 6.2.1 Prompt-Engineering Questo metodo ha una complessitÃ  di implementazione relativamente bassa. Richiede poca o nessuna programmazione. Per redigere un buon prompt e condurre esperimenti, un ingegnere delle istruzioni ha bisogno di buone competenze linguistiche, competenze di dominio e familiaritÃ  con gli approcci di apprendimento pochi tiri. 6.2.2 RAG Questo approccio ha una complessitÃ  di implementazione piÃ¹ elevata rispetto allâ€™inge- gneria delle istruzioni. Per implementare questa soluzione, sono necessarie competen- ze di codifica e di architettura. A seconda dei componenti RAG scelti, la complessitÃ  potrebbe essere molto elevata. 6.2.3 PEFT e Full fine-tuning Questi approcci sono i piÃ¹ complessi da implementare. Richiedono una profonda com- prensione di deep learning e NLP, nonchÃ© competenze in scienze dei dati per', 'modificare i pesi del modello tramite script di sintonizzazione. Ãˆ necessario considerare anche fattori come i dati di addestramento, il tasso di apprendimento, la funzione di perdita, ecc. 6.3 Accuratezza Valutare lâ€™accuratezza dei diversi approcci per lâ€™adattamento dei LLM puÃ² essere com- plesso, specialmente perchÃ© lâ€™accuratezza spesso dipende da una combinazione di me- triche distinte. La significativitÃ  di queste metriche puÃ² variare in base al caso dâ€™uso specifico. Alcune applicazioni potrebbero dare prioritÃ  al gergo specifico del dominio, mentre altre potrebbero privilegiare la capacitÃ  di risalire alla fonte della risposta del mo- dello. Per trovare lâ€™approccio piÃ¹ accurato per le tue esigenze, Ã¨ imperativo identificare le metriche di accuratezza pertinenti per la tua applicazione e confrontare le metodologie rispetto a tali criteri specifici.\n6.4. Approccio Implementato 31 6.3.1 Terminologia specifica del dominio Il fine-tuning puÃ² essere utilizzato per impartire in modo efficace ai LLM la', 'terminologia specifica del dominio. Mentre RAG Ã¨ competente nel recupero dei dati, potrebbe non catturare modelli, vocabolario e sfumature specifiche del dominio tanto bene quanto un modello sintonizzato. Per compiti che cercano una forte affinitÃ  di dominio, il fine-tuning Ã¨ la strada da percorrere. 6.4 Approccio Implementato Lâ€™approccio scelto per lo sviluppo finale del chatbot BandiPugliaBot ha integrato lâ€™impie- go del modello GPT-3.5 Turbo, opportunamente fine-tuned con un insieme selezionato di bandi della Regione Puglia, abbinato allâ€™impiego della tecnologia Retrieval Augmented Generation (RAG). Questa combinazione ha significativamente migliorato la precisio- ne delle risposte fornite dal sistema. La decisione di adottare questa strategia Ã¨ emersa dalla valutazione comparativa delle varie tecniche durante lo sviluppo del progetto. Ta- le scelta ha permesso non solo di ottimizzare le performance del sistema in termini di accuratezza e rilevanza delle informazioni fornite, ma anche di mantenere', 'lâ€™implemen- tazione allâ€™interno dei limiti di budget prefissati, dopo aver confrontato i risultati delle diverse tecniche, dimostrando lâ€™efficacia dellâ€™integrazione di queste avanzate tecnologie di intelligenza artificiale nel rispondere alle esigenze informative specifiche degli utenti interessati ai bandi della Regione Puglia.\n\n33 Capitolo 7 Progetto: sviluppo su notebook Il progetto si Ã¨ diviso in notebook per la parte di machine learning e codice chatbot per la UX. Nel notebook "01_Dataset_Building", lâ€™attenzione Ã¨ rivolta alla preparazione del dataset destinato al fine-tuning del modello davinci-002. 7.1 Raccolta dei Documenti Inizialmente, si effettua la raccolta di tutti i documenti in formato PDF relativi ai ban- di attualmente in corso, reperibili sul sito sistema.puglia.it. Questi documenti vengono successivamente scaricati e archiviati nella cartella "Documenti" del repository sul drive. 7.2 Estrazione del Testo dai PDF Per gestire lâ€™estrazione del testo dai PDF, viene impiegata la libreria', 'PyPDF2, consenten- do cosÃ¬ di creare una lista di 26 documenti. Successivamente, si procede alla creazione di un dizionario, utilizzando il titolo di ciascun documento come chiave e associandovi il testo estratto come valore. 7.2.1 Libreria PyPDF2 PyPDF2 Ã¨ una libreria Python open-source che semplifica il processo di lavoro con i file PDF. Fornisce una vasta gamma di funzionalitÃ , tra cui la lettura e la scrittura di file PDF, lâ€™estrazione di testo e metadati, la divisione e lâ€™unione di documenti, lâ€™aggiunta di filigrane, la crittografia e la decrittografia di file e altro ancora. Per gestire lâ€™estrazione del testo dai PDF nel progetto, Ã¨ stato utilizzato PyPDF2, che ha permesso di creare una lista di 26 documenti. Successivamente, si Ã¨ proceduto al- la creazione di un dizionario, utilizzando il titolo di ciascun documento come chiave e associandovi il testo estratto come valore. 1 import os 2 import PyPDF2 3 4 def carica_pdf(cartella):\n34 Capitolo 7. Progetto: sviluppo su notebook 5 6 pdf_list = [] 7', 'pdf_readers = [] 8 for file in os.listdir(cartella): 9 if file.endswith(".pdf"): 10 pdf_list.append(file) 11 pdf_reader = PyPDF2.PdfReader(os.path.join(cartella , file)) 12 pdf_readers.append(pdf_reader.pages) 13 14 return pdf_readers ,pdf_list 15 16 17 if __name__ == "__main__": 18 cartella = "./ Documenti" 19 20 pdf_readers ,pdf_list = carica_pdf(cartella) 21 22 documenti_dict = {} 23 24 for pdf , reader in zip(pdf_list , pdf_readers): 25 testo = "" 26 for i, page in enumerate(reader): 27 testo += page.extract_text () 28 documenti_dict [pdf] = testo SNIPPET 7.1: Caricamento PDF Bandi Sistema Puglia 7.3 Organizzazione del Dataset Lâ€™enumerazione dei documenti viene eseguita per garantire unâ€™organizzazione chiara e ordinata del dataset. Nel corso di questa fase, si rileva che alcuni documenti superano il limite massimo di 4096 caratteri. CiÃ² Ã¨ stato rilevato usando una funzione di tiktoken per contare tutti i numeri di token allâ€™interno di un documento. Il numero di token nei diversi modelli non Ã¨', 'direttamente proporzionale al numero di caratteri in quanto la codifica dei token Ã¨ diversa per ogni modello utilizzato. [20] 1 import tiktoken 2 def num_tokens_from_string (string: str , encoding_name : str) -> int: 3 """ Returns the number of tokens in a text string.""" 4 encoding = tiktoken.get_encoding( encoding_name ) 5 num_tokens = len(encoding.encode(string)) 6 return num_tokens SNIPPET 7.2: Funzione per contare i token decodificati\n7.4. Gestione dei Documenti di Grandi Dimensioni 35 FIGURA 7.1: Istogramma che raffigura la frequenza del numero della lun- ghezza di token nei chunk 7.4 Gestione dei Documenti di Grandi Dimensioni Per affrontare questa sfida, si adotta la libreria LangChain, impiegando la funzione di suddivisione del testo denominata â€™RecursiveCharacterTextSplitterâ€™, impostando un pa- rametro di â€™chunk_sizeâ€™ pari a 3796 caratteri, in modo da mantenere complessivamente tra prompt e completion un totale di circa 4000 caratteri. Questa scelta Ã¨ cruciale, poichÃ© il numero di caratteri', 'influisce direttamente sulle risposte del modello fine-tunato. Se gli esempi presentano una lunghezza uniforme, il modello, una volta addestrato, risponderÃ  coerentemente con risposte brevi o lunghe a seconda del pattern di lunghezza presente negli esempi di addestramento. Quindi si Ã¨ cercato di mantenere una lunghezza di caratteri pari a un massimo di 4000, incluso il prompt(la domanda) e la completion(lâ€™esempio di risposta), in quanto dovrebbero equivalere circa a 1024 token, sapendo che un prompt di Chat-GPT riesce a produrre fino ad un massimo di 4096 token(incluso il prompt), possiamo includere un â€™storicoâ€™ di 3 esempi di risposte generate come prompt, in modo da migliorare la qualitÃ  della risposta in un massimo di 1024 token. CiÃ² viene fatto anche perchÃ¨: â€¢ Gestione della memoria: I modelli di linguaggio, come GPT-3 di OpenAI, hanno un limite massimo alla lunghezza del testo che possono gestire in una singola ite- razione, noto come â€œlunghezza massima della sequenzaâ€. Per GPT-3, questo limite Ã¨ di', '4096 token. Dividendo i documenti in segmenti piÃ¹ piccoli, siamo in grado di adattarci a questo vincolo.\n36 Capitolo 7. Progetto: sviluppo su notebook â€¢ Attenzione del modello: I modelli Transformer, come GPT-3, utilizzano un mecca- nismo di attenzione che permette al modello di â€œguardare indietroâ€ ai token prece- denti nella sequenza quando genera un nuovo token. Tuttavia, lâ€™attenzione dimi- nuisce con la distanza, quindi i token allâ€™inizio di un documento lungo potrebbero avere meno impatto sui token successivi. Dividendo un documento in segmenti piÃ¹ piccoli, ogni token ha piÃ¹ probabilitÃ  di influenzare la generazione di token successivi. â€¢ Efficienza computazionale: Il fine-tuning e la RAG richiedono una grande quantitÃ  di calcolo. Dividendo i documenti in segmenti piÃ¹ piccoli, possiamo distribuire il calcolo su piÃ¹ unitÃ  di elaborazione, accelerando il processo di addestramento. â€¢ Precisione del modello: Infine, dividendo i documenti in segmenti piÃ¹ piccoli, pos- siamo ottenere una rappresentazione piÃ¹', 'precisa del documento. Questo perchÃ© i modelli di linguaggio sono addestrati su segmenti di testo di dimensioni simili durante la fase di pre-addestramento. Pertanto, lavorare con segmenti di dimen- sioni simili durante il fine-tuning o la RAG puÃ² portare a migliori prestazioni del modello. 7.4.1 Libreria LangChain LangChain Langchain [13] Ã¨ un framework progettato per semplificare la creazione di applicazioni che utilizzano modelli di linguaggio di grandi dimensioni. Ãˆ molto piÃ¹ di un semplice framework; Ã¨ un ecosistema completo composto da diversi moduli. Le librerie LangChain, disponibili sia in Python che in JavaScript, offrono interfacce e integrazioni per vari componenti. Forniscono una runtime di base per combinare questi componenti in chain e agenti, insieme a implementazioni pronte per lâ€™uso. LangChain Ã¨ stato progettato per supportare la messa in produzione dei prototipi, senza modifiche al codice, dalla chain piÃ¹ semplice â€œprompt + LLMâ€ alle chain piÃ¹ complesse. 7.4.2 Text Splitter Dunque il testo', 'Ã¨ stato suddiviso come si vede nel sequente codice: 1 r_splitter = RecursiveCharacterTextSplitter ( 2 chunk_size =3796 , 3 chunk_overlap =30, 4 separators =["\\n\\n", "\\n", "(? <=\\. )", " ", ""] 5 ) 6 for doc in documenti_dict.keys (): 7 documenti_dict [doc] = r_splitter.split_text( documenti_dict [doc]) SNIPPET 7.3: Funzione per splittare i documenti\n7.4. Gestione dei Documenti di Grandi Dimensioni 37 [14] Dove chunk_overlap che si riferisce a una sovrapposizione o sovrapponibilitÃ  tra i "chunk" o blocchi di dati. Mentre Il parametro separators riveste un ruolo chiave nella funzione RecursiveCharacterTextSplitter della libreria LangChain. Questo parametro consente di specificare un insieme di separatori che la funzione utilizzerÃ  per suddividere il testo in porzioni piÃ¹ piccole, comunemente chiamate "chunk". Quando si utilizzano gli separatori, la procedura avviene seguendo un ordine, parten- do dal primo elemento dellâ€™insieme. La funzione cerca di suddividere il testo utilizzando il primo separatore. Se la', 'dimensione del blocco risultante Ã¨ ancora eccessiva, la funzione passa al separatore successivo nella lista. Per esempio, considera un insieme di separatori come {["\\n\\n", "\\n", " ", ""]} . In questo caso, la funzione cercherÃ  di suddividere il testo usando {\\n\\n} come primo separatore. Se il blocco risultante Ã¨ ancora troppo grande, passerÃ  a {\\n} e cosÃ¬ via. Questa strategia di divisione del testo Ã¨ progettata per mantenere insieme il piÃ¹ a lungo possibile le porzioni di testo semanticamente correlate, come paragrafi e frasi. Ãˆ rilevante notare che la dimensione del blocco Ã¨ misurata in termini di numero di ca- ratteri. Inoltre, Ã¨ possibile personalizzare la funzione RecursiveCharacterTextSplitter utilizzando separatori arbitrari, forniti come parametro separato. La struttura originaria dei documenti prima della suddivisione era la seguente: 1 Il documento â€™Scheda MicroPrestito della Regione Puglia - edizione 2021.pdf â€™ ha una lista di 6289 caratteri. che corrispondono ad un numero di token 2644 2 Il', 'documento â€™Scheda NIDI - Nuove iniziative dâ€™impresa_ Strumento di ingegneria finanziaria.pdf â€™ ha una lista di 16305 caratteri. che corrispondono ad un numero di token 6771 3 Il documento â€™Aiuti agli Investimenti delle Piccole e Medie Imprese (Titolo II Capo 3 Reg_ Regionale 17 _2014)_br ___br __.pdf â€™ ha una lista di 25127 caratteri. che corrispondono ad un numero di token 10847 4 Il documento â€™Aiuti Agli Investimenti delle PMI nel Settore Turistico - Alberghiero (Titolo II capo 6 Reg_ Reg_ 17 _2014) _br ___br __.pdf â€™ ha una lista di 26391 caratteri. che corrispondono ad un numero di token 11486 Mentre dopo esser stati splittati ogni chunk ad esempio Ã¨ formato cosÃ¬ :\n38 Capitolo 7. Progetto: sviluppo su notebook 1 Il documento â€™Scheda MicroPrestito della Regione Puglia - edizione 2021.pdf â€™ ha una lista di 3674 caratteri. che corrispondono ad un numero di token 1450 2 Il documento â€™Scheda NIDI - Nuove iniziative dâ€™impresa_ Strumento di ingegneria finanziaria.pdf â€™ ha una lista di 3723 caratteri. che', 'corrispondono ad un numero di token 1431 3 Il documento â€™Aiuti agli Investimenti delle Piccole e Medie Imprese (Titolo II Capo 3 Reg_ Regionale 17 _2014)_br ___br __.pdf â€™ ha una lista di 3689 caratteri. che corrispondono ad un numero di token 1465 4 Il documento â€™Aiuti Agli Investimenti delle PMI nel Settore Turistico - Alberghiero (Titolo II capo 6 Reg_ Reg_ 17 _2014) _br ___br __.pdf â€™ ha una lista di 3715 caratteri. che corrispondono ad un numero di token 1498 7.4.3 Generazione Automatica del Dataset con lâ€™API di OpenAI Il problema associato al fine-tuning risiede nella necessitÃ  di costruire manualmente un dataset composto da esempi, ciascuno contenente una domanda e la relativa risposta. Questa procedura puÃ² risultare dispendiosa in termini di tempo, specialmente quando si gestiscono numerosi esempi, come nel nostro caso con 100 di essi. Per superare questa sfida in modo automatizzato, Ã¨ stato sfruttato lâ€™API di OpenAI utilizzando il modello "text-davinci-003", noto per la sua capacitÃ  di generare', 'comple- tamenti del contesto. Al fine di creare i prompt, ossia le domande, Ã¨ stato adottato il seguente approccio: 1 import openai 2 import json 3 import time 4 5 openai.api_key = â€™sk - VByZ1sFayj9tFERV4a3uT3BlbkFJ9sQwMSJ1Arru3LvhNH5N â€™ 6 7 data_to_save = [] 8 9 for key , value in documenti_dict .items (): 10 if isinstance(value , list): 11 for text in value: 12 prompt_text = f"Genera una domanda basata sul seguente testo :\\n {text }\\ nDomanda:" 13 14 num_tokens_input = len(prompt_text) / 4 15 print(f"Numero approssimativo di token dell â€™input: { num_tokens_input }") 16 17 response = None 18 while response is None: 19 try: 20 response = openai.Completion.create( 21 engine="text -davinci -003",\n7.4. Gestione dei Documenti di Grandi Dimensioni 39 22 prompt=prompt_text , 23 max_tokens =150 , 24 n=1, 25 stop=None , 26 temperature =0.7 27 ) 28 except openai.error. RateLimitError as e: 29 print(f"Rate limit reached. Waiting for 20 seconds ...") 30 time.sleep (20) 31 32 question = response.choices [0]. text.strip', '() 33 print(question) 34 35 data = { 36 "prompt": question + f" in riferimento al documento: â€™{key }â€™", 37 "completion": text 38 } 39 data_to_save.append(data) 40 41 time.sleep (2) 42 43 44 with open(â€™domande_e_testi .json â€™, â€™wâ€™) as json_file: 45 json.dump(data_to_save , json_file , indent =4) La risposta generata Ã¨ stata successivamente inclusa in un dizionario denominato â€œdataâ€, contenente sia il prompt generato che la completion, ovvero il testo su cui Ã¨ stata generata la domanda, con riferimento al nome del documento a cui appartiene. Il costo totale della generazione delle domande utilizzando il modello â€œtext-davinci-003â€ Ã¨ di $3.14. Pur essendo un modello piÃ¹ costoso rispetto a â€œgpt-3.5-turboâ€, offre una maggiore precisione nel testo generato, con un costo di $0.0200 per ogni 1.000 token rispetto ai $0.0015 / 1.000 token di â€œgpt-3.5-turbo-0613â€. Di seguito sono presenti alcuni esempi di prompt generati: â€¢ â€œQuali sono i codici ATECO ammissibili per le imprese confiscate? in riferimento al documento:', 'â€˜Scheda NIDI - Nuove iniziative dâ€™impresa_ Strumento di ingegneria finanziaria.pdfâ€™ â€ â€¢ â€œQual Ã¨ lâ€™agevolazione prevista per gli investimenti compresi tra 50.000 e 100.000 euro per le iniziative in continuitÃ  con imprese pre-esistenti? in riferimento al do- cumento: â€˜Scheda NIDI - Nuove iniziative dâ€™impresa_ Strumento di ingegneria finanziaria.pdfâ€™ â€ â€¢ â€œQuali spese non sono ammissibili alle agevolazioni previste dal Fondo europeo di sviluppo regionale per il periodo di programmazione 2014/2020? in riferimento al documento: â€˜Aiuti agli Investimenti delle Piccole e Medie Imprese (Titolo II Capo 3 Reg_ Regionale 17_2014)_br __br __.pdfâ€™ â€\n40 Capitolo 7. Progetto: sviluppo su notebook Successivamente, Ã¨ stato seguito un procedimento analogo migliorando la completion per ottenere una struttura simile a una risposta anzichÃ© a una porzione di testo del PDF. Di seguito Ã¨ riportato il codice per eseguire questo miglioramento: 1 import openai 2 import json 3 import time 4 5 for data in data_to_save: 6 prompt_text =', 'f"Migliora il seguente testo :\\n{data[â€™completion â€™]} rimanendo fedele alla domanda {data[â€™prompt â€™]}" 7 8 response = None 9 while response is None: 10 try: 11 response = openai.Completion.create( 12 engine="text -davinci -003", 13 prompt=prompt_text , 14 max_tokens =1024 , 15 n=1, 16 stop=None , 17 temperature =0.7 18 ) 19 except openai.error. RateLimitError as e: 20 print(openai.error. RateLimitError ) 21 print(f"Rate limit reached. Waiting for 20 seconds ...") 22 time.sleep (2) 23 24 improved_completion = response.choices [0]. text.strip () 25 print( improved_completion ) 26 # Estrai il numero di token utilizzati dalla risposta 27 number_of_tokens_used = response[â€™usage â€™][â€™total_tokens â€™] 28 print(f"Numero di token utilizzati: { number_of_tokens_used }") 29 data[â€™completion â€™] = improved_completion 30 31 time.sleep (2) Per eseguire questo lavoro appunto Ã¨ stata utilizzata la tecnica del Prompt-Engineering, discussa nei capitoli precedenti, applicata su text-davinci-003 per appunto ricercare il mi- glior', 'prompt che eseguisse al meglio e piÃ¹ efficacemente il completamento del Dataset per fine-tunare il modello. 7.5 Fine-Tuning: Procedura e Risultati Metrici Nei notebook dedicati allâ€™avvio del Fine-Tuning, sono stati inizialmente stimati i costi as- sociati ai modelli OpenAI, in particolare a davinci-002 e gpt-3.5-turbo, sui quali verrÃ  suc- cessivamente eseguito il Fine-Tuning. I costi sono suddivisi tra il costo del Fine-Tuning in sÃ© e il costo dellâ€™utilizzo del modello Fine-Tuned.\n7.5. Fine-Tuning: Procedura e Risultati Metrici 41 Complessivamente, nel corso del progetto, il Fine-Tuning Ã¨ stato eseguito per tre itera- zioni. La prima iterazione ha coinvolto il Fine-Tuning del dataset comprendente i prompt generati e le completion migliorate, con un numero di epoche pari a 4 (solitamente utiliz- zato per impostazione predefinita). Nel secondo processo di Fine-Tuning, il dataset con i prompt generati Ã¨ stato utilizzato, ma in questo caso la completion non Ã¨ stata migliorata da text-davinci-003. Il numero di', 'epoche Ã¨ stato aumentato a 6 al fine di ottenere una risposta piÃ¹ simile al modello addestrato su un dataset migliorato. Nel terzo processo, il dataset Ã¨ stato trasfor- mato nel formato JSONL e sottoposto a Fine-Tuning utilizzando la struttura conversazio- nale di "gpt-3.5-turbo". Questo modello Ã¨ stato necessario per la fase di RetrievalQA di LangChain per la RAG. Il totale dei token nellâ€™intero dataset si aggira intorno ai 120,000. Questa Ã¨ la tabella che include il costo di Fine-Tuning stimato per le 3 iterazioni dopo aver stimato il costo con il seguente codice: 1 def costo_utilizzo (texts): 2 import tiktoken 3 enc = tiktoken. encoding_for_model ("davinci -002") 4 total_tokens =(len(enc.encode(texts))) 5 print(fâ€™Totale token :{ total_tokens}â€™) 6 costo_input = (total_tokens /1000* 0.0120) 7 costo_output = (total_tokens /1000* 0.0120) 8 costo_totale = costo_input + costo_output 9 print(fâ€™Costo utilizzo Davinci -002 in $ (OpenAI): {costo_totale :.6f}â€™) 10 return costo_totale 11 costo_utilizzo_davinci =', 'costo_utilizzo (json_str)   {costo_totale :.6f}â€™) 10 return costo_totale 11 costo_utilizzo_davinci = costo_utilizzo (json_str) | Nome Model- | Costo Trai- | Costo Utilizzo | Costo Trai-.1 | Costo Totale | |----------------|---------------|------------------|-----------------|----------------| | lo | ning Stimato | del Modello | ning Effettivo | | | | | stimato | | | | davinci-002 | $1.665 | $1.665 | $1.51 | $3.165 | | con 4 epoche | | | | | | davinci-002 | $5.412 | $1.665 | $3.6 | $5.265 | | con 6 epoche | | | | | | ma completion | | | | | | non migliorata | | | | | | gpt-3.5-turbo | $2.220 | $0.665 | $1.85 | $2.515 | g TABELLA 7.1: Tabella relativa alla stima dei costi di Fine-Tuning Una volta completata la fase di stima dei costi, lâ€™effettivo processo di Fine-Tuning im- plica il caricamento del file contenente il dataset attraverso la funzione openai.File.create. Successivamente, lâ€™ID del file viene utilizzato in unâ€™altra funzione di OpenAI per avviare immediatamente il', 'Fine-Tuning, come eseguito nel codice:\n42 Capitolo 7. Progetto: sviluppo su notebook 1 import os 2 import openai 3 4 openai.api_key = os.getenv("OPENAI_API_KEY ") 5 6 c = openai.File.create( 7 file=open("./ Tesi/ fine_tune_improved_list .json", "rb"), 8 purpose=â€™fine -tune â€™ 9 ) 10 11 12 r = openai.FineTuningJob.create( training_file ="file -3 KvLZc6kKFMdNrn8Juuh2LvU ", model="davinci -002", suffix="test -Signorile", hyperparameters ={"n_epochs":4}) 13 14 jobname = "ftjob - XuAnm3MliLFIyjxoJFN0wyxC " 15 16 r = openai.FineTuningJob.retrieve(jobname) 17 e = openai.FineTuningJob.list_events(id=jobname , limit =10) Dove "training_file" rappresenta lâ€™ID del file, "model" indica il modello da sottoporre a Fine-Tuning, "suffix" consente di specificare un suffisso per il nome del modello gene- rato e "hyperparameters" permette di definire gli iperparametri per il processo di adde- stramento. La funzione di Fine-Tuning restituirÃ  un "job name" come, ad esempio, "ftjob- XuAnm3MliLFIyjxoJFN0wyxC". Attraverso la', 'funzione openai.FineTuningJob.retrieve(jobname), Ã¨ possibile ottenere lo stato del Fine-Tuning, visualizzando tutti gli iperparametri utiliz- zati nellâ€™addestramento. Inoltre, con openai.FineTuningJob.list_events(id=jobname, li- mit=10), Ã¨ possibile visualizzare gli eventi, inclusi gli step e le relative training loss del processo di addestramento Queste sono le metriche di tutti le esecuzioni di Fine-Tuning per i modelli:  cesso di addestramento Queste sono le metriche di tutti le esecuzioni di Fine-Tuning per i modelli: | Modello | Epochs | Trained tokens | Training Loss | |---------------|----------|------------------|-----------------| | Davinci-002 | 4 | 251 120 | 1.1438 | | Davinci-002 | 6 | 613 428 | 0.7629 | | Gpt-3.5-turbo | 4 | 251 120 | 1.0772 | TABELLA 7.2: Tabella relativa alle metriche di addestramento del fine- tuning di OpenAI Come possiamo notare dalla tabella eseguendo il Fine-Tuning di Davinci-002 con 4 epoche produce una Training loss molto maggiore rispetto ad', 'addestrarlo con 6 epoche, il che si traduce in un potenziale miglioramento del modello o una maggiore efficenza nellâ€™addestramento con lâ€™aumentare delle epoche, aumentando tuttavia il costo del Fine- Tuning dovuto al numero maggiore di tokens allenati. Anche GPT-3.5-Turbo a 4 epoche evidenzia una leggera previsione piÃ¹ accurata sul modello sempre con 4 epoche.\n7.6. Evaluations 43 In sintesi, la tabella fornisce una visione chiara su quanto bene ciascun modella ha appreso durante il Fine-Tuning. 7.6 Evaluations Le tabelle che seguono presentano una serie di valutazioni effettuate da tre esperti di- stinti. Ogni tabella corrisponde a unâ€™analisi dettagliata delle prestazioni dei modelli in esame, offrendo insight critici e osservazioni approfondite sulle risposte generate dai modelli ai vari prompt. 7.6.1 Davinci-002 fine-tuned con 4 epoche si procede con la valutazione dei modelli basata sulle risposte fornite ai prompt su cui sono stati addestrati. Inizialmente, viene eseguita una prova con il modello', 'fine-tuned utilizzando davinci-002 per un totale di 4 epoche. Questa valutazione viene effettuata attraverso lâ€™impiego della funzione openai.Completion.create(). Al momento dellâ€™implementazione, non essendo ancora pienamente a conoscenza del funzionamento della funzione, sono stati impostati alcuni parametri, tra cui una temperature pari a 0.5. La temperature Ã¨ un parametro che influisce sulla creativitÃ  della risposta generata dal modello. Un valore di 0.5 suggerisce un livello moderato di creativitÃ , in cui la ri- sposta Ã¨ piÃ¹ prevedibile. Valori piÃ¹ vicini a 0 tendono a produrre completamenti piÃ¹ deterministici. A temperature = 1, dopo una valutazione degli esperti, si puÃ² osservare che il mo- dello produce risposte estremamente creative, spesso prive di senso. Nel caso della tem- peratura 0.7, la prima risposta mostra frasi ripetitive in forma di loop. La risposta piÃ¹ equilibrata e coerente si ottiene a temperatura 0.5. Ãˆ da notare che, occasionalmente, le risposte a questa temperatura possono presentare', 'fenomeni di "hallucination". In tal caso, rigenerare le risposte puÃ² contribuire a ottenere una formulazione piÃ¹ coerente. Successivamente, sono state generate da text-davinci-003 dieci domande simili a: â€œQua- li sono le modalitÃ  per ottenere lâ€™agevolazione prevista dallâ€™Avviso? in riferimento al do- cumento: â€™Aiuti agli Investimenti delle Piccole e Medie Imprese (Titolo II Capo 3 Reg_ Regionale 17_2014)_br ___br __.pdfâ€™". Una volta ottenute le 10 domande, sono state sottoposte al modello per lâ€™analisi delle risposte.\n44 Capitolo 7. Progetto: sviluppo su notebook Prompt "Qual Ã¨ lâ€™agevolazione prevista per gli investimenti compresi tra 50.000 e 100.000 euro per le iniziative in continuitÃ  con imprese pre-esistenti? in riferimento al documento: â€™Scheda NIDI - Nuove iniziative dâ€™impresa_ Strumento di ingegneria finanziaria.pdfâ€™" Completion Originale "Per le iniziative in continuitÃ  con imprese pre-esistenti quali: passaggio generazio- nale, rilevamento di impresa in crisi... Lâ€™agevolazione per investimenti', 'compresi tra 50.000 e 100.000 euro Ã¨ pari al 90%: un contributo a fondo perduto pari al 50% del to- tale degli investimenti agevolati e un prestito rimborsabile pari al 50%..." Risposta Generata con Temperatura 0.7 "Sono previste agevolazioni fino al 50% dellâ€™investimento... Il finanziamento in conto capitale, a tasso agevolato, non supera il 25% dellâ€™importo complessivo del progetto..." Risposta Generata con Temperatura 0.5 "Per gli investimenti compresi tra 50.000 e 100.000 euro, lâ€™agevolazione Ã¨ pari al 25% del finanziamento concesso... La misura agevolativa massima Ã¨ pari a 60.000 euro per gli investimenti fino a 250.000 euro..." Risposta Generata con Temperatura 1 "Per riferimento alla domanda â€™Qual Ã¨ lâ€™agevolazione prevista...â€™ il Fondo europeo per lo Sviluppo Regionale (FESR), il Fondo europeo agricolo per lo Sviluppo Rurale (FESR) e il POR Puglia 2014-2020 attribuiscono unâ€™agevolazione pari al 30% dellâ€™investimen- to..." TABELLA 7.3: Tabella che rappresenta il confronto tra le risposte date dal', 'modello davinci-002 con 4 epoche\n7.6. Evaluations 45', 'presenta il confronto tra le risposte date dal modello davinci-002 con 4 epoche\n7.6. Evaluations 45 | Domanda | Risposta | |----------------------------|-----------------------------------------------------------| | Quali sono i requisiti | Per accedere allâ€™agevolazione prevista nellâ€™Avviso i | | necessari per accedere | requisiti necessari sono: etÃ  massima dellâ€™impresa al | | allâ€™agevolazione prevista | momento della presentazione della domanda di | | nellâ€™Avviso? | agevolazione pari a 10 anni per le nuove imprese e/o | | | quelle nate da ristrutturazioni o trasformazioni | | | societarie... | | Qual Ã¨ la durata | Le agevolazioni sono erogate in conto interessi con un | | dellâ€™agevolazione una | tasso fisso pari al 0.25% annuo. Il rimborso delle stesse | | volta ottenuta? | avviene in unâ€™unica soluzione alla scadenza del | | | finanziamento (anno 5)... | | Câ€™Ã¨ una scadenza per | SÃ¬, la scadenza Ã¨ il 30 giugno 2016. Per prese... | | presentare la domanda di | | | ... | | | Quali sono i criteri di | I criteri di valutazione utilizzati per selezi... | | valutazione utilizzati... | | | Qual Ã¨ lâ€™importo massimo | Aiuti agli Investimenti delle Piccole e Medie ... | | dellâ€™agevolazione che... | | | Quali sono le tipologie di | Si possono ottenere agevolazioni per investime... | | investimenti ammiss... | | TABELLA 7.4: Tabella relativa a Domande e Risposte generate dal modello Al fine di evitare la presen', 'za ricorrente di frasi ripetute in forma di loop nelle ri- sposte, Ã¨ stato introdotto un parametro denominato frequency_penalty nellâ€™ambito del- la Completion. Tale parametro agisce come un metodo di controllo sulla ripetitivitÃ , assegnando un peso a ciascun token generato. Un intervallo ragionevole per il valo- re di frequency_penalty va da 0.1 a 1. Un incremento oltre 2 potrebbe compromettere significativamente la qualitÃ  della risposta. Lâ€™analisi delle risposte evidenzia la presenza di completamenti molto estesi, superan- do frequentemente i 1000 token. Questa caratteristica Ã¨ il risultato diretto del processo di addestramento, durante il quale il modello ha appreso a rispondere alle domande se- guendo la struttura degli esempi forniti. Va sottolineato che tali esempi presentavano una lunghezza media di circa 1400 token. 7.6.2 Davinci-002 fine-tuned con 6 epoche Nel progetto viene anche eseguito un confronto del comportamento del secondo mo- dello addestrato utilizzando la completion che include il testo', 'non migliorato. Questa completion presenta il testo suddiviso per ciascun prompt correlato.\n46 Capitolo 7. Progetto: sviluppo su notebook Si procede quindi a sottoporre al nuovo modello le stesse domande giÃ  poste al mo- dello precedente: Prompt "Quali sono i requisiti necessari per accedere allâ€™agevolazione prevista nellâ€™Avviso? in riferimento al documento: â€™Aiuti agli Investimenti delle Piccole e Medie Imprese (Titolo II Capo 3 Reg_ Regionale 172014) br___br__.pd f â€²â€ Risposta 05 gennaio 2016 Pag 1 di 2 Centro Servizi Sistema Puglia www.sistema.puglia.it Avvi- so 12/2016 Aiuti agli Investimenti delle Piccole e Medie Imprese (Titolo II Capo 3 Reg. Regionale 17/2014) Le modalitÃ  di presentazione delle domande di agevolazione sono le seguenti: a) telematicamente, mediante la compilazione ed il successivo invio on line dei moduli messi a disposizione sul sito www.sistema.puglia.it , mediante lâ€™atti- vazione della procedura â€œAvviso 12/2016â€ e della specifica voce â€œPresenta domandaâ€; b) telematicamente, mediante', 'la compilazione ed il successivo invio on line dei moduli messi a disposizione sul sito www.sistema.puglia.it , mediante lâ€™attivazione della pro- cedura â€œAvviso 12/2016â€ e della specifica voce â€œScarica moduli e allegatiâ€, di seguito siglati â€œBIâ€ ed â€œAAâ€ e di quelle presenti nellâ€™Avviso, in carta semplice, in originale, pena lâ€™inefficacia degli atti, redatte ai sensi degli artt. 1340 e 1341 del codice civile e degli artt. 1414 e 1423 TABELLA 7.5: Tabella contenente prompt e risposta del modello fine-tuned con 6 epoche con completion non migliorata Dalla risposta generata Ã¨ possibile fornire alcune ulteriori osservazioni: Il modello dimostra una notevole fedeltÃ  alla struttura del testo originale presente nei documenti PDF, mantenendo nella risposta esattamente lâ€™intestazione e il numero di pa- gina. Tale accuratezza puÃ² essere un vantaggio nel caso si voglia generare un documento con la medesima struttura. La risposta generata offre una quantitÃ  considerevole di dettagli e informazioni relati- ve alle', 'modalitÃ  di presentazione delle domande di agevolazione. Tuttavia, Ã¨ opportuno valutare che la lunghezza eccessiva puÃ² influire sulla chiarezza e la concisione delle ri- sposte, tenendo conto delle preferenze degli utenti per informazioni dirette e sintetiche. Nonostante la fedeltÃ  al formato originale, Ã¨ evidente la presenza di testo non stretta- mente correlato alla domanda, dunque non risulta una risposta adatta alle esigenze di un chatbot. Dalla tabella Ã¨ possibile notare che entrambe le risposte soddisfano la domanda po- sta; tuttavia, il modello con la completion migliorata si distingue per la sua notevole capacitÃ  di fornire una risposta piÃ¹ chiara e facilmente interpretabile, risultando quindi strutturato in modo piÃ¹ efficace. In conclusione, tra i due modelli considerati, il primo risulta preferibile. Prima di sottoporre il modello al processo di Fine-Tuning, Ã¨ consigliabile migliorare il testo di esempio, anche considerando i costi associati al perfezionamento con text-davinci-003. I costi', 'complessivi sono paragonabili; infatti, il miglioramento della completion ha comportato una spesa di 3.14$, mentre lâ€™aumento delle epoche da 4 a 6 ha generato un\n7.6. Evaluations 47 costo aggiuntivo di 3$. Tali costi sono stati effettuati con la speranza di ottenere risposte di miglior qualitÃ . 7.6.3 Fine-Tuning di GPT-3.5 Turbo Per eseguire il fine-tuning di GPT-3.5 Turbo, Ã¨ stato necessario convertire il dataset pre- cedente nel formato JSONL, strutturato con {"prompt":" "} e {"completion": " "} in un JSONL piÃ¹ complesso in modo da adattarsi alle specifiche di GPT-3.5 Turbo. Il formato richiesto Ã¨ organizzato come una lista di messages, come mostrato nellâ€™esempio seguente: 1 { 2 "messages ": [ 3 {" role ": "system", "content ": "You are a helpful assistant ."}, 4 {" role ": "user", "content ": "Quale documentazione necessaria per la presentazione della domanda di agevolazione? in riferimento al documento: â€™Scheda MicroPrestito della Regione Puglia - edizione 2021. pdf â€™"}, 5 {" role ": "assistant",', '"content ": "Per presentare la domanda di agevolazione , necessario produrre i seguenti documenti: - Domanda di agevolazione firmata digitalmente ; - Dichiarazione di consenso al trattamento dei dati personali ....."} 6 ] 7 } SNIPPET 7.4: Esempio di formato JSONL per il fine-tuning di GPT-3.5 Turbo Possiamo confrontare le risposte usando lo stesso prompt tra le diverse tecniche: La risposta generata con la Retriever-Augmented Generation (RAG) fornisce infor- mazioni dettagliate sulla procedura di partecipazione. Raccomanda lâ€™iscrizione al porta- le Sistema Puglia allâ€™indirizzo https://microcredito2021.sistema.puglia.it e fornisce det- tagli su documenti necessari e ulteriori approfondimenti disponibili sul sito ufficiale https://regione.puglia.it/web/guest/bandi-e-avvisi. La risposta generata da gpt-3.5-turbo fine-tuned offre una guida piÃ¹ sintetica, eviden- ziando i passaggi chiave per partecipare allâ€™Avviso MicroPrestito della Regione Puglia. Descrive il processo di registrazione sul portale', 'www.nuvola.puglia.it, la consegna dellâ€™i- stanza, lâ€™ottenimento di un codice identificativo, la compilazione della domanda online e lâ€™attesa dellâ€™esito della valutazione. Dâ€™altra parte, la risposta generata da davinci-002 con 4 epoche Ã¨ piÃ¹ esaustiva e si con- centra sul contesto dellâ€™Avviso MicroPrestito della Regione Puglia, delineando le finalitÃ , i requisiti e le categorie di imprese eleggibili. Ãˆ evidente che ciascun modello ha un approccio unico nel generare risposte. La sin- teticitÃ  di gpt-3.5-turbo lo rende adatto a un contesto di chatbot conversazionale, mentre la profonditÃ  e la specificitÃ  di davinci-002 potrebbero essere piÃ¹ adatte a esigenze di informazioni piÃ¹ dettagliate.\n48 Capitolo 7. Progetto: sviluppo su notebook Prompt "Quale Ã¨ il modo per partecipare allâ€™Avviso MicroPrestito della Regione Puglia? In rife- rimento al documento: â€™Scheda MicroPrestito della Regione Puglia - edizione 2021.pdf" Risposta generata da GPT-3.5-Turbo fine-tuned Il modo per partecipare allâ€™Avviso MicroPrestito', 'della Regione Puglia Ã¨ il seguente: 1. Registrarsi sul portale www.nuvola.puglia.it e compilare lâ€™istanza di accesso; 2. Conse- gnare lâ€™istanza presso un Ufficio Postale o un Patronato abilitato; 3. Ottenere un codice identificativo di pratica e inserirlo nel portale NU.VO.LA.; 4. Compilare la domanda online, allegando tutta la documentazione richiesta e sottoscrivendo digitalmente; 5. Attendere lâ€™esito della valutazione; 6. In caso di esito positivo, firmare il contratto di microcredito presso una delle SoggettivitÃ  Convenzionate (banche o altri operatori fi- nanziari). Risposta generata con davinci-002 fine-tuned con 4 epoche Il MicroPrestito della Regione Puglia, in riferimento al documento â€™Scheda MicroPre- stito della Regione Puglia - edizione 2021.pdfâ€™, Ã¨ un avviso pubblico per la concessione di finanziamenti agevolati finalizzati allâ€™attuazione di progetti imprenditoriali di mi- croimprese e piccole imprese che si attesteranno tra i 25.000,00 euro e i 200.000,00 euro di spesa complessiva ed', 'impatteranno in modo significativo nel breve periodo. Lâ€™av- viso Ã¨ rivolto alle microimprese e alle piccole imprese che intendono: 1) Reinvestire i propri utili, anche non distribuiti, o una percentuale del ricavo netto dellâ€™esercizio precedente, destinata al reinvestimento in determinati beni mobili e/o immobili. con- tinua... Risposta generata con la RAG e GPT-3.5-Turbo non fine-tuned Il modo per partecipare allâ€™Avviso MicroPrestito della Regione Puglia Ã¨ pre- sentare la domanda di partecipazione direttamente dallâ€™impresa che richie- de lâ€™agevolazione, previa iscrizione al portale Sistema Puglia allâ€™indirizzo https://microcredito2021.sistema.puglia.it. La procedura di partecipazione Ã¨ in- teramente telematica. continua... TABELLA 7.6: Tabella con risposte generate dalla Rag e dal modello fine- tuned con gpt-3.5-turbo e davinci-002\n7.6. Evaluations 49 7.6.4 Confronto RAG e Fine-Tuning nel chatbot sui Bandi DopodichÃ¨ , si Ã¨ effettuato un confronto tra la tecnica RAG e il Fine-Tuning. La Retriever- Augmented', 'Generation (RAG) Ã¨ una tecnica usata per la creazione di Chatbot ottenuta unendo i vantaggi dei modelli di retrieval, che cercano chunk significativi allâ€™interno di un database chiamato Vector Store, insieme ai modelli linguaggio GPT, come gpt-3.5- turbo. Per implementarla, si Ã¨ utilizzata la libreria LangChain, che consente la suddivisione del testo e la traduzione in vettori attraverso gli embeddings di OpenAI, una funzione che traduce il testo contenuto nei chuck usando modelli come text-embeddings-002, con Pinecone come database per indicizzare i vettori e i relativi metadata. Pinecone Pinecone [22] Ã¨ un vector-database cloud,e ultimamente anche locale, progettato per mi- gliorare le applicazioni AI attraverso la memorizzazione a lungo termine tramite vector embeddings, un tipo di dato che rappresenta le informazioni semanticamente. Questi vector embeddings consentono alle applicazioni AI o agli LLM di acquisire conoscenza in modo da eseguire task complessi. Rispetto ai database scalari tradizionali che', 'non riescono a stare dietro alla complessitÃ  e alla grandezza dei dati di documenti piÃ¹ lun- ghi e elaborati, Pinecone combina le caratteristiche principali dei database tradizionali con performance ottimizzate degli index vector, supportando operazioni CRUD e query veloci su miliardi di vettori, restituendo cosÃ¬ risultati a bassa latenza e precisi, per miglio- rare ancora di piÃ¹ le performance si possono filtrare le query con namespaces(ad esempio contenuto o titolo) o metadata. Con il seguente codice vengono inseriti allâ€™interno del Database di Pinecone i vettori dei chunk dei documenti: 1 def insert_or_fetch_embeddings (index_name , chunks): 2 import pinecone 3 from langchain.vectorstores import Pinecone 4 from langchain.embeddings.openai import OpenAIEmbeddings 5 6 embeddings = OpenAIEmbeddings () 7 8 pinecone.init(api_key=os.environ.get(â€™PINECONE_API_KEY â€™), environment= os.environ.get(â€™PINECONE_ENV â€™)) 9 10 if index_name in pinecone.list_indexes (): 11 print(fâ€™Index {index_name} esiste. Loading', 'embeddings ... â€™, end=â€™â€™ ) 12 vector_store = Pinecone. from_existing_index (index_name , embeddings) 13 print(â€™Okâ€™) 14 else: 15 print(fâ€™Creazione di index {index_name} e embeddings ...â€™, end=â€™â€™) 16 pinecone.create_index(index_name , dimension =1536 , metric=â€™cosine â€™) 17 vector_store = Pinecone. from_documents (chunks , embeddings , index_name=index_name)\n50 Capitolo 7. Progetto: sviluppo su notebook 18 print(â€™Okâ€™) 19 20 return vector_store 7.6.5 Query Retrieval Dopo aver caricato i vector data su Pinecone, contenuti i chunk dei documenti dei Bandi, si puÃ² usare il seguente codice per testare la ricerca dei documenti: 1 results = vector_store. similarity_search (query) Dove results sarÃ  una lista di k Document che avranno similaritÃ  semantica rispetto alla query. La RetrievalQA Ã¨ una tecnica comune utilizzata dai chatbot per arricchire le risposte con dati al di fuori dei dati di addestramento del modello di chat utilizzando il vec- tor store di Pinecone come retriever. Per implementare la retrieval, Ã¨', 'possibile crea- re un retriever utilizzando il codice seguente come descritto nella documentazione di Langchain. 1 from langchain.chains import RetrievalQA 2 from langchain.chat_models import ChatOpenAI 3 4 llm = ChatOpenAI(model=â€™gpt -3.5 - turbo â€™, temperature = 0.7) 5 retriever = vector_store.as_retriever(search_type=â€™similarity â€™, search_kwargs ={â€™kâ€™:3}) 6 chain = RetrievalQA. from_chain_type (llm = llm , chain_type=â€™stuff â€™, retriever = retriever) Dando in input una query la chain che sarebbe una catena di azioni o di prompt, darÃ  in output dallâ€™LLM la risposta. 1 query="Di cosa si parla nel documento Scheda Efficientamento Energetico Edifici Pubblici.pdf" 2 answer = chain.run(query) Per migliorare la ricerca per indirizzare il retriever sui giusti chunk piÃ¹ velocemente possiamo inserire un metadato. 1 answer1 = chain.run(query1 , metadata = {â€™label â€™: â€™Compilazione della domanda â€™}) Una risposta esempio sarÃ : answer1 = "Per partecipare allâ€™Avviso MicroPrestito della Regione Puglia, devi presentare la', 'domanda di agevolazione tramite la procedura online "PIA Medie Imprese" disponibile sul sito www.sistema.puglia.it. Dovrai compilare il mo- dulo di istanza di accesso utilizzando la modulistica fornita e apporre la firma digitale. Lâ€™istanza di accesso deve descrivere le caratteristiche tecniche ed economiche del proget- to integrato e includere la compilazione telematica richiesta dallâ€™Avviso e prevista dalla piattaforma telematica. Saranno effettuate delle verifiche da parte di Puglia Sviluppo e successivamente la Regione adotterÃ  un provvedimento di ammissione o inammissibili- tÃ  della proposta. SarÃ  comunicato lâ€™esito dellâ€™esame e, in caso di ammissibilitÃ , saranno indicati i termini per la presentazione della documentazione progettuale."\n51 Capitolo 8 Applicazione chatbot: BandiPugliaBot Il chatbot in questione Ã¨ stato sviluppato usando la tecnica RAG in combinazione con il modello fine-tuned di GPT-3.5-Turbo. Utilizzando la libreria di Pinecone e dunque i chunk tradotti in vector data e archiviati come', 'descritto precedentemente, il chatbot dopo il prompt dellâ€™utente Ã¨ in grado di fare la retrieval di questi chunk per migliorare la risposta, tutto ciÃ² utilizzando 8.1 Streamlit Streamlit [24] Ã¨ un framework open-source che consente agli sviluppatori di creare rapi- damente applicazioni web per la data science e il machine learning con poche righe di codice Python. Ãˆ progettato per semplificare e accelerare il processo di creazione di inter- facce utente interattive, consentendo agli sviluppatori di trasformare script di analisi dati in app condivisibili. Streamlit si distingue per la sua facilitÃ  dâ€™uso, efficienza e la capacitÃ  di integrare funzionalitÃ  avanzate senza la necessitÃ  di conoscenze approfondite in web development. Il codice di Streamlit relativo al BandiPugliaBot Ã¨ il seguente: 1 # Streamlit code 2 st.title(â€™Chatbot Bandi in corso Sistema Puglia â€™) 3 chatbt_instance = chatbt () 4 5 uploaded_file = st.file_uploader ("File upload", type="pdf") 6 if uploaded_file: 7 chatbt_instance', '.load_pdf(uploaded_file ) 8 9 # messaggio di benvenuto 10 with st.chat_message(â€™assistant â€™): 11 st.write("Ciao , sono il tuo assistente personale personalizzato per rispondere a domande relative ai bandi in corso della regione Puglia presenti sul sito [link ]( https :// www.sistema.puglia.it/)!") 12 13 # Initialize chat history 14 if "messages" not in st.session_state : 15 st. session_state.messages = []\n52 Capitolo 8. Applicazione chatbot: BandiPugliaBot 16 17 # Display chat messages from history on app rerun 18 for message in st.session_state .messages: 19 with st.chat_message(message["role"]): 20 st.markdown(message["content"]) 21 22 if user_input := st.chat_input("Inserisci la tua domanda:"): 23 with st.chat_message("user"): 24 st.markdown(user_input) 25 26 result = chatbt_instance .qa({"question": user_input , "chat_history": chatbt_instance .chat_history }) 27 chatbt_instance .chat_history .append ([( user_input , result["answer"])]) 28 chatbt_instance .qa = chatbt_instance .load_db("stuff", 4) 29', 'chatbt_instance .vector_store = chatbt_instance . load_vector_store () 30 chatbt_instance .answer = result[â€™answer â€™] 31 32 # Add user message to chat history 33 st. session_state.messages.append ({"role": "user", "content": user_input }) 34 35 # Display assistant response in chat message container 36 with st.chat_message("assistant"): 37 message_placeholder = st.empty () 38 full_response = "" 39 assistant_response = chatbt_instance .answer 40 # Simulate stream of response with milliseconds delay 41 for chunk in assistant_response .split (): 42 full_response += chunk + " " 43 time.sleep (0.05) 44 # Add a blinking cursor to simulate typing 45 message_placeholder .markdown( full_response + " ") 46 message_placeholder .markdown( full_response ) 47 # Add assistant response to chat history 48 chatbt_instance .memory.save_context ({"input": user_input}, {"output": full_response }) 49 st. session_state.messages.append ({"role": "assistant", "content": full_response }) SNIPPET 8.1: Codice Streamlit per Chatbot Il', 'seguente codice in maniera riassuntiva esegue: â€¢ Inizializzazione e Titolo: La prima parte del codice inizializza lâ€™applicazione Stream- lit e imposta il titolo della pagina. â€¢ Caricamento PDF: Viene offerta la possibilitÃ  di caricare un file PDF tramite unâ€™in- terfaccia di upload. Se un file viene caricato, la funzione load_pdf della classe chatbt viene chiamata per elaborare il documento. â€¢ Messaggio di Benvenuto: Viene visualizzato un messaggio di benvenuto nellâ€™inter- faccia chat dellâ€™applicazione, introducendo lâ€™assistente virtuale e il suo scopo.\n8.1. Streamlit 53 â€¢ Gestione della Cronologia delle Chat: Il codice mantiene una cronologia delle in- terazioni utente allâ€™interno della sessione corrente, utilizzando st.session_state per memorizzare i messaggi scambiati. â€¢ Input dellâ€™Utente e Risposta del Chatbot: Lâ€™utente puÃ² inserire domande tramite un campo di input chat. Le domande vengono poi elaborate dalla classe chatbt, che utilizza il meccanismo di conversazione e recupero informazioni per fornire', 'una risposta pertinente basata sul contesto del PDF caricato e su altre fonti di dati integrate. â€¢ Visualizzazione Dinamica delle Risposte: Le risposte del chatbot vengono visualiz- zate nella chat in modo dinamico, simulando la digitazione per rendere lâ€™interazio- ne piÃ¹ naturale e coinvolgente. â€¢ Memorizzazione e Aggiornamento della Cronologia: Le interazioni vengono conti- nuamente aggiornate e memorizzate, consentendo al sistema di mantenere un con- testo delle conversazioni e di apprendere dalle interazioni passate per migliorare la qualitÃ  delle risposte future. Una volta caricato il chatbot sul server di Streamlit Ã¨ completamente accessibile at- traverso il link https://chatbot-bandi-puglia.streamlit.app/, la sua interfaccia visibile dallâ€™Utente Ã¨ la seguente: FIGURA 8.1: Streamlit Interface per BandiPugliaBot\n54 Capitolo 8. Applicazione chatbot: BandiPugliaBot 8.2 Classe chatbt La classe chatbt Ã¨ progettata per implementare un chatbot che gestisce conversazioni, interroga database e carica documenti', 'PDF per lâ€™analisi del testo: 1 class chatbt: 2 pdf_caricato = False 3 4 def __init__(self): 5 self.chat_history = [] 6 self.memory = ConversationTokenBufferMemory (memory_key=" chat_history", return_messages =True , llm=OpenAI (), max_token_limit =1200) 7 self.qa = self.load_db("stuff", 4) 8 self.pdf_caricato = True 9 self.vector_store = self. load_vector_store () 10 11 def load_db(self , chain_type , k): 12 vector_store = self. load_vector_store () 13 retriever = vector_store .as_retriever(search_type="similarity", search_kwargs ={"k": k}) 14 # memory = ConversationBufferMemory (memory_key =" chat_history ", return_messages =True) 15 16 qa = ConversationalRetrievalChain .from_llm( 17 llm=ChatOpenAI(model_name=â€™ft:gpt -3.5- turbo -1106: links:gpt -3-5- signorile :8 S8SNEPI â€™, 18 temperature =0.6 , 19 max_tokens =1024 , 20 model_kwargs= {" frequency_penalty ": 0.5}) , 21 chain_type=chain_type , 22 retriever=retriever , 23 memory = self.memory , 24 verbose= True 25 ) 26 return qa 27 28 def load_vector_store', '(self): 29 embeddings = OpenAIEmbeddings () 30 directory = â€™Documenti/docs/chroma/â€™ 31 # Utilizza Chroma solo se il PDF non stato caricato 32 if self.pdf_caricato == True: 33 vector_store = Chroma( persist_directory = directory , embedding_function = embeddings) 34 else: 35 indexname=â€™embedding -bandi â€™ 36 vector_store = PineconeStore (index_name = indexname , embedding = embeddings) 37 return vector_store 38 39 40 def load_pdf(self , uploaded_file ):\n8.2. Classe chatbt 55 41 directory = â€™Documenti/docs/chroma/â€™ 42 temp_dir = tempfile.mkdtemp () 43 path = os.path.join(temp_dir , uploaded_file .name) 44 with open(path , "wb") as f: 45 f.write(uploaded_file .getvalue ()) 46 pdf_loader = PyPDFLoader(path) 47 pdf_text = pdf_loader.load () 48 text_splitter = RecursiveCharacterTextSplitter (chunk_size =1000 , chunk_overlap =150) 49 docs = text_splitter. split_documents (pdf_text) 50 embeddings = OpenAIEmbeddings () 51 vector_store = Chroma. from_documents (docs , embeddings , persist_directory = directory) 52 53', 'chatbt_instance .vector_store = vector_store 54 chatbt_instance .qa = chatbt_instance .load_db("stuff", 4) 55 self.pdf_caricato = True 56 return st.success("Documento PDF caricato con successo!"), vector_store SNIPPET 8.2: Classe chatbt Un riassunto di cosa fa la seguente classe Ã¨: â€¢ Inizializzazione: Al momento della creazione di unâ€™istanza, la classe inizializza una cronologia di chat vuota, configura la memoria per tracciare il contesto delle con- versazioni, carica un database di domande e risposte, segnala che un PDF Ã¨ stato caricato (anche se il caricamento effettivo avviene in seguito), e prepara un nuovo vector embeddings per lâ€™analisi semantica dei documenti caricati dallâ€™utente . â€¢ Caricamento del Database (load_db): Questo metodo carica un database di doman- de e risposte utilizzando un modello di retrieval conversazionale che integra ricerca semantica e capacitÃ  di memorizzazione delle conversazioni. Utilizza il modello di OpenAI Fine-Tuned per il processamento delle domande e delle risposte sui', 'Bandi della Regione Puglia. â€¢ Caricamento dello Spazio di Memorizzazione Vettoriale (load_vector_store): Pre- para uno spazio di memorizzazione vettoriale per lâ€™analisi semantica, utilizzando Chroma per i documenti giÃ  caricati o Pinecone per nuovi caricamenti, a seconda dello stato del flag pdf_caricato. â€¢ Caricamento di PDF (load_pdf): Carica un documento PDF, estrae il testo, lo divi- de in segmenti gestibili, genera rappresentazioni vettoriali del testo e aggiorna lo spazio di memorizzazione vettoriale e il sistema di recupero conversazionale per riflettere il nuovo contenuto.\n56 Capitolo 8. Applicazione chatbot: BandiPugliaBot 8.3 Questionario Valutazione UsabilitÃ  del Chatbot Dopo aver analizzato i dati raccolti dai questionari sullâ€™usabilitÃ  di un chatbot destinato a rispondere ai bandi della regione Puglia, si possono evidenziare diversi aspetti signi- ficativi che riflettono le valutazioni complessive degli utenti. Questi risultati si basano sulle risposte fornite da 10 partecipanti. Innanzitutto,', 'il punteggio medio complessivo del CUQ (Customer Usability Que- stionnaire) si attesta a 81.56, indicando un livello di usabilitÃ  generalmente alto. Questo punteggio riflette la percezione degli utenti riguardo lâ€™efficacia, lâ€™efficienza e la soddisfa- zione nellâ€™utilizzo del chatbot. Analizzando i punteggi medi per ciascuna domanda, emergono alcuni trend che meritano attenzione: â€¢ Le domande legate alla facilitÃ  dâ€™uso e allâ€™intuitivitÃ  dellâ€™interfaccia (Q1, Q3, Q5) hanno ricevuto valutazioni elevate, con medie che variano da 4.1 a 4.4 su una scala da 1 a 5, suggerendo che gli utenti trovano il chatbot relativamente facile e piacevole da utilizzare. â€¢ La domanda Q7, che potrebbe riferirsi alla capacitÃ  del chatbot di fornire risposte pertinenti e utili, ha ottenuto il punteggio medio piÃ¹ alto di 4.7, indicando unâ€™alta soddisfazione degli utenti riguardo la qualitÃ  delle interazioni. â€¢ Le domande Q4, Q12, Q14, e Q16, con medie che vanno da 1.3 a 1.5, potrebbero riguardare aspetti critici dellâ€™esperienza', 'utente, come la gestione degli errori o la complessitÃ  delle funzionalitÃ . Questi punteggi bassi suggeriscono aree di miglio- ramento significative, dove gli utenti hanno riscontrato difficoltÃ  o insoddisfazione. â€¢ In generale, le domande relative alla soddisfazione dellâ€™utente (Q9, Q11, Q13, Q15) hanno ricevuto valutazioni molto positive, con punteggi che variano da 4.3 a 4.8, confermando che, nonostante alcune criticitÃ , lâ€™esperienza complessiva con il chat- bot Ã¨ stata percepita positivamente. Questi risultati suggeriscono che, mentre il chatbot presenta unâ€™alta usabilitÃ  genera- le e soddisfa le esigenze di base degli utenti nella gestione delle richieste legate ai bandi della regione Puglia, ci sono aree specifiche che richiedono attenzione e miglioramento. In particolare, aspetti legati alla gestione degli errori, alla comprensione delle istruzio- ni complesse e alla navigazione nellâ€™interfaccia possono essere ottimizzati per elevare ulteriormente lâ€™esperienza utente. Lâ€™adozione di miglioramenti mirati', 'in base a questi feedback puÃ² contribuire signifi- cativamente a ottimizzare il fine-tuning del chatbot, garantendo che le interazioni siano non solo funzionali ma anche piacevoli e intuitive per lâ€™utente finale. Questo processo di iterazione continua, basato sulla raccolta e sullâ€™analisi di feedback specifici, Ã¨ cruciale per lo sviluppo di soluzioni di chatbot efficaci e apprezzati dagli utenti.\n57 Conclusioni Il viaggio intrapreso in questa tesi ha esplorato le frontiere dellâ€™intelligenza artificiale at- traverso il fine-tuning di ChatGPT, con un focus specifico sui bandi della Regione Puglia. Attraverso unâ€™esplorazione meticolosa dei Large Language Models (LLMs), il fine-tuning completo, il fine-tuning efficiente in termini di parametri, il Prompt-Engineering, e la Re- trieval Augmented Generation (RAG), questo lavoro ha cercato di ottimizzare la risposta di ChatGPT in un ambito tanto specifico quanto complesso. Lâ€™applicazione pratica di questi concetti attraverso il BandiPugliaBot ha dimostra- to che Ã¨', 'possibile migliorare significativamente lâ€™accessibilitÃ  e lâ€™efficacia delle informa- zioni sui bandi regionali per gli utenti. La valutazione dellâ€™usabilitÃ , condotta con un questionario specifico, ha fornito feedback prezioso che sottolinea non solo lâ€™efficacia dellâ€™approccio adottato ma anche le aree di potenziale miglioramento. Implicazioni e Riflessioni La capacitÃ  di ChatGPT di fornire risposte pertinenti e contestualmente appropriate ai quesiti sui bandi della Regione Puglia apre nuove vie per lâ€™interazione tra cittadini e am- ministrazioni pubbliche. Tuttavia, come evidenziato nella sezione "Sfide e Considerazio- ni Future", questo lavoro non Ã¨ esente da sfide. La gestione della privacy, lâ€™allineamento dei modelli ai valori umani, la necessitÃ  di risorse considerevoli per lâ€™addestramento, e il rischio di generazione di informazioni false o fuorvianti rappresentano questioni cruciali che richiedono unâ€™attenzione continua. Sviluppi Futuri Uno sviluppo futuro di questâ€™applicazione sarebbe quello di', 'automatizzare ulteriormente il processo di Fine-Tuning per adattare dinamicamente ChatGPT alle mutevoli esigenze informative e alle nuove pubblicazioni di bandi. Implementando algoritmi di apprendi- mento automatico che monitorano in tempo reale le tendenze e le domande degli utenti, si potrebbe sviluppare un sistema capace di auto-aggiornarsi, riducendo cosÃ¬ la neces- sitÃ  di interventi manuali nel processo di addestramento e nel mantenimento del mo- dello. Questo porterebbe a un miglioramento della precisione delle risposte fornite, una maggiore personalizzazione dellâ€™esperienza utente, e una riduzione dei tempi di rispo- sta. Lâ€™automazione del fine-tuning potrebbe essere estesa anche ad altre aree o funzio- ni richieste dallâ€™utente, trasformando ChatGPT in uno strumento ancora piÃ¹ versatile e adattabile Un altro sviluppo potenziale riguarda lâ€™analisi completa del sito web per lâ€™addestra- mento del chatbot su ogni sezione specifica, mirando a creare un assistente specializzato\n58 Conclusioni per il', 'suddetto sito. Questo addestramento sarebbe personalizzato e si aggiornerebbe automaticamente ad ogni rinnovamento del sito web e dei documenti in esso contenuti. In aggiunta, il chatbot sarebbe progettato per apprendere e rispondere in modo ottimale alle domande piÃ¹ frequenti poste dagli utenti. In conclusione, questo lavoro di tesi rappresenta un passo importante verso la com- prensione e lâ€™applicazione pratica dei modelli di linguaggio avanzati. La strada da per- correre Ã¨ ancora lunga e ricca di sfide, ma le possibilitÃ  e le potenzialitÃ  che si aprono sono immense e promettenti. La collaborazione interdisciplinare tra ricercatori, sviluppatori, e policy makers sarÃ  fondamentale per navigare il futuro degli LLMs in modo etico ed efficace, assicurando che la tecnologia continui a servire lâ€™umanitÃ  nel modo piÃ¹ positivo possibile.\n59 Ringraziamenti In questi lunghi anni che mi hanno portato alla realizzazione di questa tesi e dunque alla conclusione della triennale, ci sono state molte persone che hanno', 'avuto un ruolo fondamentale nel mio percorso. Primo fra tutti, desidero esprimere la mia piÃ¹ profonda gratitudine ai miei genitori. Il loro incondizionato sostegno, la fiducia che hanno riposto in me e i sacrifici che hanno fatto lungo tutta la mia carriera universitaria sono stati il faro che ha illuminato il mio cammino. Senza il loro amore, la loro pazienza e il loro incoraggiamento, non sarei la persona che sono oggi. Vorrei inoltre ringraziare tutti i miei colleghi e amici, la mia rete di supporto quoti- diano. Grazie per tutte le discussioni stimolanti, per i momenti di condivisione e per le risate che abbiamo condiviso. La vostra presenza ha reso questo viaggio meno arduo e decisamente piÃ¹ gioioso. Un ringraziamento speciale a Ester che mi ha supportato e ha reso possibile la mia laurea concretamente, svolgendo progetti di coppia e fornendomi il materiale per com- prendere piÃ¹ facilmente qualsiasi esame, rispondendo a qualsiasi mia domanda e dubbio. Un ringraziamento anche a Donatello che mi ha', 'aiutato nel comprendere gli esercizi di analisi e a Giuseppe nellâ€™impresa di superare un muro. Un ringraziamento va anche a Lucrezia. Grazie per aver creduto in me anche quando dubitavo di me stesso. Grazie a tutti voi per aver camminato al mio fianco in questa importante fase della mia vita. Le vostre parole di incoraggiamento, i vostri consigli e la vostra fiducia in me sono stati molto utili per il raggiungimento di questo importante obiettivo.\n\n61 Bibliografia [1] Colin Raffel et al. Â«Exploring the Limits of Transfer Learning with a Unified Text- to-Text TransformerÂ». In: Journal of Machine Learning Research. 2020. [2] Jason Wei et al. Â«Chain of Thought Prompting Elicits Reasoning in Large Language ModelsÂ». In: arXiv preprint arXiv:2201.11903 (2022). [3] Tom B. Brown et al. Â«Language Models are Few-Shot LearnersÂ». In: arXiv preprint arXiv:2005.14165 (2020). [4] Jacob Devlin et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. 2019. arXiv: 1810.04805 [cs.CL]. [5] Steven', 'Y. Feng et al. A Survey of Data Augmentation Approaches for NLP. 2021. arXiv: 2105.03075 [cs.CL]. [6] Zihao Fu et al. On the Effectiveness of Parameter-Efficient Fine-Tuning. 2022. arXiv: 2211.15583 [cs.CL]. [7] Yunfan Gao et al. Retrieval-Augmented Generation for Large Language Models: A Sur- vey. 2024. arXiv: 2312.10997 [cs.CL]. [8] https://neo4j.com/developer-blog/fine-tuning-retrieval-augmented generation/. Â«Supervised fine-tuning flow.Â» In: Neo4J (2023). [9] Edward J. Hu et al. LoRA: Low-Rank Adaptation of Large Language Models. 2021. arXiv: 2106.09685 [cs.CL]. [10] Jie Huang e Kevin Chen-Chuan Chang. Towards Reasoning in Large Language Models: A Survey. 2023. arXiv: 2212.10403 [cs.CL]. [11] YuHe Ke et al. Development and Testing of Retrieval Augmented Generation in Large Language Models â€“ A Case Study Report. 2024. arXiv: 2402.01733 [cs.CL]. [12] Dawid J. Kopiczko, Tijmen Blankevoort e Yuki M. Asano. VeRA: Vector-based Ran- dom Matrix Adaptation. 2024. arXiv: 2310.11454 [cs.CL]. [13] Langchain. Getting', 'Started - Introduction. 2024. URL: https://python.langchain. com/docs/get_started/introduction (visitato il 23/02/2024). [14] LangChain. RecursiveCharacterTextSplitter Function Documentation. 2024. URL: https: //www.langchain.com/docs/recursivecharactertextsplitter.\n62 Bibliografia [15] Patrick Lewis et al. Â«Retrieval-Augmented Generation for Knowledge-Intensive NLP TasksÂ». In: Advances in Neural Information Processing Systems. A cura di H. La- rochelle et al. Vol. 33. Curran Associates, Inc., 2020, pp. 9459â€“9474. URL: https:// proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5- Paper.pdf. [16] Vladislav Lialin, Vijeta Deshpande e Anna Rumshisky. Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning. 2023. arXiv: 2303.15647 [cs.CL]. [17] Yuning Mao et al. Generation-Augmented Retrieval for Open-domain Question Answe- ring. 2021. arXiv: 2009.08553 [cs.CL]. [18] Humza Naveed et al. A Comprehensive Overview of Large Language Models. 2024. arXiv: 2307.06435', '[cs.CL]. [19] OpenAI. Â«Fine-TuningÂ». In: https://platform.openai.com/docs/guides/fine-tuning (2023). [20] OpenAI. GPT-3.5-turbo: How many tokens are in my text string? 2022. URL: https: //platform.openai.com/docs/guides/turbo/how-many-tokens-are-in-my- text-string. [21] OpenAI. Use Cases of Embeddings. https://platform.openai.com/docs/guides/ embeddings/use-cases. Accessed: 2023-02-21. 2023. [22] Pinecone Systems Inc. Overview of Pinecone: A Vector Database for Machine Learning Applications. https://docs.pinecone.io/docs/overview. Accessed: 2024-02-26. 2023. [23] Alec Radford e Karthik Narasimhan. Â«Improving Language Understanding by Ge- nerative Pre-TrainingÂ». In: 2018. URL: https://api.semanticscholar.org/CorpusID: 49313245. [24] Streamlit. Streamlit Documentation. https://docs.streamlit.io/. Accessed: 2024- 02-26. 2023. [25] Ashish Vaswani et al. Â«Attention is all you needÂ». In: Advances in neural information processing systems. 2017, pp. 5998â€“6008. [26] Wayne Xin Zhao et al. A Survey of Large Language', 'Models. 2023. arXiv: 2303.18223 [cs.CL].']
2024-04-02 11:02:47,845 - INFO - elenco_documenti_per_il_ricongiungimento.pdf
2024-04-02 11:02:49,015 - INFO - [DetectionCheckpointer] Loading from model/model_final.pth ...
2024-04-02 11:02:49,015 - INFO - [Checkpointer] Loading from model/model_final.pth ...
2024-04-02 11:02:49,248 - WARNING - The checkpoint state_dict contains keys that are not used by the model:
  [35mpixel_mean[0m
  [35mpixel_std[0m
2024-04-02 11:02:50,456 - INFO - 1
2024-04-02 11:02:50,456 - INFO - No tables
2024-04-02 11:02:50,906 - INFO - 2
2024-04-02 11:02:50,906 - INFO - No tables
2024-04-02 11:02:51,367 - INFO - 3
2024-04-02 11:02:51,367 - INFO - No tables
2024-04-02 11:02:51,821 - INFO - 4
2024-04-02 11:02:51,821 - INFO - No tables
2024-04-02 11:02:52,267 - INFO - 5
2024-04-02 11:02:52,267 - INFO - No tables
2024-04-02 11:02:52,729 - INFO - 6
2024-04-02 11:02:52,734 - INFO - 
2024-04-02 11:02:53,306 - INFO - 7
2024-04-02 11:02:53,309 - INFO - 
2024-04-02 11:02:53,608 - INFO -  
Convenzionale 
Comprensivo quota 
CUAF 
Senza quota  
CUAF  
 
â‚¬  6,97 
 
 
â‚¬  7,88 
 
 
â‚¬  9,59 
 
â‚¬  1,49   (0,35)  
 
 
â‚¬  1,68   (0,40) 
 
 
â‚¬  2,05   (0,48)  
 
â‚¬   1,50    (0,35)  
 
 
â‚¬   1,69    (0,40)  
 
 
â‚¬   2,06    (0,48)  
 
â‚¬  5,07 
 
â‚¬  1,08  (0,25)  
 
â‚¬   1,09    (0,25)  

2024-04-02 11:02:54,161 - INFO - 8
2024-04-02 11:02:54,161 - INFO - No tables
2024-04-02 11:02:54,610 - INFO - 9
2024-04-02 11:02:54,610 - INFO - No tables
2024-04-02 11:02:55,062 - INFO - 10
2024-04-02 11:02:55,062 - INFO - No tables
2024-04-02 11:02:55,514 - INFO - 11
2024-04-02 11:02:55,514 - INFO - No tables
2024-04-02 11:02:55,970 - INFO - 12
2024-04-02 11:02:55,971 - INFO - No tables
2024-04-02 11:02:55,976 - INFO - 27
2024-04-02 11:02:55,977 - INFO - length : 1022 text splitted : ['Ministero dellâ€™Interno Dipartimento per le LibertÃ  Civili e lâ€™Immigrazione Direzione Centrale per le Politiche dellâ€™Immigrazione e dellâ€™Asilo ELENCHI DOCUMENTAZIONE E MODULISTICA PER IL RILASCIO DEL NULLA OSTA AL RICONGIUNGIMENTO FAMILIARE (modelli SM e GN) DOCUMENTI ANAGRAFICI 1) pagine del passaporto del richiedente e dei familiari allâ€™estero dove siano visibili il numero e i dati anagrafici (unico file); 2) carta di soggiorno o permesso di soggiorno in corso di validitÃ , ovvero, permesso scaduto, con allegata ricevuta di presentazione dellâ€™istanza di rinnovo; 3) codice fiscale del richiedente; 4) certificato di stato famiglia del richiedente rilasciato dal comune di residenza (anche in autocertificazione) 5) certificato di stato di famiglia relativo alle persone che abitano nellâ€™alloggio ove dimoreranno i familiari ricongiunti rilasciato dal comune di residenza con la dicitura â€œuso immigrazioneâ€ (anche in autocertificazione); DOCUMENTAZIONE PER Lâ€™ALLOGGIO \x01 SE IN AFFITTO: 1) contratto di affitto che deve', 'essere di durata non inferiore a sei mesi a decorrere dalla data di presentazione della domanda con ricevuta di registrazione e/o rinnovo; 2) certificato di idoneitÃ  alloggiativa rilasciato dal Comune per finalitÃ  di ricongiungimento familiare oppure copia della ricevuta di richiesta del certificato indicante il codice RIA; 3) dichiarazione redatta dal titolare/i dellâ€™appartamento su mod. â€œS2â€, attestante il consenso ad ospitare anche i ricongiunti; 4) documento dâ€™identitÃ  del titolare/i dellâ€™alloggio, debitamente firmata dal medesimo/i; N.B.: in caso di ricongiungimento a favore di un solo minore di anni 14, il certificato di idoneitÃ  alloggiativa puÃ² essere sostituito dalla copia del contratto di affitto e dalla dichiarazione di ospitalitÃ  del titolare/i dellâ€™appartamento redatta su mod. â€œS1â€, oltre a fotocopia del documento dâ€™identitÃ  del dichiarante/i, debitamente firmata dal medesimo/i. In questo caso non va inviato il modello â€œS2â€. \x01 SE IN COMODATO: 1) dichiarazione di cessione fabbricato per', 'ospitalitÃ  redatta dal titolare/i dellâ€™appartamento o contratto di comodato dâ€™uso che deve essere di durata non inferiore\nMinistero dellâ€™Interno Dipartimento per le LibertÃ  Civili e lâ€™Immigrazione Direzione Centrale per le Politiche dellâ€™Immigrazione e dellâ€™Asilo a sei mesi a decorrere dalla data di presentazione della domanda con ricevuta di registrazione e/o rinnovo; 5) certificato di idoneitÃ  alloggiativa rilasciato dal Comune per finalitÃ  di ricongiungimento familiare oppure copia della ricevuta di richiesta del certificato indicante il codice RIA; 2) dichiarazione redatta dal titolare/i dellâ€™appartamento su mod. â€œS2â€, attestante il consenso ad ospitare anche i ricongiunti; 3) documento dâ€™identitÃ  del titolare/i dellâ€™alloggio, debitamente firmata dal medesimo/i; N.B.: in caso di ricongiungimento a favore di un solo minore di anni 14, il certificato di idoneitÃ  alloggiativa puÃ² essere sostituito dalla copia del contratto di affitto e dalla dichiarazione di ospitalitÃ  del titolare/i dellâ€™appartamento', 'redatta su mod. â€œS1â€, oltre a fotocopia del documento dâ€™identitÃ  del dichiarante/i, debitamente firmata dal medesimo/i. In questo caso non va inviato il modello â€œS2â€. \x01 SE DI PROPRIETÃ€: 1) contratto di compravendita; 2) certificato di idoneitÃ  alloggiativa rilasciato dal Comune per finalitÃ  di ricongiungimento familiare oppure copia della ricevuta di richiesta del certificato indicante il codice RIA; DOCUMENTAZIONE PER IL REDDITO \x01 LAVORATORI DIPENDENTI: 1) in caso di attivitÃ  intrapresa da oltre un anno: ultima dichiarazione dei redditi, oppure, ultimo CUD o modello UNICO; in caso di attivitÃ  intrapresa da meno di un anno, devono essere allegate tutte le buste paga; 2) contratto di lavoro/lettera di assunzione (Unilav); 3) ultime tre buste paga (facoltativo se sono state allegate tutte le buste paga); 4) autocertificazione del datore di lavoro, redatta su modello â€œS3â€ con data non anteriore di mesi 1,, da cui risulti lâ€™attualitÃ  del rapporto di lavoro e la retribuzione mensile corrisposta; 5) documento', 'dâ€™identitÃ  del datore di lavoro, debitamente firmato dal medesimo; \x01 LAVORATORI DOMESTICI: 1) ultima dichiarazione dei redditi, oppure, ultimo CUD o modello UNICO, ove prevista; in caso di attivitÃ  intrapresa da meno di un anno, devono essere allegate tutte le buste paga; 2) comunicazione di assunzione allâ€™INPS;\nMinistero dellâ€™Interno Dipartimento per le LibertÃ  Civili e lâ€™Immigrazione Direzione Centrale per le Politiche dellâ€™Immigrazione e dellâ€™Asilo 3) ove previsti, ultimi 3 bollettini di versamento dei contributi INPS con attestazione dellâ€™avvenuto pagamento; 4) autocertificazione del datore di lavoro, redatta su modello â€œS3â€, con data non anteriore di mesi 1 da cui risulti lâ€™attualitÃ  del rapporto di lavoro e la retribuzione mensile corrisposta; 5) documento dâ€™identitÃ  del datore di lavoro, debitamente firmata dal medesimo; \x01 TITOLARI DI DITTE INDIVIDUALI: 1) visura camerale non anteriore a trenta giorni; 2) certificato di attribuzione P. IVA; 3) licenza comunale, ove prevista; 4) se lâ€™attivitÃ  Ã¨ stata', 'avviata da piÃ¹ di 1 anno: a. ultima dichiarazione dei redditi, oppure, ultimo CUD o modello UNICO con allegata ricevuta di presentazione telematica b. bilancino, relativo al periodo dal 1Â° gennaio dellâ€™anno in corso, alla data di presentazione della domanda, timbrato e sottoscritto dal professionista che lo ha redatto; c. copia del documento di identitÃ  del professionista e del tesserino dellâ€™ordine; 1) se lâ€™attivitÃ  Ã¨ stata avviata da meno di 1 anno: a. bilancino, relativo al periodo dal 1Â° gennaio dellâ€™anno in corso, alla data di presentazione della domanda, timbrato e sottoscritto dal professionista che lo ha redatto; b. copia del documento di identitÃ  del professionista e del tesserino dellâ€™ordine; \x01 REDDITO DERIVANTE DA PARTECIPAZIONE IN SOCIETAâ€™: 1) visura camerale della societÃ , non anteriore a trenta giorni; 2) certificato di attribuzione P. IVA; 3) se lâ€™attivitÃ  Ã¨ stata avviata da piÃ¹ di 1 anno: a. ultima dichiarazione dei redditi, oppure, ultimo CUD o modello UNICO con allegata ricevuta di', 'presentazione telematica b. bilancino, relativo al periodo dal 1Â° gennaio dellâ€™anno in corso, alla data di presentazione della domanda, timbrato e sottoscritto dal professionista che lo ha redatto; c. copia del documento di identitÃ  del professionista e del tesserino dellâ€™ordine; 4) se lâ€™attivitÃ  Ã¨ stata avviata da meno di 1 anno: a. bilancino, relativo al periodo dal 1Â° gennaio dellâ€™anno in corso, alla data di presentazione della domanda, timbrato e sottoscritto dal professionista che lo ha redatto; b. copia del documento di identitÃ  del professionista e del tesserino dellâ€™ordine; 5) atto costitutivo della societÃ ; \x01 REDDITO DERIVANTE DA CONTRATTO DI LAVORO PER COLLABORAZIONE COORDINATA E CONTINUATIVA: 1) contratto di lavoro;\nMinistero dellâ€™Interno Dipartimento per le LibertÃ  Civili e lâ€™Immigrazione Direzione Centrale per le Politiche dellâ€™Immigrazione e dellâ€™Asilo 2) copia del documento di identitÃ  dellâ€™altra parte contraente; 3) se a contratto da piÃ¹ di un anno modello UNICO (dichiarazione dei redditi),', 'se da meno di un anno, devono essere presentate le fatture relative ai compensi ricevuti o la dichiarazione IVA; \x01 SOCI LAVORATORI: 1) certificato di attribuzione partita IVA della cooperativa; 2) dichiarazione del presidente della cooperativa da cui risulti lâ€™attualitÃ  del rapporto di lavoro; 3) ultima dichiarazione dei redditi, oppure, ultimo CUD o modello UNICO, ove previsto; 4) ultime tre buste paga oppure, se lavoratore autonomo, fatture relative ai compensi ricevuti; 5) contratto di lavoro/lettera di assunzione (Unilav); 6) copia del libro soci dal quale risulti lâ€™iscrizione del lavoratore; \x01 LIBERI PROFESSIONISTI: 1) iscrizione allâ€™albo del libero professionista 2) se lâ€™attivitÃ  Ã¨ stata avviata da piÃ¹ di 1 anno: a. ultima dichiarazione dei redditi, oppure, ultimo CUD o modello UNICO con allegata ricevuta di presentazione telematica b. bilancino, relativo al periodo dal 1Â° gennaio dellâ€™anno in corso, alla data di presentazione della domanda, timbrato e sottoscritto dal professionista che lo ha redatto;', 'c. copia del documento di identitÃ  del professionista e del tesserino dellâ€™ordine; 3) se lâ€™attivitÃ  Ã¨ stata avviata da meno di 1 anno: a. bilancino, relativo al periodo dal 1Â° gennaio dellâ€™anno in corso, alla data di presentazione della domanda, timbrato e sottoscritto dal professionista che lo ha redatto; b. copia del documento di identitÃ  del professionista e del tesserino dellâ€™ordine; DOCUMENTAZIONE PER I FAMILIARI ULTRASESSANTACINQUENNI 1) dichiarazione di impegno a sottoscrivere una polizza assicurativa sanitaria, o altro titolo idoneo a garantire la copertura di tutti i rischi nel territorio nazionale, in favore dei genitori ultrasessantacinquenni.\nMinistero dellâ€™Interno Dipartimento per le LibertÃ  Civili e lâ€™Immigrazione Direzione Centrale per le Politiche dellâ€™Immigrazione e dellâ€™Asilo A seguito di numerosi quesiti ricevuti da questa Direzione Centrale relativi alla determinazione del reddito dei lavoratori domestici, si Ã¨ provveduto a richiedere allâ€™INPS come debba essere calcolato il reddito di', 'tali lavoratori. Di seguito vengono riportate le istruzioni ricevute dallâ€™INPS - Area Gestione Separata, Lavoratori Autonomi e Lavoratori Domestici â€“ Retribuzione e reddito del lavoratore domestico Il reddito del lavoratore domestico Ã¨ la paga effettivamente percepita, comprensiva della tredicesima e al netto delle quote dei contributi a carico del lavoratore e non quella convenzionale, presente sugli estratti INPS generalizzati â€œUNEXâ€, che Ã¨ invece un valore medio, stabilito dalle normative vigenti, sul quale Ã¨ calcolato lâ€™importo dei contributi da pagare, valido ai soli fini della liquidazione delle prestazioni erogate dallâ€™INPS (pensioni e assegni famigliari, maternitÃ  e disoccupazione). Il datore di lavoro deve indicare sul bollettino il numero di ore lavorate e la paga oraria effettiva, in base alla quale viene stabilita la fascia contributiva di appartenenza. La paga oraria effettiva indicata sul bollettino quindi Ã¨ la somma di piÃ¹ voci: paga pattuita + quota oraria di eventuale indennitÃ  di vitto e', 'alloggio + rateo di 13^. Sulla somma di tali voci Ã¨ determinata la fascia contributiva di appartenenza in base alla quale Ã¨ stabilita una retribuzione convenzionale e un contributo orario (che altro non Ã¨ che lâ€™applicazione dellâ€™aliquota contributiva alla retribuzione convenzionale). Il reddito che il datore di lavoro deve certificare sulla dichiarazione sostitutiva del CUD invece deve essere la somma di tutte le retribuzioni erogate mensilmente al lavoratore piÃ¹ la quota di tredicesima (come previsto allâ€™ art. 33 punto 4 del CCNL Domestici 1 luglio 2013.) Di fatto, dunque, ricade in capo al datore di lavoro la responsabilitÃ  di quanto dichiarato in relazione ai redditi percepiti dal lavoratore. Per i motivi sopra elencati gli Uffici Territoriali del Governo, cosÃ¬ come gli uffici dellâ€™Agenzia delle Entrate, non possono prendere a riferimento i redditi riportati nellâ€™estratto â€œUNEXâ€ INPS, validi appunto ai soli fini della misura della prestazione pensionistica e le prestazioni al sostegno del reddito', '(trattandosi di un montante del capitale calcolato direttamente dalle procedure informatiche sulla retribuzione convenzionale), ma devono considerare i redditi da dichiarazione sostitutiva del CUD rilasciata dal datore di lavoro; tali redditi sono comunque riscontrabili anche dai bollettini MAV pagati, prendendo a riferimento la paga oraria effettiva di ciascun trimestre moltiplicata per il numero delle ore di contributi pagati.\nMinistero dellâ€™Interno Dipartimento per le LibertÃ  Civili e lâ€™Immigrazione Direzione Centrale per le Politiche dellâ€™Immigrazione e dellâ€™Asilo Esempio: - paga oraria effettiva indicata sul MAV â‚¬ 8,66 (â‚¬ 8,00 paga oraria pattuita piÃ¹ â‚¬ 0,66 di rateo 13^) - ore lavorate nel trimestre 325 Il reddito di questo lavoratore per quel trimestre Ã¨ pari a â‚¬ 8,66 x 325 = â‚¬ 2.814,50 (â‚¬ 938,16 al mese) Il reddito convenzionale che comparirÃ  in â€œUNEXâ€ per lo stesso periodo sarÃ  invece di soli â‚¬ 5,00 x 325 h = â‚¬ 1.625,00 (â‚¬ 541,66 al mese). La differenza Ã¨ evidente. Per calcolare il reddito annuo si', 'dovranno, dunque moltiplicare le ore lavorate in un anno x la paga oraria effettiva dichiarata sul MAV. Si troverÃ  cosÃ¬ il reddito lordo del lavoratore dal quale si dovrÃ  poi sottrarre la quota di contributi a carico del lavoratore stesso (vedi tabelle contributi Inps annuali) per ricavare il reddito effettivo come da Dichiarazione annuale del datore di lavoro (CUD) Naturalmente anche per lâ€™Agenzia delle Entrate il reddito da considerare ai fini fiscali non Ã¨ quello convenzionale di â€œUNEXâ€, ma quello del CUD, giustificato anche dalla paga oraria effettiva presente nei bollettini. Contributi anno 2016 - 2017 â€¢ senza contributo addizionale (comma 28, art.2 L. 92/2012)', 'bollettini. Contributi anno 2016 - 2017 â€¢ senza contributo addizionale (comma 28, art.2 L. 92/2012) | LAVORATORI ITALIANI E STRANIERI | Unnamed: 1 | |-----------------------------------|---------------------------| | RETRIBUZIONE ORARIA | IMPORTO CONTRIBUTO ORARIO | \nMinistero dellâ€™Interno Dipartimento per le LibertÃ  Civili e lâ€™Immigrazione Direzione Centrale per l', 'e Politiche dellâ€™Immigrazione e dellâ€™Asilo fino a â‚¬ 7,88 oltre â‚¬ 7,88 fino a â‚¬ 9,59 oltre â‚¬ 9,59 â‚¬ 6,97 â‚¬ 7,88 â‚¬ 9,59 â‚¬ 1,39 (0,35) â‚¬ 1,57 (0,40) â‚¬ 1,91 (0,48) â‚¬ 1,40 (0,35) â‚¬ 1,58 (0,40) â‚¬ 1,93 (0,48) Orario di lavoro superiore a 24 ore settimanali â‚¬ 5,07 â‚¬ 1,01 (0,25) â‚¬ 1,02 (0,25) â€¢ comprensivo contributo addizionale (comma 28, art.2 L. 92/2012) da applicare ai rapporti di lavoro a tempo determinato', 'uto addizionale (comma 28, art.2 L. 92/2012) da applicare ai rapporti di lavoro a tempo determinato | Unnamed: 0 | |---------------------------------------------------------| | LAVORATORI ITALIANI E STRANIERI | | RETRIBUZIONE ORARIA IMPORTO CONTRIBUTO ORARIO | | Comprensivo quota Senza quota | | Effettiva Convenzionale | | CUAF CUAF | | | | fino a â‚¬ 7,88 â‚¬ 6,97 â‚¬ 1,49(0,35) â‚¬1,50(0,35) | | | | oltreâ‚¬ 7,88 | | fino a â‚¬ 9,59 â‚¬ 7,88 â‚¬ 1,68(0,40) â‚¬1,69(0,40) | | | | | | oltreâ‚¬ 9,59 â‚¬ 9,59 â‚¬ 2,05(0,48) â‚¬2,06(0,48) | | | | Orario di lavoro | | superiore a 24 ore â‚¬ 5,07 â‚¬ 1,08 (0,25) â‚¬1,09(0,25) | | settimanali |  | Unnamed: 0 | Unnamed: 1 | Unnamed: 2 | Unnamed: 4 | Unnamed: 5 | Unnamed: 6 | |----', 'io di lavoro | | superiore a 24 ore â‚¬ 5,07 â‚¬ 1,08 (0,25) â‚¬1,09(0,25) | | settimanali |  | Unnamed: 0 | Unnamed: 1 | Unnamed: 2 | Unnamed: 4 | Unnamed: 5 | Unnamed: 6 | |--------------------|--------------|-----------------|--------------|--------------|--------------| | fino a â‚¬ 7,88 | â‚¬ 6,97 | â‚¬ 1,39(0,35) | â‚¬ | 1,40 | (0,35) | | | | | | | | | oltreâ‚¬ 7,88 | | | | | | | fino a â‚¬ 9,59 | â‚¬ 7,88 | â‚¬ 1,57(0,40) | â‚¬ | 1,58 | (0,40) | | | | | | | | | | | | | | | | oltreâ‚¬ 9,59 | â‚¬ 9,59 | â‚¬ 1,91(0,48) | â‚¬ | 1,93 | (0,48) | | | | | | | | | Orario di lavoro | | | | | | | superiore a 24 ore | â‚¬ 5,07 | â‚¬ 1,01 (0,25) | â‚¬ | 1,02 | (0,25) | | settimanali | | | | | | â€¢ comprensivo contributo addizionale (comma 28, art.2 L. 92/2012) da applicare ai rapporti di lavoro a tempo determinato LAVORATORI ITALIANI E STRANIERI RETRIBUZIONE ORARIA IMPORTO CONTRIBUTO ORARIO Effettiva Convenzionale Comprensivo quota CUAF Senza quota CUAF fino a â‚¬ 7,88 oltre â‚¬ 7,88 fino a â‚¬ 9,59 oltre â‚¬ 9,59 â‚¬ 6,97', 'â‚¬ 7,88 â‚¬ 9,59 â‚¬ 1,49 (0,35) â‚¬ 1,68 (0,40) â‚¬ 2,05 (0,48) â‚¬ 1,50 (0,35) â‚¬ 1,69 (0,40) â‚¬ 2,06 (0,48) Orario di lavoro superiore a 24 ore settimanali â‚¬ 5,07 â‚¬ 1,08 (0,25) â‚¬ 1,09 (0,25)\nMinistero dellâ€™Interno Dipartimento per le LibertÃ  Civili e lâ€™Immigrazione Direzione Centrale per le Politiche dellâ€™Immigrazione e dellâ€™Asilo LEGENDA: PAGA ORARIA: Ã¨ il valore della prestazione di unâ€™ora di lavoro (dalla quale va trattenuta sulla busta paga la quota a carico del lavoratore) che il datore dichiara allâ€™atto dellâ€™iscrizione di lavoro domestico allâ€™INPS. PAGA ORARIA EFFETTIVA: Ã¨ la paga oraria comprensiva del rateo di 13^ e di eventuale vitto e alloggio che il datore dichiara nel MAV per il versamento contributivo. PAGA ORARIA CONVENZIONALE: Ã¨ la paga oraria media calcolata dalla procedura necessaria per il calcolo della liquidazione delle prestazioni INPS. QUOTA A CARICO DEL LAVORATORE: Ã¨ la parte contributiva versata dal lavoratore (trattenuta in busta paga), inserita fra parentesi accanto al contributo orario.', 'ELENCO DOCUMENTAZIONE RICHIESTA PER RICONGIUNGIMENTO FAMILIARI AL SEGUITO (modello T) DOCUMENTI ANAGRAFICI A. Delega a favore di cittadino italiano o straniero regolarmente soggiornante in Italia, a presentare lâ€™istanza di nulla osta per familiare al seguito (MODS4), formata presso le competenti autoritÃ  locali, debitamente tradotta e legalizzata dalla rappresentanza diplomatica consolare italiana allâ€™estero. Qualora il delegante (lavoratore) si trovi in Italia, la delega puÃ² essere effettuata dal notaio B. documento di identitÃ  del richiedente DOCUMENTAZIONE PER Lâ€™ALLOGGIO \x01 SE IN AFFITTO: 1) contratto di affitto che deve essere di durata non inferiore a sei mesi a decorrere dalla data di presentazione della domanda con ricevuta di registrazione e/o rinnovo; 2) certificato di idoneitÃ  alloggiativa rilasciato dal Comune per finalitÃ  di ricongiungimento familiare oppure copia della ricevuta di richiesta del certificato indicante il codice RIA;\nMinistero dellâ€™Interno Dipartimento per le LibertÃ  Civili e', 'lâ€™Immigrazione Direzione Centrale per le Politiche dellâ€™Immigrazione e dellâ€™Asilo 3) dichiarazione redatta dal titolare/i dellâ€™appartamento su mod. â€œS2â€, attestante il consenso ad ospitare anche i ricongiunti; 4) documento dâ€™identitÃ  del titolare/i dellâ€™alloggio, debitamente firmata dal medesimo/i; N.B.: in caso di ricongiungimento a favore di un solo minore di anni 14, il certificato di idoneitÃ  alloggiativa puÃ² essere sostituito dalla copia del contratto di affitto e dalla dichiarazione di ospitalitÃ  del titolare/i dellâ€™appartamento redatta su mod. â€œS1â€, oltre a fotocopia del documento dâ€™identitÃ  del dichiarante/i, debitamente firmata dal medesimo/i. In questo caso non va inviato il modello â€œS2â€. \x01 SE IN COMODATO: 1) dichiarazione di cessione fabbricato per ospitalitÃ  redatta dal titolare/i dellâ€™appartamento o contratto di comodato dâ€™uso che deve essere di durata non inferiore a sei mesi a decorrere dalla data di presentazione della domanda con ricevuta di registrazione e/o rinnovo; 2) certificato di', 'idoneitÃ  alloggiativa rilasciato dal Comune per finalitÃ  di ricongiungimento familiare oppure copia della ricevuta di richiesta del certificato indicante il codice RIA; 3) dichiarazione redatta dal titolare/i dellâ€™appartamento su mod. â€œS2â€, attestante il consenso ad ospitare anche i ricongiunti; 4) documento dâ€™identitÃ  del titolare/i dellâ€™alloggio, debitamente firmata dal medesimo/i; N.B.: in caso di ricongiungimento a favore di un solo minore di anni 14, il certificato di idoneitÃ  alloggiativa puÃ² essere sostituito dalla copia del contratto di affitto e dalla dichiarazione di ospitalitÃ  del titolare/i dellâ€™appartamento redatta su mod. â€œS1â€, oltre a fotocopia del documento dâ€™identitÃ  del dichiarante/i, debitamente firmata dal medesimo/i. In questo caso non va inviato il modello â€œS2â€. \x01 SE DI PROPRIETÃ€: 1) contratto di compravendita; 2) certificato di idoneitÃ  alloggiativa rilasciato dal Comune per finalitÃ  di ricongiungimento familiare oppure copia della ricevuta di richiesta del certificato indicante il', 'codice RIA; N.B.: in caso di alloggio in albergo o residence Ã¨ sufficiente allegare al posto dellâ€™idoneitÃ  alloggiativa, la conferma di prenotazione per un mese su carta intestata. DOCUMENTAZIONE RELATIVA AL REDDITO DEI RICHIEDENTI IL RICONGIUNGIMENTO PER FAMILIARE AL SEGUITO, POSSESSORI DI CARTA DI SOGGIORNO.\nMinistero dellâ€™Interno Dipartimento per le LibertÃ  Civili e lâ€™Immigrazione Direzione Centrale per le Politiche dellâ€™Immigrazione e dellâ€™Asilo \x01 LAVORATORI DIPENDENTI: 1) in caso di attivitÃ  intrapresa da oltre un anno ultima dichiarazione dei redditi, oppure, ultimo CUD o modello UNICO; in caso di attivitÃ  intrapresa da meno di un anno, devono essere allegate tutte le buste paga; 2) contratto di lavoro/lettera di assunzione (Unilav); 3) ultime tre buste paga (facoltativo se sono state allegate tutte le buste paga); 4) autocertificazione del datore di lavoro, redatta su modello â€œS3â€ con data non anteriore di mesi 1,, da cui risulti lâ€™attualitÃ  del rapporto di lavoro e la retribuzione mensile', 'corrisposta; 5) documento dâ€™identitÃ  del datore di lavoro, debitamente firmata dal medesimo; \x01 LAVORATORI DOMESTICI: 1) ultima dichiarazione dei redditi, oppure, ultimo CUD o modello UNICO, ove prevista; in caso di attivitÃ  intrapresa da meno di un anno, devono essere allegate tutte le buste paga; 2) comunicazione di assunzione allâ€™INPS; 3) ove previsti, ultimi 3 bollettini di versamento dei contributi INPS con attestazione dellâ€™avvenuto pagamento; 4) autocertificazione del datore di lavoro, redatta su modello â€œS3â€, con data non anteriore di mesi 1 da cui risulti lâ€™attualitÃ  del rapporto di lavoro e la retribuzione mensile corrisposta; 5) documento dâ€™identitÃ  del datore di lavoro, debitamente firmata dal medesimo; \x01 TITOLARI DI DITTE INDIVIDUALI: 1) visura camerale non anteriore a trenta giorni; 2) certificato di attribuzione P. IVA; 3) licenza comunale, ove prevista; 4) se lâ€™attivitÃ  Ã¨ stata avviata da piÃ¹ di 1 anno: a. ultima dichiarazione dei redditi, oppure, ultimo CUD o modello UNICO con allegata', 'ricevuta di presentazione telematica b. bilancino, relativo al periodo dal 1Â° gennaio dellâ€™anno in corso, alla data di presentazione della domanda, timbrato e sottoscritto dal professionista che lo ha redatto con allegata copia del documento di identitÃ  dello stesso e del tesserino dellâ€™ordine; 1) se lâ€™attivitÃ  Ã¨ stata avviata da meno di 1 anno: a. bilancino, relativo al periodo dal 1Â° gennaio dellâ€™anno in corso, alla data di presentazione della domanda, timbrato e sottoscritto dal professionista che lo ha redatto con allegata copia del documento di identitÃ  dello stesso e del tesserino dellâ€™ordine; \x01 REDDITO DERIVANTE DA PARTECIPAZIONE IN SOCIETAâ€™:\nMinistero dellâ€™Interno Dipartimento per le LibertÃ  Civili e lâ€™Immigrazione Direzione Centrale per le Politiche dellâ€™Immigrazione e dellâ€™Asilo 1) visura camerale della societÃ , non anteriore a trenta giorni; 2) certificato di attribuzione P. IVA; 3) se lâ€™attivitÃ  Ã¨ stata avviata da piÃ¹ di 1 anno: a. ultima dichiarazione dei redditi, oppure, ultimo CUD o modello', 'UNICO con allegata ricevuta di presentazione telematica b. bilancino, relativo al periodo dal 1Â° gennaio dellâ€™anno in corso, alla data di presentazione della domanda, timbrato e sottoscritto dal professionista che lo ha redatto con allegata copia del documento di identitÃ  dello stesso e del tesserino dellâ€™ordine; 4) se lâ€™attivitÃ  Ã¨ stata avviata da meno di 1 anno: a. bilancino, relativo al periodo dal 1Â° gennaio dellâ€™anno in corso, alla data di presentazione della domanda, timbrato e sottoscritto dal professionista che lo ha redatto con allegata copia del documento di identitÃ  dello stesso e del tesserino dellâ€™ordine; 5) atto costitutivo della societÃ ; \x01 REDDITO DERIVANTE DA CONTRATTO DI LAVORO PER COLLABORAZIONE COORDINATA E CONTINUATIVA: 1) contratto di lavoro; 2) copia del documento di identitÃ  dellâ€™altra parte contraente; 3) se a contratto da piÃ¹ di un anno ultima dichiarazione dei redditi, oppure, ultimo CUD o modello UNICO; se da meno di un anno, devono essere presentate le fatture relative ai compensi', 'ricevuti o la dichiarazione IVA; \x01 SOCI LAVORATORI: 1) certificato di attribuzione partita IVA della cooperativa; 2) dichiarazione del presidente della cooperativa da cui risulti lâ€™attualitÃ  del rapporto di lavoro; 3) ultima dichiarazione dei redditi, oppure, ultimo CUD o modello UNICO, ove previsto; 4) ultime tre buste paga oppure, se lavoratore autonomo, fatture relative ai compensi ricevuti; 5) contratto di lavoro/lettera di assunzione (Unilav); 6) copia del libro soci dal quale risulti lâ€™iscrizione del lavoratore; \x01 LIBERI PROFESSIONISTI: 1) iscrizione allâ€™albo del libero professionista 2) se lâ€™attivitÃ  Ã¨ stata avviata da piÃ¹ di 1 anno: a. ultima dichiarazione dei redditi, oppure, ultimo CUD o modello UNICO con allegata ricevuta di presentazione telematica\nMinistero dellâ€™Interno Dipartimento per le LibertÃ  Civili e lâ€™Immigrazione Direzione Centrale per le Politiche dellâ€™Immigrazione e dellâ€™Asilo b. bilancino, relativo al periodo dal 1Â° gennaio dellâ€™anno in corso, alla data di presentazione della domanda,', 'timbrato e sottoscritto dal professionista che lo ha redatto con allegata copia del documento di identitÃ  dello stesso e del tesserino dellâ€™ordine; 3) se lâ€™attivitÃ  Ã¨ stata avviata da meno di 1 anno: a. bilancino, relativo al periodo dal 1Â° gennaio dellâ€™anno in corso, alla data di presentazione della domanda, timbrato e sottoscritto dal professionista che lo ha redatto con allegata copia del documento di identitÃ  dello stesso e del tesserino dellâ€™ordine;']
2024-04-02 11:02:55,978 - INFO - Load pretrained SentenceTransformer: nickprock/sentence-bert-base-italian-uncased
2024-04-02 11:04:37,000 - INFO - 2 prompts are loaded, with the keys: ['query', 'text']
2024-04-02 11:04:47,762 - INFO - ASTRA_DB_KEYSPACE is not set. Defaulting to 'default_keyspace'
